\section{Method}
\label{sec:method}

\cstarta In this section, we will present four algorithms to address the problem variants mentioned in \cref{sec:problem}.
First, we show how to sample from \cref{eq:gaussian kde} while a part of $\variable$ is fixed, either with $\bandwidthmatrix=\bandwidth^2\identitymatrix{\dimension}$ (\cref{sec:sampling conditional simple}) or full bandwidth matrix (\cref{sec:sampling conditional hard}).
Next, we consider the more general case in which we sample from \cref{eq:gaussian kde} while the samples are subject to the linear constraints \cref{eq:linear constraint}, either with $\bandwidthmatrix=\bandwidth^2\identitymatrix{\dimension}$ (\cref{sec:sampling constrained simple}) or full bandwidth matrix (\cref{sec:sampling constrained hard}). \cenda



\subsection{Sampling with part of $\variable$ fixed and $\bandwidthmatrix=\bandwidth^2\identitymatrix{\dimension}$}
\label{sec:sampling conditional simple}

To sample from the conditional density $\densityestcond{\cdot}{\variableparta}$, we can use the right-hand side of \cref{eq:proportionality conditional density}.
Just as we decompose $\variable$ in \cref{eq:decomposition}, we split the data points $\datapoint{\indexdata}$ in two parts:
\begin{equation}
	\label{eq:decomposition data}
	\datapoint{\indexdata} = \begin{bmatrix}
		\datapointparta{\indexdata} \\ \datapointpartb{\indexdata}
	\end{bmatrix}, \forall \indexdata \in \{1,\ldots,\numberofsamples\}.
\end{equation}
Using \cref{eq:decomposition data}, we obtain
\begin{equation}
	\label{eq:conditional proportional}
	\densityestcond{\variablepartb}{\variableparta}
	\propto \sum_{\indexdata=1}^{\numberofsamples} 
	\e{
		-\frac{1}{2} 
		\begin{bmatrix}
			\variableparta - \datapointparta{\indexdata} \\ 
			\variablepartb - \datapointpartb{\indexdata}
		\end{bmatrix}^T 
		H^{-1}
		\begin{bmatrix}
			\variableparta - \datapointparta{\indexdata} \\ 
			\variablepartb - \datapointpartb{\indexdata}
		\end{bmatrix}
	}.
\end{equation}
\cstarta Note that because we describe how $\densityestcond{\variablepartb}{\variableparta}$ changes proportionally with terms that contain $\variablepartb$ (hence, the $\propto$-symbol), we have dropped the constant $\constantterm$ from the right-hand side of \cref{eq:conditional proportional}. \cenda

If we assume that $\bandwidthmatrix=\bandwidth^2\identitymatrix{\dimension}$, then \cref{eq:conditional proportional} simplifies to:
\begin{align}
	\densityestcond{\variablepartb}{\variableparta} 
	&\propto \sum_{\indexdata=1}^{\numberofsamples}
	\e{
		-\frac{1}{2h^2} \normtwo{
			\begin{bmatrix}
				\variableparta - \datapointparta{\indexdata} \\ 
				\variablepartb - \datapointpartb{\indexdata}
			\end{bmatrix}
		}^2
	} \nonumber \\
	&\propto \sum_{\indexdata=1}^{\numberofsamples}
	\e{-\frac{1}{2h^2} \normtwo{\variableparta - \datapointparta{\indexdata}}^2} \nonumber \\
	& \superquad \e{-\frac{1}{2h^2} \normtwo{\variablepartb - \datapointpartb{\indexdata}}^2} \nonumber \\
	&\propto \sum_{\indexdata=1}^{\numberofsamples} \weight{\indexdata}
	\e{-\frac{1}{2h^2} \normtwo{\variablepartb - \datapointpartb{\indexdata}}^2}, \label{eq:final conditional simple}
\end{align}
with 
\begin{equation}
	\label{eq:weights conditional simple}
	\weight{\indexdata}=\e{-\frac{1}{2h^2} \normtwo{\variableparta - \datapointparta{\indexdata}}^2},
	\forall \indexdata \in \{1,\ldots,\numberofsamples\}.
\end{equation} 
To sample from \cref{eq:final conditional simple}, two random numbers need to be generated. 
First, an integer $\indexsampling\in\{1,\ldots,\numberofsamples\}$ is randomly chosen with the likelihood of the integer $\indexsampling$ proportional to the weight $\weight{\indexsampling}$ of \cref{eq:weights conditional simple}. 
Next, a random sample is drawn from a Gaussian with covariance $\bandwidth^2 \identitymatrix{\dimensionpartb}$ and mean $\datapointpartb{\indexsampling}$.
The procedure for sampling is summarized in \cref{alg:conditional simple}.

\begin{algorithm}[t]
	\SetKwInOut{Input}{Input}
	\SetKwInOut{Output}{Output}
	\Input{$\datapoint{1}, \ldots, \datapoint{\numberofsamples}$, $\variableparta$, $\bandwidth$}
	\Output{Sample $\variablepartb$ from \cref{eq:conditional density}}
	
	$\datapointparta{1},\ldots,\datapointparta{\numberofsamples}$, $\datapointpartb{1},\ldots,\datapointpartb{\numberofsamples}$ $\gets$ Decompose the data points according to \cref{eq:decomposition data}
	
	$\weight{1},\ldots,\weight{\numberofsamples}$ $\gets$ Compute the weights according to \cref{eq:weights conditional simple}
	
	$\indexsampling$ $\gets$ Generate a random integer $\indexsampling\in\{1,\ldots,\numberofsamples\}$ with the likelihood of $\indexsampling$ proportional to $\weight{\indexsampling}$
	
	$\variablepartb$ $\gets$ Generate a random sample from a Gaussian with covariance $\bandwidth^2 \identitymatrix{\dimensionpartb}$ and mean $\datapointpartb{\indexsampling}$
		
	\caption{Sampling with part of $\variable$ fixed and $\bandwidthmatrix=\bandwidth^2 \identitymatrix{\dimension}$.}
	\label{alg:conditional simple}
\end{algorithm}



\subsection{Sampling with part of $\variable$ fixed and full bandwidth matrix}
\label{sec:sampling conditional hard}

In this section, we assume that $\bandwidthmatrix$ is a symmetric positive definite matrix. 
To ease the notation, let us use the following notation:
\begin{equation}
	\label{eq:bandwidth matrix inverse}
	\bandwidthmatrix^{-1} = 
	\begin{bmatrix}
		\bwiul & \bwiur \\ \bwibl & \bwibr
	\end{bmatrix},
\end{equation}
with $\bwiul \in \realnumbers^{\dimensionparta \times \dimensionparta}$, $\bwiur \in \realnumbers^{\dimensionparta \times \dimensionpartb}$, $\bwibl \in \realnumbers^{\dimensionpartb \times \dimensionparta}$, and $\bwibr \in \realnumbers^{\dimensionpartb \times \dimensionpartb}$.
Using the Schur complement \autocite{zhang2006schur} 
\begin{equation}
	\label{eq:schur complement}
	\bwischur=\bwiul - \bwiur \bwibr^{-1} \bwibl,
\end{equation}
we can write \cref{eq:bandwidth matrix inverse} as
\begin{equation*}
	\bandwidthmatrix^{-1}
	= \begin{bmatrix} \identitymatrix{\dimensionparta} & \bwiur \bwibr^{-1} \\ 0 & \identitymatrix{\dimensionpartb} \end{bmatrix}
	\begin{bmatrix} \bwischur  & 0 \\ 0 & \bwibr \end{bmatrix}
	\begin{bmatrix} \identitymatrix{\dimensionparta}  & 0 \\ \bwibr^{-1} \bwibl & \identitymatrix{\dimensionpartb} \end{bmatrix}.
\end{equation*}
Substituting this in the exponent of \cref{eq:conditional proportional} gives
\begin{align*}
	&\begin{bmatrix} 
		\variableparta - \datapointparta{\indexdata} \\ 
		\variablepartb - \datapointpartb{\indexdata} 
	\end{bmatrix}^T 
	\bandwidthmatrix^{-1}
	\begin{bmatrix} 
		\variableparta - \datapointparta{\indexdata} \\ 
		\variablepartb - \datapointpartb{\indexdata} 
	\end{bmatrix} \\
	&= \begin{bmatrix} 
		\variableparta - \datapointparta{\indexdata} \\ 
		\variablepartb - \datapointpartb{\indexdata} + \bwibr^{-1} \bwiur^T \left(\variableparta - \datapointparta{\indexdata} \right)
	\end{bmatrix}^T 
	\begin{bmatrix} 
		\bwischur  & 0 \\ 
		0 & \bwibr 
	\end{bmatrix} \\
	&\superquad \begin{bmatrix}
		\variableparta - \datapointparta{\indexsampling} \\
		\variablepartb - \datapointpartb{\indexsampling} + \bwibr^{-1}\bwibl \left(\variableparta - \datapointparta{\indexsampling} \right)
	\end{bmatrix} \\
	&= \left(\variableparta - \datapointparta{\indexdata}\right)^T 
	\bwischur 
	\left(\variableparta - \datapointparta{\indexdata}\right) + \nonumber \\
	&\superquad \left( \variablepartb - \datapointpartb{\indexdata} + \bwibr^{-1} \bwiur^T \left(\variableparta - \datapointparta{\indexdata} \right) \right)^T
	\bwibr \\
	&\superquad \left( \variablepartb - \datapointpartb{\indexdata} + \bwibr^{-1} \bwiur^T \left(\variableparta - \datapointparta{\indexdata} \right) \right).
\end{align*}
Using this, we can rewrite \cref{eq:conditional proportional} as
\begin{equation}
	\label{eq:final conditional hard}
	\densityestcond{\variablepartb}{\variableparta} 
	\propto \sum_{\indexdata=1}^{\numberofsamples} \weight{\indexdata}
	\e{ -\frac{1}{2} \left( \variablepartb - \datapointpartbtranslated{\indexdata} \right)^T \bwibr \left( \variablepartb - \datapointpartbtranslated{\indexdata} \right)},
\end{equation}
with
\begin{align}
	\label{eq:weights conditional hard}
	\weight{\indexdata} &= \e{
		-\frac{1}{2} \left(\variableparta - \datapointparta{\indexdata}\right)^T 
		\bwischur
		\left(\variableparta - \datapointparta{\indexdata}\right)},
	\forall \indexdata \in \{1,\ldots,\numberofsamples\}, \\
	\label{eq:offset conditional hard}
	\datapointpartbtranslated{\indexdata} &= \datapointpartb{\indexdata} - \bwibr^{-1}\bwibl \left(\variableparta - \datapointparta{\indexdata} \right),
	\forall \indexdata \in \{1,\ldots,\numberofsamples\}.
\end{align}

To sample from \cref{eq:final conditional hard}, two random numbers need to be generated. 
First, an integer $\indexsampling\in\{1,\ldots,\numberofsamples\}$ is randomly chosen with the likelihood of the integer $\indexsampling$ proportional to the weight $\weight{\indexsampling}$ of \cref{eq:weights conditional hard}. 
Next, a random sample is drawn from a Gaussian with covariance $\bwibr^{-1}$ and mean $\datapointpartbtranslated{\indexsampling}$ of \cref{eq:offset conditional hard}.
The procedure for sampling is summarized in \cref{alg:conditional hard}.


\begin{algorithm}[t]
	\SetKwInOut{Input}{Input}
	\SetKwInOut{Output}{Output}
	\Input{$\datapoint{1}, \ldots, \datapoint{\numberofsamples}$, $\variableparta$, $\bandwidthmatrix$}
	\Output{Sample $\variablepartb$ from \cref{eq:conditional density}}
	
	$\datapointparta{1},\ldots,\datapointparta{\numberofsamples}$, $\datapointpartb{1},\ldots,\datapointpartb{\numberofsamples}$ $\gets$ Decompose the data points according to \cref{eq:decomposition data}
	
	$\bwiul$, $\bwiur$, $\bwibl$, $\bwibr$, $\bwischur$ $\gets$ Compute the inverse of $\bandwidthmatrix$ according to \cref{eq:bandwidth matrix inverse} and $\bwischur$ according to \cref{eq:schur complement}.
	
	$\weight{1},\ldots,\weight{\numberofsamples}$ $\gets$ Compute the weights according to \cref{eq:weights conditional hard}
	
	$\indexsampling$ $\gets$ Generate a random integer $\indexsampling\in\{1,\ldots,\numberofsamples\}$ with the likelihood of $\indexsampling$ proportional to $\weight{\indexsampling}$
	
	$\datapointpartbtranslated{\indexsampling}$ $\gets$ Compute the mean of the Gaussian to generate a sample from according to \cref{eq:offset conditional hard}
	
	$\variablepartb$ $\gets$ Generate a random sample from a Gaussian with covariance $\bwibr^{-1}$ and mean $\datapointpartbtranslated{\indexsampling}$
		
	\caption{Sampling with part of $\variable$ fixed and full bandwidth matrix.}
	\label{alg:conditional hard}
\end{algorithm}



\subsection{Sampling with linear equality constraints and $\bandwidthmatrix=\bandwidth^2\identitymatrix{\dimension}$}
\label{sec:sampling constrained simple}

In this section, instead of sampling from \cref{eq:conditional density}, we want to sample from \cref{eq:gaussian kde} while the generated samples are subject to the constraint \cref{eq:linear constraint}.
To deal with the constraint \cref{eq:linear constraint}, we will perform a rotation of $\variable$, such that a part of the resulting vector is fixed by the constraint \cref{eq:linear constraint}, while the other part of the resulting vector can be freely chosen.
Therefore, after this rotation, we can use the same approach as explained in \cref{sec:sampling conditional simple}.
To perform the rotation, we employ \iac{svd} \autocite{golub2013matrix} of $\constraintmatrix$:
\begin{equation}
	\label{eq:svd constraint matrix}
	\constraintmatrix 
	= \svdu \begin{bmatrix} \svds & 0 \end{bmatrix} \svdv^T
	= \svdu
	\begin{bmatrix} \svds & 0 \end{bmatrix}
	\begin{bmatrix} \svdva^T \\ \svdvb^T \end{bmatrix}
	= \svdu \svds \svdva^T.
\end{equation}
Here, $\svdu \in \realnumbers^{\dimensionparta \times \dimensionparta}$ and $\svdv \in \realnumbers^{\dimension \times \dimension}$ are orthonormal matrices, i.e., $\svdu^{-1} = \svdu^T$ and $\svdv^{-1} = \svdv^T$.
The first $\dimensionparta$ columns of $\svdv$ are denoted by $\svdva$ while $\svdvb$ denotes the remaining columns of $\svdv$.
Moreover, $\svds \in \realnumbers^{\dimensionparta \times \dimensionparta}$ is a diagonal matrix with its so-called singular values on its diagonal.
Because $\constraintmatrix$ has full rank and $\numberofconstraints<\dimension$, all singular values are strictly positive.
As such, evaluating $\svds^{-1}$ is straightforward.
Now, let $\variableconstrained \in \realnumbers^{\dimensionparta}$ and $\variableunconstrained \in \realnumbers^{\dimensionpartb}$ such that
\begin{equation}
	\label{eq:rotation variable}
	\variable = \svdva \variableconstrained + \svdvb \variableunconstrained 
	= \begin{bmatrix} \svdva & \svdvb \end{bmatrix} \begin{bmatrix} \variableconstrained \\ \variableunconstrained \end{bmatrix}
	= \svdv \begin{bmatrix} \variableconstrained \\ \variableunconstrained \end{bmatrix}.
\end{equation}
Note that because $\svdv^{-1} = \svdv^T$, we have $\variableconstrained = \svdva^T \variable$ and $\variableunconstrained = \svdvb^T \variable$.
\cstarta Moreover, $\svdva^T\svdva=\identitymatrix{\dimensionparta}$ and $\svdva^T \svdvb = 0$, such that \cenda substituting \cref{eq:svd constraint matrix,eq:rotation variable} into \cref{eq:linear constraint}, gives
\begin{equation}
	\label{eq:linear constraint after rotation}
	\svdu \svds \svdva^T \left( \svdva \variableconstrained + \svdvb \variableunconstrained \right)
	= \svdu \svds \variableconstrained = \constraintvector.
\end{equation}
This means that in order to satisfy the constraint \cref{eq:linear constraint}, $\variableunconstrained$ can take any value whereas $\variableconstrained$ is fixed:
\begin{equation}
	\label{eq:fixed part}
	\variableconstrained = \svds^{-1} \svdu^T b.
\end{equation}

Similar as $\variableconstrained$ and $\variableunconstrained$, let $\datapointconstrained{\indexdata}=\svdva^T \datapoint{\indexdata}$ and $\datapointunconstrained{\indexdata}=\svdvb^T \datapoint{\indexdata}$. Using this, $\bandwidthmatrix=\bandwidth^2 \identitymatrix{\dimension}$, \cstarta $\svdv^T\svdv=\identitymatrix{\dimension}$, \cenda and \cref{eq:rotation variable}, we can rewrite \cref{eq:gaussian kde}:
\begin{align*}
	\densityest{\variableconstrained, \variableunconstrained} 
	&\propto \sum_{\indexdata=1}^{\numberofsamples}
	\e{ -\frac{1}{2\bandwidth^2}
		\normtwo{ V \begin{bmatrix} 
			\variableconstrained - \datapointconstrained{\indexdata} \\ 
			\variableunconstrained - \datapointunconstrained{\indexdata}
		\end{bmatrix}}^2
	} \\
	&\propto \sum_{\indexdata=1}^{\numberofsamples}
	\e{ -\frac{1}{2\bandwidth^2}
		\normtwo{ \begin{bmatrix} 
			\variableconstrained - \datapointconstrained{\indexdata} \\ 
			\variableunconstrained - \datapointunconstrained{\indexdata}
		\end{bmatrix}}^2
	} \\
	&\propto\sum_{\indexdata=1}^{\numberofsamples}
	\e{ -\frac{1}{2\bandwidth^2} \normtwo{ \variableconstrained - \datapointconstrained{\indexdata} }^2} \\
	&\superquad \e{ -\frac{1}{2\bandwidth^2} \normtwo{ \variableunconstrained - \datapointunconstrained{\indexdata} }^2} \\
	&\propto\sum_{\indexdata=1}^{\numberofsamples}
	\weight{\indexdata} \e{ -\frac{1}{2\bandwidth^2} \normtwo{ \variableunconstrained - \datapointunconstrained{\indexdata} }^2},
\end{align*}
with
\begin{equation}
	\label{eq:weights constrained simple}
	\weight{\indexdata} = \e{ -\frac{1}{2\bandwidth^2} \normtwo{ \variableconstrained - \datapointconstrained{\indexdata} }^2},
	\forall \indexdata \in \{1,\ldots,\numberofsamples\}.
\end{equation}

To generate samples from \cref{eq:gaussian kde} that satisfy \cref{eq:linear constraint}, two random numbers need to be generated. 
First, an integer $\indexsampling\in\{1,\ldots,\numberofsamples\}$ is randomly chosen with the likelihood of the integer $\indexsampling$ proportional to the weight $\weight{\indexsampling}$ of \cref{eq:weights constrained simple}. 
Next, a random sample is drawn from a Gaussian with covariance $\bandwidth^2 \identitymatrix{\dimensionpartb}$ and mean $\datapointunconstrained{\indexsampling}$.
Finally, this random sample is mapped according to \cref{eq:rotation variable} to obtain the final random sample.
The procedure for sampling is summarized in \cref{alg:constrained simple}.

\begin{algorithm}[t]
	\SetKwInOut{Input}{Input}
	\SetKwInOut{Output}{Output}
	\Input{$\datapoint{1}, \ldots, \datapoint{\numberofsamples}$, $\constraintmatrix$, $\constraintvector$, $\bandwidth$}
	\Output{Sample $\variable$ from \cref{eq:gaussian kde} while satisfying $\constraintmatrix \variable = \constraintvector$}
	
	$\svdu$, $\svds$, $\svdva$, $\svdvb$ $\gets$ Perform \iac{svd} of $\constraintmatrix$; see \cref{eq:svd constraint matrix}
	
	$\datapointconstrained{1},\ldots,\datapointconstrained{\numberofsamples}$ $\gets$ Map the data points using $\datapointconstrained{\indexdata} = \svdva^T \datapoint{\indexdata}$
		
	$\datapointunconstrained{1},\ldots,\datapointunconstrained{\numberofsamples}$ $\gets$ Map the data points using $\datapointunconstrained{\indexdata} = \svdvb^T \datapoint{\indexdata}$
	
	$\variableconstrained$ $\gets$ Compute $\variableconstrained$ using \cref{eq:fixed part}
	
	$\weight{1},\ldots,\weight{\numberofsamples}$ $\gets$ Compute the weights according to \cref{eq:weights constrained simple}
	
	$\indexsampling$ $\gets$ Generate a random integer $\indexsampling\in\{1,\ldots,\numberofsamples\}$ with the likelihood of $\indexsampling$ proportional to $\weight{\indexsampling}$
	
	$\variableunconstrained$ $\gets$ Generate a random sample from a Gaussian with covariance $\bandwidth^2 \identitymatrix{\dimensionpartb}$ and mean $\datapointunconstrained{\indexsampling}$
	
	$\variable$ $\gets$ Compute $\variable$ according to \cref{eq:rotation variable}
		
	\caption{Sampling with linear equality constraints and $\bandwidthmatrix=\bandwidth^2 \identitymatrix{\dimension}$.}
	\label{alg:constrained simple}
\end{algorithm}



\subsection{Sampling with linear equality constraints and full bandwidth matrix}
\label{sec:sampling constrained hard}

%In this section, we assume that $\bandwidthmatrix$ is a symmetric positive definite matrix. 
Using \cref{eq:svd constraint matrix,eq:rotation variable,eq:linear constraint after rotation,eq:fixed part}, we can rewrite \cref{eq:gaussian kde}:
\begin{equation*}
	\densityest{\variableconstrained, \variableunconstrained} 
	\propto \sum_{\indexdata=1}^{\numberofsamples}
	\e{ -\frac{1}{2} 
		\begin{bmatrix} 
			\variableconstrained - \datapointconstrained{\indexdata} \\ 
			\variableunconstrained - \datapointunconstrained{\indexdata}
		\end{bmatrix}^T \svdv^T \bandwidthmatrix^{-1} \svdv
		\begin{bmatrix} 
			\variableconstrained - \datapointconstrained{\indexdata} \\ 
			\variableunconstrained - \datapointunconstrained{\indexdata}
		\end{bmatrix}
	}.
\end{equation*}
We can now apply the same approach as in \cref{sec:sampling conditional simple}, but instead of using $\bandwidthmatrix^{-1}$, we use $\svdv^T \bandwidthmatrix^{-1} \svdv$.
Similar as in \cref{eq:bandwidth matrix inverse}, let us use the following notation:
\begin{align}
	\svdv^T \bandwidthmatrix^{-1} \svdv 
	&= \begin{bmatrix} \bwriul & \bwriur \\ \bwribl & \bwribr \end{bmatrix}, \label{eq:rotated matrix inverse} \\
	\bwrischur &= \bwriul - \bwriur \bwribr^{-1} \bwribl \label{eq:rotated schur complement}
\end{align}
with $\bwriul \in \realnumbers^{\dimensionparta \times \dimensionparta}$, $\bwriur \in \realnumbers^{\dimensionparta \times \dimensionpartb}$, $\bwribl \in \realnumbers^{\dimensionpartb \times \dimensionparta}$, and $\bwribr \in \realnumbers^{\dimensionpartb \times \dimensionpartb}$. Following the same approach as in \cref{sec:sampling conditional hard}, we can write
\begin{equation*}
	\densityest{\variableconstrained, \variableunconstrained}
	\propto \sum_{\indexdata=1}^{\numberofsamples} \weight{\indexdata}
	\e{ -\frac{1}{2} \left( \variableunconstrained - \datapointunconstrainedtranslated{\indexdata} \right)^T
		\bwribr \left( \variableunconstrained - \datapointunconstrainedtranslated{\indexdata} \right)},
\end{equation*}
with
\begin{align}
	\label{eq:weights constrained hard}
	\weight{\indexdata} &= \e{ -\frac{1}{2} \left( \variableconstrained - \datapointconstrained{\indexdata} \right)^T
		\bwrischur
		\left( \variableconstrained - \datapointconstrained{\indexdata} \right)},
	\forall \indexdata \in \{1,\ldots,\numberofsamples\}, \\
	\label{eq:offset constrained hard}
	\datapointunconstrainedtranslated{\indexdata} &= \datapointunconstrained{\indexdata} - \bwribr^{-1} \bwribl
	\left( \variableconstrained - \datapointconstrained{\indexdata} \right),
	\forall \indexdata \in \{1,\ldots,\numberofsamples\}.
\end{align}

To generate samples from \cref{eq:gaussian kde} that satisfy \cref{eq:linear constraint}, two random numbers need to be generated. 
First, an integer $\indexsampling\in\{1,\ldots,\numberofsamples\}$ is randomly chosen with the likelihood of the integer $j$ proportional to the weight $\weight{\indexsampling}$ of \cref{eq:weights constrained hard}. 
Next, a random sample is drawn from a Gaussian with covariance $\bwribr^{-1}$ and mean $\datapointunconstrainedtranslated{\indexsampling}$.
Finally, this random sample is mapped according to \cref{eq:rotation variable} to obtain the final random sample.
The procedure for sampling is summarized in \cref{alg:constrained hard}.

\begin{algorithm}[t]
	\SetKwInOut{Input}{Input}
	\SetKwInOut{Output}{Output}
	\Input{$\datapoint{1}, \ldots, \datapoint{\numberofsamples}$, $\constraintmatrix$, $\constraintvector$, $\bandwidthmatrix$}
	\Output{Sample $\variable$ from \cref{eq:gaussian kde} while satisfying $\constraintmatrix \variable = \constraintvector$}
	
	$\svdu$, $\svds$, $\svdva$, $\svdvb$ $\gets$ Perform \iac{svd} of $\constraintmatrix$; see \cref{eq:svd constraint matrix}
	
	$\datapointconstrained{1},\ldots,\datapointconstrained{\numberofsamples}$ $\gets$ Map the data points using $\datapointconstrained{\indexdata} = \svdva^T \datapoint{\indexdata}$
		
	$\datapointunconstrained{1},\ldots,\datapointunconstrained{\numberofsamples}$ $\gets$ Map the data points using $\datapointunconstrained{\indexdata} = \svdvb^T \datapoint{\indexdata}$
	
	$\variableconstrained$ $\gets$ Compute $\variableconstrained$ using \cref{eq:fixed part}
	
	$\bwriul$, $\bwriur$, $\bwribl$, $\bwribr$, $\bwrischur$ $\gets$ Compute $\svdv^T \bandwidthmatrix^{-1} \svdv$ according to \cref{eq:rotated matrix inverse} and $\bwrischur$ according to \cref{eq:rotated schur complement}
	
	$\weight{1},\ldots,\weight{\numberofsamples}$ $\gets$ Compute the weights according to \cref{eq:weights constrained hard}
	
	$\indexsampling$ $\gets$ Generate a random integer $\indexsampling\in\{1,\ldots,\numberofsamples\}$ with the likelihood of $\indexsampling$ proportional to $\weight{\indexsampling}$
	
	$\datapointunconstrainedtranslated{\indexsampling}$ $\gets$ Compute the mean of the Gaussian to generate a sample from according to \cref{eq:offset constrained hard}
	
	$\variableunconstrained$ $\gets$ Generate a random sample from a Gaussian with covariance $\bwribr^{-1}$ and mean $\datapointunconstrainedtranslated{\indexsampling}$
	
	$\variable$ $\gets$ Compute $\variable$ according to \cref{eq:rotation variable}
		
	\caption{Sampling with linear equality constraints and full bandwidth matrix.}
	\label{alg:constrained hard}
\end{algorithm}
