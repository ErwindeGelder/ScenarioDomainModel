\section{Problem definition}
\label{sec:problem}

In \ac{kde}, the \ac{pdf} $\density{\cdot}$ is estimated as follows:
\begin{equation}
	\label{eq:kde}
	\densityest{\variable} = 
	\frac{1}{\numberofsamples} \sum_{\indexdata=1}^{\numberofsamples} 
	\kernelfuncnormalized{\bandwidthmatrix}{\variable - \datapoint{\indexdata}}.
\end{equation}
Here, $\datapoint{\indexdata} \in \realnumbers^{\dimension}$ represents the $i$-th data point.
In total, there are $\numberofsamples$ data points, so $i \in \{1, \ldots, \numberofsamples\}$.
In \cref{eq:kde}, $\kernelfuncnormalized{\bandwidthmatrix}{\cdot}$ is the so-called scaled kernel with a positive definite symmetric bandwidth matrix $\bandwidthmatrix \in \realnumbers^{\dimension \times \dimension}$.
This bandwidth matrix is an important parameter of the \ac{kde}. 
We assume that $\bandwidthmatrix$ is given, either in its simplified diagonal form of $\bandwidthmatrix=\bandwidth^2 \identitymatrix{\dimension}$, where $\identitymatrix{\dimension}$ denotes a $\dimension\times\dimension$ identity matrix, or in its unconstrained form.
We refer the interested reader to \autocite{turlach1993bandwidthselection, jones1996brief, duong2007ks, gramacki2017fft} for details on the estimation of $\bandwidthmatrix$.
The choice of the kernel function is not as important as the choice of the bandwidth \autocite{turlach1993bandwidthselection}.
Often, a Gaussian kernel is opted and this paper is no exception.
The Gaussian kernel is given by
\begin{equation}
	\label{eq:gaussian kernel}
	\kernelfunc{\dummyvar} = \frac{1}{\left(2 \pi \right)^{\dimension/2}} \e{ -\frac{1}{2} \normtwo{\dummyvar}^2},
\end{equation}
where $\normtwo{\dummyvar}^2$ denotes the squared 2-norm of $\dummyvar$; i.e., $\dummyvar^T \dummyvar$.
The kernel $\kernelfunc{\cdot}$ and the scaled kernel $\kernelfuncnormalized{\bandwidthmatrix}{\cdot}$ are related using
\begin{equation}
	\label{eq:scaled kernel}
	\kernelfuncnormalized{\bandwidthmatrix}{\dummyvar}
	= \determinant{\bandwidthmatrix}^{-1/2} \kernelfunc{ \bandwidthmatrix^{-1/2} \dummyvar },
\end{equation}
where $\determinant{\cdot}$ denotes the matrix determinant.
Using \cref{eq:gaussian kernel,eq:scaled kernel} in \cref{eq:kde}, we have
\begin{equation}
	\label{eq:gaussian kde}
	\densityest{\variable}
	= \constantterm
	\sum_{\indexdata=1}^{\numberofsamples} 
	\e{-\frac{1}{2} \left( \variable - \datapoint{\indexdata} \right)^T H^{-1} \left( \variable - \datapoint{\indexdata} \right)},
\end{equation}
where $\constantterm=\frac{1}{N \left(2\pi\right)^{d/2} \determinant{\bandwidthmatrix}}$ is a constant.


%Typically, the choice of the bandwidth matrix is much more critical than the choice of the kernel function \autocite{turlach1993bandwidthselection, bashtannyk2001bandwidth}. 
Sampling new data points from $\densityest{\cdot}$ of \cref{eq:kde} is straightforward.
First, an integer $\indexsampling$ between and including $1$ and $\numberofsamples$ is randomly chosen with each integer equal likelihood. 
Next, a random number is drawn from the kernel $\kernelfuncnormalized{\bandwidthmatrix}{\variable - \datapoint{\indexsampling}}$.

With conditional sampling, the goal is to sample from $\densityest{\cdot}$ while having a part of $\variable$ already given. 
Without loss of generality, consider the variable $\variable$ that can be decomposed into two parts:
\begin{equation}
	\label{eq:decomposition}
	\variable = \begin{bmatrix}
		\variableparta \\ \variablepartb
	\end{bmatrix},
\end{equation}
such that $\variableparta \in \realnumbers^{\dimensionparta}$ and $\variablepartb \in \realnumbers^{\dimensionpartb}$ with $\dimensionparta + \dimensionpartb = \dimension$.
With this notation, the goal of conditional sampling is to sample $\variablepartb$ while $\variableparta$ is fixed, i.e., sampling from the conditional density estimation
\begin{equation}
	\label{eq:conditional density}
	\densityestcond{\variablepartb}{\variableparta}
	= \frac{\densityest{\variablepartb, \variableparta}}{\densityest{\variableparta}}.
\end{equation}

In the first half of \cref{sec:method}, we show a way to sample from a conditional density like \cref{eq:conditional density} that is estimated using \ac{kde}. 
We first show this for the simplified case in which $\bandwidthmatrix=\bandwidth^2 \identitymatrix{\dimension}$. 
In this simplified case, the results from \textcite{hyndman1996estimating, holmes2007fast} can be directly applied.
Next, we show how we can sample in the more general case of a full bandwidth matrix $\bandwidthmatrix$.
In the second half of \cref{sec:method}, we show a way to sample from \iac{kde} like \cref{eq:kde}, but instead of having few values of $\variable$ fixed, like in \cref{eq:conditional density}, we deal with linear equality constraints on $\variable$:
\begin{equation}
	\label{eq:linear constraint}
	\constraintmatrix \variable = \constraintvector.
\end{equation}
Here $\constraintmatrix \in \realnumbers^{\numberofconstraints \times \dimension}$ and $\constraintvector \in \realnumbers^{\numberofconstraints}$ denote the constraint matrix and constraint vector, respectively. 
In total, there are $\numberofconstraints < \dimension$ constraints.
As with the first half of \cref{sec:method}, we first show it in the simplified case in which $\bandwidthmatrix=\bandwidth^2 \identitymatrix{\dimension}$ and later in the more general case of a full bandwidth matrix $\bandwidthmatrix$.

