\section{Problem definition}
\label{sec:problem}

In \ac{kde}, the \ac{pdf} $\density{\cdot}$ is estimated as follows:
\begin{equation}
	\label{eq:kde}
	\densityest{\variable} = 
	\frac{1}{\numberofsamples} \sum_{\indexdata=1}^{\numberofsamples} 
	\kernelfuncnormalized{\bandwidthmatrix}{\variable - \datapoint{\indexdata}}.
\end{equation}
Here, $\datapoint{\indexdata} \in \realnumbers^{\dimension}$ represents the $i$-th data point.
In total, there are $\numberofsamples$ data points, so $i \in \{1, \ldots, \numberofsamples\}$.
In \cref{eq:kde}, $\kernelfuncnormalized{\bandwidthmatrix}{\cdot}$ is the so-called scaled kernel with a positive definite symmetric bandwidth matrix $\bandwidthmatrix \in \realnumbers^{\dimension \times \dimension}$.
The kernel $\kernelfunc{\cdot}$ and the scaled kernel $\kernelfuncnormalized{\bandwidthmatrix}{\cdot}$ are related using
\begin{equation}
	\label{eq:scaled kernel}
	\kernelfuncnormalized{\bandwidthmatrix}{\dummyvar}
	= \determinant{\bandwidthmatrix}^{-1/2} \kernelfunc{ \bandwidthmatrix^{-1/2} \dummyvar },
\end{equation}
where $\determinant{\cdot}$ denotes the matrix determinant.
The choice of the kernel function is not as important as the choice of the bandwidth matrix \autocite{turlach1993bandwidthselection}.
Often, a Gaussian kernel is opted and this paper is no exception.
The Gaussian kernel is given by
\begin{equation}
	\label{eq:gaussian kernel}
	\kernelfunc{\dummyvar} = \frac{1}{\left(2 \pi \right)^{\dimension/2}} \e{ -\frac{1}{2} \normtwo{\dummyvar}^2},
\end{equation}
where $\normtwo{\dummyvar}^2$ denotes the squared 2-norm of $\dummyvar$; i.e., $\dummyvar^T \dummyvar$.
Substituting \cref{eq:gaussian kernel,eq:scaled kernel} into \cref{eq:kde}, we obtain
\begin{equation}
	\label{eq:gaussian kde}
	\densityest{\variable}
	= \constantterm
	\sum_{\indexdata=1}^{\numberofsamples} 
	\e{-\frac{1}{2} \left( \variable - \datapoint{\indexdata} \right)^T H^{-1} \left( \variable - \datapoint{\indexdata} \right)},
\end{equation}
where $\constantterm=\frac{1}{N \left(2\pi\right)^{d/2} \determinant{\bandwidthmatrix}}$ is a constant.

The bandwidth matrix is an important parameter of the \ac{kde}. 
We assume that $\bandwidthmatrix$ is given, either in its simplified diagonal form $\bandwidthmatrix=\bandwidth^2 \identitymatrix{\dimension}$, where $\identitymatrix{\dimension}$ and $\bandwidth$ denote a $\dimension\times\dimension$ identity matrix and scalar bandwidth, respectively, or in its unconstrained form.
We refer the interested reader to \autocite{turlach1993bandwidthselection, jones1996brief, duong2007ks, gramacki2017fft} for details on the estimation of $\bandwidthmatrix$.

Sampling new data points from $\densityest{\cdot}$ of \cref{eq:gaussian kde} is straightforward.
First, an integer $\indexsampling\in\{1,\ldots,\numberofsamples\}$ is randomly chosen with each integer having equal likelihood. 
Next, a random \cstarta sample is drawn from a Gaussian with covariance $\bandwidthmatrix$ and mean $\datapoint{\indexsampling}$. \cenda

With conditional sampling, the goal is to sample from $\densityest{\cdot}$ while having a part of $\variable$ already given. 
Without loss of generality, consider that the variable $\variable$ can be decomposed into two parts:
\begin{equation}
	\label{eq:decomposition}
	\variable = \begin{bmatrix}
		\variableparta \\ \variablepartb
	\end{bmatrix},
\end{equation}
such that $\variableparta \in \realnumbers^{\dimensionparta}$ and $\variablepartb \in \realnumbers^{\dimensionpartb}$ with $\dimensionparta + \dimensionpartb = \dimension$.
With this notation, the goal of conditional sampling is to sample $\variablepartb$ while $\variableparta$ is fixed, i.e., sampling from the conditional density estimation, \cstarta which we denote by $\densityestcond{\cdot}{\variableparta}$.
Using the general product rule for probability, we have
\begin{equation}
	\label{eq:conditional density}
	\densityestcond{\variablepartb}{\variableparta}
	= \frac{\densityest{\begin{bmatrix}\variableparta^T & \variablepartb^T \end{bmatrix}^T}}
	{\int_{\realnumbers^{\dimensionpartb}} \densityest{\begin{bmatrix}\variableparta^T & \variablepartb^T \end{bmatrix}^T} \ud \variablepartb}.
\end{equation}
As we will see later, to sample from $\densityestcond{\cdot}{\variableparta}$, it is not needed to actually evaluate $\densityestcond{\variablepartb}{\variableparta}$.
Instead, it is enough to know how $\densityestcond{\variablepartb}{\variableparta}$ scales with respect to terms that contain $\variablepartb$.
Since the denominator of the right-hand side of \cref{eq:conditional density} does not depend on $\variablepartb$, there is no need to evaluate the integral.
In our notation, we will use the symbol $\propto$ to indicate this.
E.g., for \cref{eq:conditional density}, we can write
\begin{equation}
	\label{eq:proportionality conditional density}
	\densityestcond{\variablepartb}{\variableparta}
	\propto \densityest{\begin{bmatrix}\variableparta^T & \variablepartb^T \end{bmatrix}^T},
\end{equation}
which means that the left-hand side of \cref{eq:proportionality conditional density} is directly proportional to the right-hand side of \cref{eq:proportionality conditional density}.
\cenda

In the first half of \cref{sec:method}, we show a way to sample from a conditional density like \cref{eq:conditional density} that is estimated using \ac{kde}. 
We first show this for the simplified case in which $\bandwidthmatrix=\bandwidth^2 \identitymatrix{\dimension}$. 
In this simplified case, the results from \textcite{hyndman1996estimating, holmes2007fast} can be directly applied.
Next, we show how we can sample in the more general case of a full bandwidth matrix $\bandwidthmatrix$.
In the second half of \cref{sec:method}, we show a way to sample from \iac{kde} like \cref{eq:kde}, but instead of having few values of $\variable$ fixed, like in \cref{eq:conditional density}, we deal with linear equality constraints on $\variable$:
\begin{equation}
	\label{eq:linear constraint}
	\constraintmatrix \variable = \constraintvector.
\end{equation}
Here $\constraintmatrix \in \realnumbers^{\numberofconstraints \times \dimension}$ and $\constraintvector \in \realnumbers^{\numberofconstraints}$ denote the constraint matrix and constraint vector, respectively. 
Here, it is assumed that the constraint matrix $\constraintmatrix$ has full rank.
Note that if $\constraintmatrix$ has not full rank, the constraint of \cref{eq:linear constraint} can easily be reformulated using Gaussian elimination, resulting in a similar constraint with a constraint matrix that has full rank.
In total, there are $\numberofconstraints < \dimension$ constraints.
As with the first half of \cref{sec:method}, we first consider the simplified case in which $\bandwidthmatrix=\bandwidth^2 \identitymatrix{\dimension}$ and later the more general case of a full bandwidth matrix $\bandwidthmatrix$.

