\section{Discussion}
\label{sec:discussion}

% Explain how it scales with d and N
The computational cost scales quadratically with respect to number of parameters that are used to describe a single scenario, i.e., the dimension, and linearly with the number of data points.
Because the number of data points is generally much larger than the dimension of the data points, let us assume that $\numberofsamples \gg \dimension$.
Looking at \cref{alg:constrained hard} and considering $\numberofsamples \gg \dimension$, steps 2, 3, and 6 are most time consuming, because these steps contain a loop over the data points. 
It is easy to see that the number of computations of these steps scale linearly with $\numberofsamples$. 
Since these computations contain a multiplication of a $\dimension$-by-$\dimension$ matrix and a vector with $\dimension$ rows, the computational cost scales quadratically with $\dimension$. 

% Explain how it scales if we don't change the constraint
If we want to sample multiple times using the same linear constraint, it suffices to perform steps 1 till 6 of \cref{alg:constrained hard} only once.
Step 7 of \cref{alg:constrained hard} does not depend on $\dimension$ and scales linearly with $\numberofsamples$ \autocite{vose1991linear}.
Steps 8 till 10 of \cref{alg:constrained hard} do not depend on $\numberofsamples$ and scale quadratically with $\dimension$.
Because these steps do not depend on $\numberofsamples$ and $\numberofsamples \gg \dimension$, the computational cost of these steps is minor compared to step 7 of \cref{alg:constrained hard}. 
\cstarta Therefore, if we want to draw many samples, i.e., more than $\numberofsamples$, the computational cost is dominated by the computational cost of step 7, which means that, in that case, the computational cost scales linearly with $\numberofsamples$. \cenda

% Explain possible use of our method
To avoid the curse of dimensionality when estimating the \ac{pdf} \autocite{scott1992multivariate}, the number of parameters can be reduced using \iac{svd} or \iac{pca}.
With \ac{svd} and \ac{pca}, the reduced set of parameters is a linear mapping of the original set of parameters \autocite{golub2013matrix, abdi2010principal}.
As a result, if parameters are to be sampled with one or more of the original parameters fixed at a predetermined value, this would give a linear constraint on the new parameters after the reduction.
Our proposed methods in \cref{alg:constrained simple,alg:constrained hard} for sampling with linear constraints become especially useful in this case.
Thus, whereas \cref{alg:conditional simple,alg:conditional hard} could be used if \ac{kde} is used to estimate the \ac{pdf} of the original parameters, \cref{alg:constrained simple,alg:constrained hard}, respectively, should be used if the \ac{kde} is constructed for the reduced set of parameters.
Since scenario-based test cases for the assessment of \acp{av} are likely to be more complex than the ones shown in the example in \cref{sec:example}, it is conceivable that more parameters are to be used and, consequently, that a reduction technique using \iac{svd} or \iac{pca} will become useful.

% Limitation: only for Gaussian.
\cstarta One observation from the algorithms that we have derived, is that the actual conditional distribution is again \iac{kde} with a Gaussian kernel, but now with different (known) weights and a different (known) bandwidth matrix. 
It should be emphasized, however, that the method only works when using a Gaussian kernel. 
In practice, this is usually not a problem, because the choice of the kernel is not crucial \autocite{duong2007ks}. \cenda
