{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test scenario generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import NamedTuple\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import ot\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from scipy.spatial import distance\n",
    "from scipy.stats import ttest_ind\n",
    "from sklearn.model_selection import train_test_split\n",
    "from domain_model import DocumentManagement, StateVariable\n",
    "from stats import KDE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OVERWRITE = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lead vehicle decelerating"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "leaddec = DocumentManagement(os.path.join(\"data\", \"5_scenarios\", \"lead_braking2.json\"))\n",
    "print(\"Number of lead vehicle decelerating scenarios: {:d}\"\n",
    "      .format(len(leaddec.collections[\"scenario\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 50\n",
    "data = np.zeros((len(leaddec.collections[\"scenario\"]), n+1))\n",
    "for i, key in enumerate(leaddec.collections[\"scenario\"]):\n",
    "    scenario = leaddec.get_item(\"scenario\", key)\n",
    "    data[i, n] = scenario.get_duration()\n",
    "    time = np.linspace(0, data[i, n], n) + scenario.get_tstart()\n",
    "    data[i, :n] = scenario.get_state(scenario.get_actor_by_name(\"target vehicle\"),\n",
    "                                     StateVariable.LON_TARGET, time)[:, 0]\n",
    "    \n",
    "# Do the weighting\n",
    "weights = np.zeros(data.shape[1])\n",
    "weights[:n] = 2 / (np.mean(np.std(data[:, :n], axis=0))*n)\n",
    "weights[n] = 1 / np.std(data[:, :n])\n",
    "data = data * weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Nz = len(data) // 4\n",
    "N = len(data) - Nz\n",
    "Nw = 10000\n",
    "print(N, Nz, Nw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score(generated, data):\n",
    "    return ot.emd2([], [], distance.cdist(generated, data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_approach(data, n=None):\n",
    "    i = np.random.choice(np.arange(len(data)), len(data) if n is None else n)\n",
    "    return data[i, :]\n",
    "\n",
    "def new_approach(data, d=3, n=None):\n",
    "    if n is None:\n",
    "        n = len(data)\n",
    "    generated = np.zeros((n, data.shape[1]))\n",
    "    mean = np.mean(data, axis=0)\n",
    "    u,s,v = np.linalg.svd(data-mean, full_matrices=False)\n",
    "    k = KDE(u[:, :d], scaling=True)\n",
    "    k.set_bandwidth(k.silverman())\n",
    "    for i in range(n):\n",
    "        pars = k.sample()[0]\n",
    "        generated[i] = np.dot(pars*s[:d], v[:d]) + mean\n",
    "    return generated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test for different values of d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_method(name, dmin, dmax, beta=1, seed=0, overwrite=False):\n",
    "    filename = os.path.join(\"data\", \"3_test\", \n",
    "                            \"{:s}_d{:d}-{:d}_seed{:d}.p\".format(name, dmin, dmax, seed))\n",
    "    if os.path.exists(filename) and not overwrite:\n",
    "        with open(filename, \"rb\") as file:\n",
    "            scores1, scores2 = pickle.load(file)\n",
    "    else:\n",
    "        scores1 = np.zeros(dmax-dmin+2)\n",
    "        scores2 = np.zeros_like(scores1)\n",
    "        \n",
    "        np.random.seed(seed)\n",
    "        indexa, indexb = train_test_split(np.arange(len(data)), test_size=Nz, \n",
    "                                          random_state=seed)\n",
    "        dataa, datab = data[indexa, :], data[indexb, :]\n",
    "        \n",
    "        generated = naive_approach(dataa, n=Nw)\n",
    "        scores1[0] = score(generated, datab)\n",
    "        scores2[0] = score(generated, dataa)\n",
    "        \n",
    "        for i in range(dmin, dmax+1):\n",
    "            generated = new_approach(dataa, d=i, n=Nw)\n",
    "            scores1[i-dmin+1] = score(generated, datab)\n",
    "            scores2[i-dmin+1] = score(generated, dataa)\n",
    "        with open(filename, \"wb\") as file:\n",
    "            pickle.dump((scores1, scores2), file)\n",
    "    \n",
    "    combined_score = (1+beta)*scores1-beta*scores2\n",
    "    for i in range(len(scores1)):\n",
    "        print(\"q={:2d}: {:7.4f} {:7.4f} {:7.4f}\".format(i, scores1[i], scores2[i], combined_score[i]),\n",
    "              end=\"\")\n",
    "        if combined_score[i] == np.min(combined_score):\n",
    "            print(\"  *\")\n",
    "        else:\n",
    "            print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_method(\"lead_vehicle_decelerating2\", 1, 8, beta=.48, seed=0, overwrite=OVERWRITE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test multiple times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_multiple_times(name, dmin, dmax, seed=0, overwrite=False):\n",
    "    filename = os.path.join(\"data\", \"3_test\", \n",
    "                            \"{:s}_rep_d{:d}-{:d}_seed{:d}\".format(name, dmin, dmax, seed))\n",
    "    if os.path.exists(filename) and not overwrite:\n",
    "        with open(filename, \"rb\") as file:\n",
    "            return pickle.load(file)\n",
    "        \n",
    "    nrepeat = 50\n",
    "    np.random.seed(seed)\n",
    "    scores1 = np.zeros((nrepeat, dmax-dmin+2))\n",
    "    scores2 = np.zeros_like(scores1)\n",
    "    \n",
    "    for i in tqdm(range(nrepeat)):\n",
    "        indexa, indexb = train_test_split(np.arange(N+Nz), test_size=Nz,\n",
    "                                          random_state=nrepeat*seed+i)\n",
    "        dataa, datab = data[indexa, :], data[indexb, :]\n",
    "\n",
    "        # Using the default.\n",
    "        generated = naive_approach(dataa, n=Nw)\n",
    "        scores1[i, 0] = score(generated, datab)\n",
    "        scores2[i, 0] = score(generated, dataa)\n",
    "\n",
    "        # Using approach with different d values.\n",
    "        for d in range(dmin, dmax+1):\n",
    "            generated = new_approach(dataa, d=d, n=Nw)\n",
    "            scores1[i, d-dmin+1] = score(generated, datab)\n",
    "            scores2[i, d-dmin+1] = score(generated, dataa)\n",
    "    \n",
    "    with open(filename, \"wb\") as file:\n",
    "        pickle.dump((scores1, scores2), file)\n",
    "    \n",
    "    return scores1, scores2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def boxplots(scores, ylabel=\"metric\", ax=None):\n",
    "    if ax is None:\n",
    "        _, ax = plt.subplots()\n",
    "    ax.boxplot(scores)\n",
    "    ax.set_xticks(np.arange(1, dmax-dmin+3))\n",
    "    ax.set_xticklabels([\"Training set\\n{:.3f}\".format(np.median(scores[:, 0]))] +\n",
    "                       [\"d={:d}\\n{:.3f}\".format(d, np.median(scores[:, d-dmin+1])) \n",
    "                        for d in range(dmin, dmax+1)])\n",
    "    if ylabel == \"metric\":\n",
    "        ax.set_ylabel(r\"$M(\\mathcal{W},\\mathcal{Z},\\mathcal{X})$\")\n",
    "    elif ylabel == \"many\":\n",
    "        ax.set_ylabel(r\"$W(\\hat{f},f)$\")\n",
    "    elif ylabel == \"self\":\n",
    "        ax.set_ylabel(r\"$W(\\mathcal{W}, \\mathcal{X})$\")\n",
    "    elif ylabel == \"wasserstein\":\n",
    "        ax.set_ylabel(r\"$W(\\mathcal{W}, \\mathcal{Z})$\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dmin, dmax = 2, 5\n",
    "beta = .5\n",
    "s1, s2 = test_multiple_times(\"lead_vehicle_decelerating2\", dmin, dmax, overwrite=OVERWRITE)\n",
    "boxplots(s1 + beta*(s1-s2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation of scoring measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation_scoring(name, dreal, dmin, dmax, overwrite=False):\n",
    "    filename = os.path.join(\"data\", \"3_test\", \n",
    "                            \"{:s}_val_d{:d}-{:d}_dreal{:d}\".format(name, dmin, dmax, dreal))\n",
    "    if os.path.exists(filename) and not overwrite:\n",
    "        with open(filename, \"rb\") as file:\n",
    "            return pickle.load(file)\n",
    "    \n",
    "    nrepeat = 10\n",
    "    np.random.seed(0)\n",
    "    \n",
    "    old_few = np.zeros((nrepeat, dmax-dmin+2))\n",
    "    old_many = np.zeros_like(old_few)\n",
    "    self_few = np.zeros_like(old_few)\n",
    "    \n",
    "    for i in tqdm(range(nrepeat)):\n",
    "        data_new = new_approach(data, d=dreal, n=N)\n",
    "        data_few = new_approach(data, d=dreal, n=Nz)\n",
    "        data_many = new_approach(data, d=dreal, n=Nw)\n",
    "\n",
    "        # Using the default.\n",
    "        generated= naive_approach(data_new, n=Nw)\n",
    "        old_few[i, 0] = score(generated, data_few)\n",
    "        old_many[i, 0] = score(generated, data_many)\n",
    "        self_few[i, 0] = score(generated, data_new)\n",
    "\n",
    "        # Using new approach with different d\n",
    "        for d in range(dmin, dmax+1):\n",
    "            generated = new_approach(data_new, d=d, n=Nw)\n",
    "            old_few[i, d-dmin+1] = score(generated, data_few)\n",
    "            old_many[i, d-dmin+1] = score(generated, data_many)\n",
    "            self_few[i, d-dmin+1] = score(generated, data_new)\n",
    "            \n",
    "    with open(filename, \"wb\") as file:\n",
    "        pickle.dump((old_few, old_many, self_few), file)\n",
    "    \n",
    "    return old_few, old_many, self_few"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_correlation(old_many, old_few, self_few):\n",
    "    beta = np.linspace(0, 1, 100)\n",
    "    correlation = np.zeros(len(beta))\n",
    "    for i in range(len(beta)):\n",
    "        new_score = old_few + beta[i]*(old_few - self_few)\n",
    "        correlation[i] = np.corrcoef(np.median(old_many, axis=0),\n",
    "                                     np.median(new_score, axis=0))[0, 1]\n",
    "        \n",
    "    _, ((ax11, ax12), (ax21, ax22)) = plt.subplots(2, 2, figsize=(12, 8))\n",
    "    boxplots(old_many, \"many\", ax=ax11)\n",
    "    boxplots(old_few, \"wasserstein\", ax=ax12)\n",
    "    boxplots(self_few, \"self\", ax=ax21)\n",
    "    ax22.plot(beta, correlation)\n",
    "    ax22.set_xlabel(r\"$\\beta$\")\n",
    "    ax22.set_ylabel(r\"Correlation medians of $W(\\hat{f},f)$ and \"+\n",
    "                    r\"$M(\\mathcal{W}, \\mathcal{Z}, \\mathcal{X})$\")\n",
    "    ax22.set_title(\"Max correlation: {:.3f} at beta={:.2f}\".format(np.max(correlation), \n",
    "                                                                   beta[np.argmax(correlation)]))\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dreal = 2\n",
    "old_few, old_many, self_few = validation_scoring(\"lead_vehicle_decelerating2\", dreal, dmin, \n",
    "                                                 dmax, overwrite=OVERWRITE)\n",
    "plot_correlation(old_many, old_few, self_few)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cut-in"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data\n",
    "\n",
    "y(t) - (1) speed of other car, (2) y of other car\n",
    "\n",
    "theta - (1) initial ego vehicle speed, (2) initial longitudinal position other car"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cutins = DocumentManagement(os.path.join(\"data\", \"5_scenarios\", \"cut_in_scenarios2.json\"))\n",
    "print(\"Number of cut-in scenarios: {:d}\".format(len(cutins.collections[\"scenario\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 50\n",
    "data = np.zeros((len(cutins.collections[\"scenario\"]), 2*n+3))\n",
    "for i, key in enumerate(cutins.collections[\"scenario\"]):\n",
    "    scenario = cutins.get_item(\"scenario\", key)\n",
    "    data[i, 2*n] = scenario.get_duration()\n",
    "    time = np.linspace(0,  data[i, 2*n], n) + scenario.get_tstart()\n",
    "    data[i, :n] = scenario.get_state(scenario.get_actor_by_name(\"target vehicle\"),\n",
    "                                     StateVariable.LON_TARGET, time)[:, 0]\n",
    "    data[i, n:2*n] = scenario.get_state(scenario.get_actor_by_name(\"target vehicle\"),\n",
    "                                        StateVariable.LAT_TARGET, time)\n",
    "    data[i, 2*n+1] = scenario.get_state(scenario.get_actor_by_name(\"ego vehicle\"),\n",
    "                                        StateVariable.SPEED, scenario.get_tstart())\n",
    "    data[i, 2*n+2] = scenario.get_state(scenario.get_actor_by_name(\"target vehicle\"),\n",
    "                                        StateVariable.LON_TARGET, scenario.get_tstart())[1]\n",
    "\n",
    "# Do the weighting\n",
    "weights = np.zeros(data.shape[1])\n",
    "weights[:n] = 1 / (np.mean(np.std(data[:, :n], axis=0))*n)\n",
    "weights[n:2*n] = 1 / (np.mean(np.std(data[:, n:2*n], axis=0))*n)\n",
    "weights[2*n:] = 1 / np.std(data[:, 2*n:], axis=0)\n",
    "data = data * weights\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Nz = len(data) // 4\n",
    "N = len(data) - Nz\n",
    "Nw = 5000\n",
    "print(N, Nz, Nw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test for different values of d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_method(\"cutin2\", 1, 8, beta=.2, seed=0, overwrite=OVERWRITE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test multiple times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dmin, dmax = 2, 7\n",
    "beta = 0.2\n",
    "s1, s2 = test_multiple_times(\"cutin2\", dmin, dmax, overwrite=OVERWRITE)\n",
    "boxplots(s1 + beta*(s1-s2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation of scoring measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dreal = 3\n",
    "old_few, old_many, self_few = validation_scoring(\"cutin2\", dreal, dmin, \n",
    "                                                 dmax, overwrite=OVERWRITE)\n",
    "plot_correlation(old_many, old_few, self_few)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
