{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test scenario generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import NamedTuple\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from tikzplotlib import save\n",
    "import numpy as np\n",
    "import ot\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from scipy.spatial import distance\n",
    "from scipy.stats import ttest_ind\n",
    "from sklearn.model_selection import train_test_split\n",
    "from domain_model import DocumentManagement, StateVariable\n",
    "from stats import KDE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OVERWRITE = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lead vehicle decelerating"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "leaddec = DocumentManagement(os.path.join(\"data\", \"5_scenarios\", \"lead_braking2.json\"))\n",
    "print(\"Number of lead vehicle decelerating scenarios: {:d}\"\n",
    "      .format(len(leaddec.collections[\"scenario\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 50\n",
    "data1 = np.zeros((len(leaddec.collections[\"scenario\"]), n+1))\n",
    "for i, key in enumerate(leaddec.collections[\"scenario\"]):\n",
    "    scenario = leaddec.get_item(\"scenario\", key)\n",
    "    data1[i, n] = scenario.get_duration()\n",
    "    time = np.linspace(0, data1[i, n], n) + scenario.get_tstart()\n",
    "    data1[i, :n] = scenario.get_state(scenario.get_actor_by_name(\"target vehicle\"),\n",
    "                                      StateVariable.LON_TARGET, time)[:, 0]\n",
    "    \n",
    "# Do the weighting\n",
    "weights1 = np.zeros(data1.shape[1])\n",
    "weights1[:n] = 2 / (np.mean(np.std(data1[:, :n], axis=0))*n)\n",
    "weights1[n] = 1 / np.std(data1[:, :n])\n",
    "data1 = data1 * weights1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Nz = len(data1) // 4\n",
    "N = len(data1) - Nz\n",
    "Nw = 10000\n",
    "print(N, Nz, Nw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score(generated, data):\n",
    "    return ot.emd2([], [], distance.cdist(generated, data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_approach(data, n=None):\n",
    "    i = np.random.choice(np.arange(len(data)), len(data) if n is None else n)\n",
    "    return data[i, :]\n",
    "\n",
    "def new_approach(data, d=3, n=None):\n",
    "    if n is None:\n",
    "        n = len(data)\n",
    "    generated = np.zeros((n, data.shape[1]))\n",
    "    mean = np.mean(data, axis=0)\n",
    "    u,s,v = np.linalg.svd(data-mean, full_matrices=False)\n",
    "    k = KDE(u[:, :d], scaling=True)\n",
    "    # k.set_bandwidth(k.silverman())\n",
    "    k.compute_bandwidth(max_bw=k.silverman())\n",
    "    for i in range(n):\n",
    "        pars = k.sample()[0]\n",
    "        generated[i] = np.dot(pars*s[:d], v[:d]) + mean\n",
    "    return generated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test for different values of d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_method(name, data, dmin, dmax, beta=1, seed=0, overwrite=False):\n",
    "    filename = os.path.join(\"data\", \"3_test\", \n",
    "                            \"{:s}_d{:d}-{:d}_seed{:d}.p\".format(name, dmin, dmax, seed))\n",
    "    if os.path.exists(filename) and not overwrite:\n",
    "        with open(filename, \"rb\") as file:\n",
    "            scores1, scores2 = pickle.load(file)\n",
    "    else:\n",
    "        scores1 = np.zeros(dmax-dmin+2)\n",
    "        scores2 = np.zeros_like(scores1)\n",
    "        \n",
    "        np.random.seed(seed)\n",
    "        indexa, indexb = train_test_split(np.arange(len(data)), test_size=Nz, \n",
    "                                          random_state=seed)\n",
    "        dataa, datab = data[indexa, :], data[indexb, :]\n",
    "        \n",
    "        generated = naive_approach(dataa, n=Nw)\n",
    "        scores1[0] = score(generated, datab)\n",
    "        scores2[0] = score(generated, dataa)\n",
    "        \n",
    "        for i in range(dmin, dmax+1):\n",
    "            generated = new_approach(dataa, d=i, n=Nw)\n",
    "            scores1[i-dmin+1] = score(generated, datab)\n",
    "            scores2[i-dmin+1] = score(generated, dataa)\n",
    "        with open(filename, \"wb\") as file:\n",
    "            pickle.dump((scores1, scores2), file)\n",
    "    \n",
    "    combined_score = (1+beta)*scores1-beta*scores2\n",
    "    for i in range(len(scores1)):\n",
    "        print(\"q={:2d}: {:7.4f} {:7.4f} {:7.4f}\".format(i, scores1[i], scores2[i], combined_score[i]),\n",
    "              end=\"\")\n",
    "        if combined_score[i] == np.min(combined_score):\n",
    "            print(\"  *\")\n",
    "        else:\n",
    "            print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show singular values\n",
    "def plot_show_svd(data):\n",
    "    s_max = 8\n",
    "    mean = np.mean(data, axis=0)\n",
    "    _, s, _ = np.linalg.svd(data-mean, full_matrices=False)\n",
    "    plt.semilogy(np.arange(len(s))+1, s, '.')\n",
    "    plt.xlim(0, s_max+.5)\n",
    "    plt.ylim(s[s_max+1], s[0]*2)\n",
    "    plt.xlabel(\"Index\")\n",
    "    plt.ylabel(\"Singular value\")\n",
    "    \n",
    "    print(\" #  Explained variance [%]\")\n",
    "    for i in range(1, 9):\n",
    "        print(\"{:2d} {:.1f} %\".format(i, np.sum(s[:i]**2) / np.sum(s**2) * 100))\n",
    "plot_show_svd(data1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_method(\"lead_vehicle_decelerating_loocv\", data1, 1, 8, beta=.48, seed=0, overwrite=OVERWRITE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test multiple times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_multiple_times(name, data, dmin, dmax, seed=0, overwrite=False):\n",
    "    filename = os.path.join(\"data\", \"3_test\", \n",
    "                            \"{:s}_rep_d{:d}-{:d}_seed{:d}\".format(name, dmin, dmax, seed))\n",
    "    if os.path.exists(filename) and not overwrite:\n",
    "        with open(filename, \"rb\") as file:\n",
    "            return pickle.load(file)\n",
    "        \n",
    "    nrepeat = 50\n",
    "    np.random.seed(seed)\n",
    "    scores1 = np.zeros((nrepeat, dmax-dmin+2))\n",
    "    scores2 = np.zeros_like(scores1)\n",
    "    \n",
    "    for i in tqdm(range(nrepeat)):\n",
    "        indexa, indexb = train_test_split(np.arange(N+Nz), test_size=Nz,\n",
    "                                          random_state=nrepeat*seed+i)\n",
    "        dataa, datab = data[indexa, :], data[indexb, :]\n",
    "\n",
    "        # Using the default.\n",
    "        generated = naive_approach(dataa, n=Nw)\n",
    "        scores1[i, 0] = score(generated, datab)\n",
    "        scores2[i, 0] = score(generated, dataa)\n",
    "\n",
    "        # Using approach with different d values.\n",
    "        for d in range(dmin, dmax+1):\n",
    "            generated = new_approach(dataa, d=d, n=Nw)\n",
    "            scores1[i, d-dmin+1] = score(generated, datab)\n",
    "            scores2[i, d-dmin+1] = score(generated, dataa)\n",
    "    \n",
    "    with open(filename, \"wb\") as file:\n",
    "        pickle.dump((scores1, scores2), file)\n",
    "    \n",
    "    return scores1, scores2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def boxplots(scores, ylabel=\"metric\", ax=None):\n",
    "    if ax is None:\n",
    "        _, ax = plt.subplots()\n",
    "    ax.boxplot(scores)\n",
    "    ax.set_xticks(np.arange(1, dmax-dmin+3))\n",
    "    ax.set_xticklabels([\"Training set\\n{:.3f}\".format(np.median(scores[:, 0]))] +\n",
    "                       [\"d={:d}\\n{:.3f}\".format(d, np.median(scores[:, d-dmin+1])) \n",
    "                        for d in range(dmin, dmax+1)])\n",
    "    if ylabel == \"metric\":\n",
    "        ax.set_ylabel(r\"$M(\\mathcal{W},\\mathcal{Z},\\mathcal{X})$\")\n",
    "    elif ylabel == \"many\":\n",
    "        ax.set_ylabel(r\"$W(\\hat{f},f)$\")\n",
    "    elif ylabel == \"self\":\n",
    "        ax.set_ylabel(r\"$W(\\mathcal{W}, \\mathcal{X})$\")\n",
    "    elif ylabel == \"penalty\":\n",
    "        ax.set_ylabel(r\"$W(\\mathcal{W}, \\mathcal{Z}) - W(\\mathcal{W}, \\mathcal{X})$\")\n",
    "    elif ylabel == \"wasserstein\":\n",
    "        ax.set_ylabel(r\"$W(\\mathcal{W}, \\mathcal{Z})$\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dmin, dmax = 2, 5\n",
    "beta1 = .25\n",
    "s1, s2 = test_multiple_times(\"lead_vehicle_decelerating_loocv\", data1, dmin, dmax, overwrite=OVERWRITE)\n",
    "boxplots(s1 + beta1*(s1-s2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation of scoring measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation_scoring(name, data, dreal, dmin, dmax, overwrite=False):\n",
    "    filename = os.path.join(\"data\", \"3_test\", \n",
    "                            \"{:s}_val_d{:d}-{:d}_dreal{:d}\".format(name, dmin, dmax, dreal))\n",
    "    if os.path.exists(filename) and not overwrite:\n",
    "        with open(filename, \"rb\") as file:\n",
    "            return pickle.load(file)\n",
    "    \n",
    "    nrepeat = 50\n",
    "    np.random.seed(0)\n",
    "    \n",
    "    old_few = np.zeros((nrepeat, dmax-dmin+2))\n",
    "    old_many = np.zeros_like(old_few)\n",
    "    self_few = np.zeros_like(old_few)\n",
    "    \n",
    "    for i in tqdm(range(nrepeat)):\n",
    "        data_new = new_approach(data, d=dreal, n=N)\n",
    "        data_few = new_approach(data, d=dreal, n=Nz)\n",
    "        data_many = new_approach(data, d=dreal, n=Nw)\n",
    "\n",
    "        # Using the default.\n",
    "        generated= naive_approach(data_new, n=Nw)\n",
    "        old_few[i, 0] = score(generated, data_few)\n",
    "        old_many[i, 0] = score(generated, data_many)\n",
    "        self_few[i, 0] = score(generated, data_new)\n",
    "\n",
    "        # Using new approach with different d\n",
    "        for d in range(dmin, dmax+1):\n",
    "            generated = new_approach(data_new, d=d, n=Nw)\n",
    "            old_few[i, d-dmin+1] = score(generated, data_few)\n",
    "            old_many[i, d-dmin+1] = score(generated, data_many)\n",
    "            self_few[i, d-dmin+1] = score(generated, data_new)\n",
    "            \n",
    "    with open(filename, \"wb\") as file:\n",
    "        pickle.dump((old_few, old_many, self_few), file)\n",
    "    \n",
    "    return old_few, old_many, self_few"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_correlation(old_many, old_few, self_few):\n",
    "    beta = np.linspace(0, 1, 100)\n",
    "    correlation = np.zeros(len(beta))\n",
    "    for i in range(len(beta)):\n",
    "        new_score = old_few + beta[i]*(old_few - self_few)\n",
    "        correlation[i] = np.corrcoef(np.median(old_many, axis=0),\n",
    "                                     np.median(new_score, axis=0))[0, 1]\n",
    "        \n",
    "    _, ((ax11, ax12), (ax21, ax22)) = plt.subplots(2, 2, figsize=(12, 8))\n",
    "    boxplots(old_many, \"many\", ax=ax11)\n",
    "    boxplots(old_few, \"wasserstein\", ax=ax12)\n",
    "    boxplots(old_few - self_few, \"penalty\", ax=ax21)\n",
    "    ax22.plot(beta, correlation)\n",
    "    ax22.set_xlabel(r\"$\\beta$\")\n",
    "    ax22.set_ylabel(r\"Correlation medians of $W(\\hat{f},f)$ and \"+\n",
    "                    r\"$M(\\mathcal{W}, \\mathcal{Z}, \\mathcal{X})$\")\n",
    "    ax22.set_title(\"Max correlation: {:.3f} at beta={:.2f}\".format(np.max(correlation), \n",
    "                                                                   beta[np.argmax(correlation)]))\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dreal = 2\n",
    "old_few1, old_many1, self_few1 = validation_scoring(\"lead_vehicle_decelerating_loocv\", data1, dreal, \n",
    "                                                    dmin, dmax, overwrite=OVERWRITE)\n",
    "plot_correlation(old_many1, old_few1, self_few1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cut-in"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data\n",
    "\n",
    "y(t) - (1) speed of other car, (2) y of other car\n",
    "\n",
    "theta - (1) initial ego vehicle speed, (2) initial longitudinal position other car"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cutins = DocumentManagement(os.path.join(\"data\", \"5_scenarios\", \"cut_in_scenarios2.json\"))\n",
    "print(\"Number of cut-in scenarios: {:d}\".format(len(cutins.collections[\"scenario\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 50\n",
    "data2 = np.zeros((len(cutins.collections[\"scenario\"]), 2*n+3))\n",
    "for i, key in enumerate(cutins.collections[\"scenario\"]):\n",
    "    scenario = cutins.get_item(\"scenario\", key)\n",
    "    data2[i, 2*n] = scenario.get_duration()\n",
    "    time = np.linspace(0,  data2[i, 2*n], n) + scenario.get_tstart()\n",
    "    data2[i, :n] = scenario.get_state(scenario.get_actor_by_name(\"target vehicle\"),\n",
    "                                      StateVariable.LON_TARGET, time)[:, 0]\n",
    "    data2[i, n:2*n] = scenario.get_state(scenario.get_actor_by_name(\"target vehicle\"),\n",
    "                                         StateVariable.LAT_TARGET, time)\n",
    "    data2[i, 2*n+1] = scenario.get_state(scenario.get_actor_by_name(\"ego vehicle\"),\n",
    "                                         StateVariable.SPEED, scenario.get_tstart())\n",
    "    data2[i, 2*n+2] = scenario.get_state(scenario.get_actor_by_name(\"target vehicle\"),\n",
    "                                         StateVariable.LON_TARGET, scenario.get_tstart())[1]\n",
    "\n",
    "# Do the weighting\n",
    "weights2 = np.zeros(data2.shape[1])\n",
    "weights2[:n] = 1 / (np.mean(np.std(data2[:, :n], axis=0))*n)\n",
    "weights2[n:2*n] = 1 / (np.mean(np.std(data2[:, n:2*n], axis=0))*n)\n",
    "weights2[2*n:] = 1 / np.std(data2[:, 2*n:], axis=0)\n",
    "data2 = data2 * weights2\n",
    "weights2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Nz = len(data2) // 4\n",
    "N = len(data2) - Nz\n",
    "Nw = 5000\n",
    "print(N, Nz, Nw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_show_svd(data2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test for different values of d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_method(\"cutin_loocv\", data2, 1, 8, beta=.2, seed=0, overwrite=OVERWRITE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test multiple times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dmin, dmax = 2, 7\n",
    "beta2 = 0.25\n",
    "s1, s2 = test_multiple_times(\"cutin_loocv\", data2, dmin, dmax, overwrite=OVERWRITE)\n",
    "boxplots(s1 + beta2*(s1-s2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation of scoring measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dreal = 3\n",
    "old_few2, old_many2, self_few2 = validation_scoring(\"cutin_loocv\", data2, dreal, dmin, \n",
    "                                                 dmax, overwrite=OVERWRITE)\n",
    "plot_correlation(old_many2, old_few2, self_few2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dreal = 7\n",
    "old_few3, old_many3, self_few3 = validation_scoring(\"cutin_loocv\", data2, dreal, dmin, \n",
    "                                                    dmax, overwrite=OVERWRITE)\n",
    "plot_correlation(old_many3, old_few3, self_few3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plots for papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FOLDER = os.path.join(\"..\", \"..\", \"20210301 Scenario generation\", \"figs\")\n",
    "\n",
    "def tikz_save(name):\n",
    "    save(os.path.join(FOLDER, \"{:s}.tikz\".format(name)),\n",
    "         axis_width='\\\\figurewidth', axis_height='\\\\figureheight',\n",
    "         extra_axis_parameters=[\"xticklabel style={align=center}\",\n",
    "                                \"yticklabel style={/pgf/number format/fixed,\"+\n",
    "                                \"/pgf/number format/precision=3}\",\n",
    "                                \"scaled y ticks=false\"])\n",
    "\n",
    "old_manys = [old_many1, old_many2]\n",
    "old_fews = [old_few1, old_few2]\n",
    "self_fews = [self_few1, self_few2]\n",
    "betas = [beta1, beta2]\n",
    "symbols = [\"s\", \"D\", \"X\"]\n",
    "linestyles = [\"-\", \"--\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot scores for both methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (old_few, self_few, beta) in enumerate(zip(old_fews, self_fews, betas)):\n",
    "    f, ax = plt.subplots()\n",
    "    score = np.median(old_few + beta*(old_few - self_few), axis=0)\n",
    "    dmax = old_few.shape[1] + 2 - dmin\n",
    "    ax.plot(score, \"o\", c=(0, 0, 0))\n",
    "    ax.set_xticks(np.arange(0, dmax-dmin+2))\n",
    "    ax.set_xticklabels([r\"Training\\\\set\"] + \n",
    "                       [r\"\\$\\dimension={:d}\\$\".format(d) for d in range(dmin, dmax+1)])\n",
    "    ax.set_ylabel(r\"\\$\\proposedmetric{\\scenariosetgenerated}{\\scenariosettest}{\\scenarioset}\\$\")\n",
    "    tikz_save(\"scores_{:d}\".format(i+1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot \"true\" Wasserstein"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for i, (old_many, old_few, self_few) in enumerate(zip(old_manys, old_fews, self_fews)):\n",
    "    f, ax = plt.subplots()\n",
    "    dmax = old_few.shape[1] + 2 - dmin\n",
    "    # (s) square = \"true\" Wasserstein\n",
    "    # (D) diamond = empirical Wasserstein\n",
    "    # (X) cross = \"penalty\"\n",
    "    for x, s in zip([old_many, old_few, old_few-self_few], symbols):\n",
    "        ax.plot(np.median(x, axis=0), marker=s, c=(0, 0, 0), linestyle=\"\")\n",
    "    ax.set_xticks(np.arange(0, dmax-dmin+2))\n",
    "    ax.set_xticklabels([r\"Training\\\\set\"] + \n",
    "                       [r\"\\$\\dimension={:d}\\$\".format(d) for d in range(dmin, dmax+1)])\n",
    "    ax.set_ylabel(\"Wasserstein metric and penalty\")\n",
    "    tikz_save(\"wasserstein_{:d}\".format(i+1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots()\n",
    "# solid = lead vehicle decelerating\n",
    "# dashed = cut-in\n",
    "for old_many, old_few, self_few, ls in zip(old_manys, old_fews, self_fews, linestyles):\n",
    "    beta = np.linspace(0, 1, 100)\n",
    "    correlation = np.zeros(len(beta))\n",
    "    for i in range(len(beta)):\n",
    "        new_score = old_few + beta[i]*(old_few - self_few)\n",
    "        correlation[i] = np.corrcoef(np.median(old_many, axis=0),\n",
    "                                     np.median(new_score, axis=0))[0, 1]\n",
    "    ax.plot(beta, correlation, c=(0, 0, 0), linestyle=ls)\n",
    "ax.set_xlim(0, 1)\n",
    "ax.set_xlabel(r\"\\$\\penaltyweight\\$\")\n",
    "ax.set_ylabel(\"Correlation\")\n",
    "tikz_save(\"correlation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
