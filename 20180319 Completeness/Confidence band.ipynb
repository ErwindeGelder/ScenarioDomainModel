{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "from fastkde import KDE\n",
    "from gaussianmixture import GaussianMixture\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import scipy.stats\n",
    "import h5py\n",
    "import os\n",
    "from matplotlib2tikz import save\n",
    "import time\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Show KDE with confidence intervals univariate case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "seed = 0\n",
    "ndatapoints = 500\n",
    "confidence = 0.95\n",
    "nrepeat = 1000  # Number of repeats for the bootstrap\n",
    "npdf = 101\n",
    "overwrite = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create object for generating data from a Gaussian mixture\n",
    "xlim = [-3, 3]\n",
    "gm = GaussianMixture([-1, 1], [0.5, 0.3])\n",
    "gmb = GaussianMixture([-0.5, 0.5, 1.5], [0.3, 0.5, 0.3])\n",
    "(xpdf,), ypdf = gm.pdf(minx=[xlim[0]], maxx=[xlim[1]], n=npdf)\n",
    "_, ypdfb = gmb.pdf(minx=[xlim[0]], maxx=[xlim[1]], n=npdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a figure of the distribution and export to tikz\n",
    "f, ax = plt.subplots(1, 1, figsize=(7, 4))\n",
    "ax.plot(xpdf, ypdf, lw=5)\n",
    "ax.plot(xpdf, ypdfb, lw=5)\n",
    "ax.grid(True)\n",
    "ax.set_xlabel('$x$')\n",
    "ax.set_ylabel('$f(x)$')\n",
    "ax.set_xlim(xlim)\n",
    "save(os.path.join('..', '20181002 Completeness question', 'figures', 'true_pdf.tikz'),\n",
    "     figureheight='\\\\figureheight', figurewidth='\\\\figurewidth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plug-in method\n",
    "\n",
    "The uncertainty can be approximated by $\\sqrt{\\frac{\\mu_K}{n h^d} p(x)}$ with:\n",
    "\n",
    "- $\\mu_K$ is a constant that only depends on the chosen Kernel: $\\mu_K = \\int K(x)^2 dx$ where the integral is taken over all $x$.\n",
    "- $n$ represents the number of datapoints\n",
    "- $h$ denotes the bandwidth\n",
    "- $d$ denotes the dimension of $x$\n",
    "- $p(x)$ is the real probability density at $x$\n",
    "\n",
    "In reality, the uncertainty also depends on terms that are proportional with higher order terms of $h$. It is assumed that $h$ is sufficiently small such that these terms can be ignored.\n",
    "\n",
    "The plug-in method simply replaces $p(x)$ with the estimated probability density function $\\hat{p}(x)$, where $\\hat{p}(x)$ is estimated using Kernel Density Estimation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the plug-in method\n",
    "np.random.seed(seed)\n",
    "x = gm.generate_samples(ndatapoints)\n",
    "kde = KDE(data=x)\n",
    "kde.compute_bw()  # Compute the bandwidth using one-leave-out cross validation\n",
    "bandwidth = kde.bw  # Store the bandwidth for later usage\n",
    "print(\"Bandwidth: {:.5f}\".format(bandwidth))\n",
    "ypdf_estimated = kde.score_samples(xpdf)\n",
    "low_plugin, up_plugin = kde.confidence_interval(xpdf, confidence=confidence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bootstrap\n",
    "\n",
    "With bootstrapping, we create $N$ artifical datasets by taking each time $n$ datapoints from the original dataset with replacement. In total, the real PDF $p(x)$ is $N$ times estimated. \n",
    "\n",
    "The first bootstrap method (bootstrap and plug-in approach) estimates the variance of the estimated probability at $x$ using the $N$ estimates. This variance is used to compose the confidence intervals.\n",
    "\n",
    "The second bootstrap method (simply called Bootstrap) directly composes the confedence intervals by taking the appropriate percentiles of the estimated probabilities at each value of $x$.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the bootstrap method for determining the confidence interval\n",
    "np.random.seed(seed)\n",
    "filename = os.path.join(\"hdf5\", \"pdfs_bootstrap.hdf5\")\n",
    "if overwrite or not os.path.exists(filename):\n",
    "    pdfs_bootstrap = np.zeros((nrepeat, len(xpdf)))\n",
    "    for i in tqdm(range(nrepeat)):\n",
    "        kde = KDE(data=x[np.random.choice(len(x), size=len(x), replace=True)], bw=bandwidth)\n",
    "        pdfs_bootstrap[i] = kde.score_samples(xpdf)\n",
    "        \n",
    "    # Store the PDFs\n",
    "    with h5py.File(filename, \"w\") as f:\n",
    "        f.create_dataset(\"pdfs\", data=pdfs_bootstrap)\n",
    "else:\n",
    "    with h5py.File(filename, \"r\") as f:\n",
    "        pdfs_bootstrap = f[\"pdfs\"][:]\n",
    "        \n",
    "# The first bootstrap method (bootstrap and plug-in approach)\n",
    "std = np.std(pdfs_bootstrap, axis=0)\n",
    "zvalue = scipy.stats.norm.ppf(confidence/2+0.5)\n",
    "low_bootstrap1 = ypdf_estimated - zvalue*std  # \"Bootstrap and plug-in approach\"\n",
    "up_bootstrap1 = ypdf_estimated + zvalue*std\n",
    "\n",
    "# The second bootstrap method (simply called \"bootstrap\")\n",
    "deviation = np.percentile(np.abs(pdfs_bootstrap - np.mean(pdfs_bootstrap, axis=0)), confidence*100, axis=0)\n",
    "low_bootstrap2 = ypdf_estimated - deviation\n",
    "up_bootstrap2 = ypdf_estimated + deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform the KDE many times to see the real uncertainty\n",
    "np.random.seed(seed)\n",
    "pdfs = np.zeros((nrepeat, len(xpdf)))\n",
    "filename = os.path.join(\"hdf5\", \"pdfs.hdf5\")\n",
    "if overwrite or not os.path.exists(filename):\n",
    "    for i in tqdm(range(nrepeat)):\n",
    "        x = gm.generate_samples(ndatapoints)\n",
    "        kde = KDE(data=x, bw=bandwidth)\n",
    "        kde.compute_bw(min_bw=0.05, max_bw=0.5)\n",
    "        pdfs[i] = kde.score_samples(xpdf)\n",
    "        \n",
    "    # Store the PDFs\n",
    "    with h5py.File(filename, \"w\") as f:\n",
    "        f.create_dataset(\"pdfs\", data=pdfs)\n",
    "else:\n",
    "    with h5py.File(filename, \"r\") as f:\n",
    "        pdfs = f[\"pdfs\"][:]\n",
    "\n",
    "low_real = np.percentile(pdfs, (1-confidence)*50, axis=0)\n",
    "up_real = np.percentile(pdfs, (1+confidence)*50, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(1, 1, figsize=(16, 10))\n",
    "\n",
    "# Plot the results\n",
    "plt_pdf, = ax.plot(xpdf, ypdf)\n",
    "plt_estimated, = ax.plot(xpdf, ypdf_estimated, '-.')\n",
    "plt_plugin = ax.plot(np.array([xpdf, xpdf]).T, np.array([low_plugin, up_plugin]).T, '--', color='#ff8080')\n",
    "plt_bootstrap1 = ax.plot(np.array([xpdf, xpdf]).T, np.array([low_bootstrap1, up_bootstrap1]).T, '--', color='#80ff80')\n",
    "plt_bootstrap2 = ax.plot(np.array([xpdf, xpdf]).T, np.array([low_bootstrap2, up_bootstrap2]).T, '--', color='#8080ff')\n",
    "plt_realconf = ax.fill_between(xpdf, low_real, up_real, facecolor=[.6, .6, .6], alpha=.5)\n",
    "ax.legend([plt_pdf, plt_estimated, plt_plugin[0], plt_bootstrap1[0], plt_bootstrap2[0], plt_realconf], \n",
    "          ['Real', 'Estimated', '{:.0f}\\% Confidence (plug-in)'.format(confidence*100),\n",
    "           '{:.0f}\\% Confidence (bootstrap and plug-in)'.format(confidence*100),\n",
    "           '{:.0f}\\% Confidence (bootstrap)'.format(confidence*100),\n",
    "           '{:.0f}\\% Confidence (real)'.format(confidence*100)])\n",
    "ax.grid(True)\n",
    "_ = ax.set_xlim(xlim)  # The \"_ =\" suppresses the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(1, 1, figsize=(16, 10))\n",
    "\n",
    "# Plot the results\n",
    "ax.plot(xpdf, ypdf)\n",
    "ax.plot(xpdf, ypdf_estimated, '-.')\n",
    "ax.plot(np.array([xpdf, xpdf]).T, np.array([low_plugin, up_plugin]).T, '--', color='#ff8080')\n",
    "ax.fill_between(xpdf, low_real, up_real, facecolor=[.6, .6, .6], alpha=.5)\n",
    "ax.grid(True)\n",
    "_ = ax.set_xlim(xlim)  # The \"_ =\" suppresses the output\n",
    "save(os.path.join('..', 'progress_reports', 'report10', 'confidence_interval.tikz'),\n",
    "     figureheight='\\\\figureheight', figurewidth='\\\\figurewidth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Show confidence interval multivariate case (2-d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "seed = 0\n",
    "ndatapoints = 500\n",
    "confidence = 0.95\n",
    "nrepeat = 1000  # Number of repeats for the bootstrap\n",
    "factor_of_max = 0.1  # Control height of PDF for contour plot\n",
    "npdf = 101\n",
    "overwrite = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create object for generating data from a Gaussian mixture\n",
    "x1lim, x2lim = [-3, 3], [-3, 3]\n",
    "gm2 = GaussianMixture([[-1, -1], [1, 1]], [[[0.5, 0.25], [0.25, 0.4]], [[0.5, -0.25], [-0.25, 0.4]]])\n",
    "(x1pdf, x2pdf), ypdf = gm2.pdf(minx=[x1lim[0], x2lim[0]], maxx=[x1lim[1], x2lim[1]], n=npdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the plug-in method\n",
    "np.random.seed(seed)\n",
    "x = gm2.generate_samples(ndatapoints)\n",
    "kde = KDE(data=x)\n",
    "kde.compute_bw()  # Compute the bandwidth using one-leave-out cross validation\n",
    "bandwidth = kde.bw  # Store the bandwidth for later usage\n",
    "print(\"Bandwidth: {:.5f}\".format(bandwidth))\n",
    "xpdf2 = np.transpose(np.array([x1pdf, x2pdf]), [1, 2, 0])\n",
    "ypdf_estimated = kde.score_samples(xpdf2)\n",
    "low_plugin2, up_plugin2 = kde.confidence_interval(xpdf2, confidence=confidence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Use the bootstrap method for determining the confidence interval\n",
    "np.random.seed(seed)\n",
    "filename = os.path.join(\"hdf5\", \"pdfs_bootstrap2.hdf5\")\n",
    "pdfs_bootstrap2 = np.zeros((nrepeat, len(x1pdf), len(x2pdf)))\n",
    "\n",
    "if overwrite or not os.path.exists(filename):\n",
    "    for i in tqdm(range(nrepeat)):\n",
    "        kde = KDE(data=x[np.random.choice(len(x), size=len(x), replace=True)], bw=bandwidth)\n",
    "        pdfs_bootstrap2[i] = kde.score_samples(xpdf2)\n",
    "\n",
    "    # Store the PDFs\n",
    "    with h5py.File(filename, \"w\") as f:\n",
    "        f.create_dataset(\"pdfs\", data=pdfs_bootstrap2)\n",
    "else:\n",
    "    with h5py.File(filename, \"r\") as f:\n",
    "        pdfs_bootstrap2 = f[\"pdfs\"][:]\n",
    "        \n",
    "std = np.std(pdfs_bootstrap2, axis=0)\n",
    "zvalue = scipy.stats.norm.ppf(confidence/2+0.5)\n",
    "low_bootstrap1_2 = ypdf_estimated - zvalue*std  # \"Bootstrap and plug-in approach\"\n",
    "up_bootstrap1_2 = ypdf_estimated + zvalue*std\n",
    "deviation = np.percentile(np.abs(pdfs_bootstrap2 - np.mean(pdfs_bootstrap2, axis=0)), confidence*100, axis=0)\n",
    "low_bootstrap2_2 = ypdf_estimated - deviation\n",
    "up_bootstrap2_2 = ypdf_estimated + deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform the KDE many times to see the real uncertainty\n",
    "np.random.seed(seed)\n",
    "filename = os.path.join(\"hdf5\", \"pdfs2.hdf5\")\n",
    "pdfs2 = np.zeros((nrepeat, len(x1pdf), len(x2pdf)))\n",
    "\n",
    "if overwrite or not os.path.exists(filename):\n",
    "    for i in tqdm(range(nrepeat)):\n",
    "        x = gm2.generate_samples(ndatapoints)\n",
    "        kde = KDE(data=x)\n",
    "        kde.compute_bw(min_bw=0.05, max_bw=0.5)\n",
    "        pdfs2[i] = kde.score_samples(xpdf2)\n",
    "\n",
    "    # Store the PDFs\n",
    "    with h5py.File(filename, \"w\") as f:\n",
    "        f.create_dataset(\"pdfs\", data=pdfs2)\n",
    "else:\n",
    "    with h5py.File(filename, \"r\") as f:\n",
    "        pdfs2 = f[\"pdfs\"][:]\n",
    "        \n",
    "low_real2 = np.percentile(pdfs2, (1-confidence)*50, axis=0)\n",
    "up_real2 = np.percentile(pdfs2, (1+confidence)*50, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(1, 1, figsize=(16, 10))\n",
    "\n",
    "# Plot the result of the plug-in method\n",
    "ycontour = np.max(ypdf)*factor_of_max\n",
    "plt.contour(x1pdf, x2pdf, ypdf, ycontour, colors=plt_pdf.get_color())\n",
    "plt.contour(x1pdf, x2pdf, ypdf_estimated, ycontour, colors=plt_estimated.get_color())\n",
    "plt.contour(x1pdf, x2pdf, low_plugin2, ycontour, linestyles='dashed', colors=plt_plugin[0].get_color())\n",
    "plt.contour(x1pdf, x2pdf, up_plugin2, ycontour, linestyles='dashed', colors=plt_plugin[0].get_color())\n",
    "plt.contour(x1pdf, x2pdf, low_bootstrap1_2, ycontour, linestyles='dashed', colors=plt_bootstrap1[0].get_color())\n",
    "plt.contour(x1pdf, x2pdf, up_bootstrap1_2, ycontour, linestyles='dashed', colors=plt_bootstrap1[0].get_color())\n",
    "plt.contour(x1pdf, x2pdf, low_bootstrap2_2, ycontour, linestyles='dashed', colors=plt_bootstrap2[0].get_color())\n",
    "plt.contour(x1pdf, x2pdf, up_bootstrap2_2, ycontour, linestyles='dashed', colors=plt_bootstrap2[0].get_color())\n",
    "plt.contourf(x1pdf, x2pdf, up_real2, [ycontour, 10000], colors=plt_realconf.get_facecolor())\n",
    "plt.contourf(x1pdf, x2pdf, low_real2, [ycontour, 10000], colors=[[1, 1, 1, 1]])\n",
    "conf_patch = mpatches.Patch(color=plt_realconf.get_facecolor()[0])  # Because plt.contourf cannot be used as legend handle\n",
    "plt.xlabel('$x_1$')\n",
    "plt.ylabel('$x_2$')\n",
    "plt.title(\"Contour plots at p = {:.3f} (p = pmax * {:.0f}%)\".format(ycontour, factor_of_max*100))\n",
    "plt.grid(True)\n",
    "ax.legend([plt_pdf, plt_estimated, plt_plugin[0], plt_bootstrap1[0], plt_bootstrap2[0], conf_patch], \n",
    "          ['Real', 'Estimated', '{:.0f}% Confidence (plug-in)'.format(confidence*100),\n",
    "           '{:.0f}% Confidence (bootstrap and plug-in)'.format(confidence*100),\n",
    "           '{:.0f}% Confidence (bootstrap)'.format(confidence*100),\n",
    "           '{:.0f}% Confidence (real)'.format(confidence*100)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estimated MISE for increasing $n$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "nmax = 1000\n",
    "nmin = 100\n",
    "nstep = 100\n",
    "seed = 1\n",
    "overwrite = True\n",
    "nrepeat = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate datapoints\n",
    "np.random.seed(seed)\n",
    "X = gm.generate_samples(nmax*nrepeat).reshape((nrepeat, nmax))\n",
    "(xpdf,), ypdf = gm.pdf(minx=[xlim[0]], maxx=[xlim[1]], n=npdf)\n",
    "nn = np.arange(nmin, nmax+nstep, nstep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = os.path.join(\"hdf5\", \"mise.hdf5\")\n",
    "if overwrite or not os.path.exists(filename):\n",
    "    kde_estimated = np.zeros((len(nn), nrepeat, len(xpdf)))\n",
    "    bw = np.zeros(len(nn))\n",
    "    \n",
    "    # We use the first set of datapoints (i.e., X[0]) for determining the bandwidth\n",
    "    # This has quite some influence on the remainder of the procedure...\n",
    "    i_bw = 0\n",
    "    \n",
    "    # For now, just set the data for all the kdes\n",
    "    print(\"Initialize all KDEs\")\n",
    "    kdes = [KDE(data=x) for x in X]\n",
    "    \n",
    "    print(\"Estimate for different number of datapoints {:d} pdfs\".format(nrepeat))\n",
    "    for i, n in enumerate(tqdm(nn)):\n",
    "        # Compute the optimal bandwidth using 1-leave-out\n",
    "        kdes[i_bw].set_n(n)\n",
    "        kdes[i_bw].compute_bw()\n",
    "        bw[i] = kdes[i_bw].bw\n",
    "        \n",
    "        # Compute all the nrepeats pdfs. We need to do this many times in order to determine the real MISE\n",
    "        for j in range(nrepeat):\n",
    "            kdes[j].set_n(n)\n",
    "            kdes[j].set_bw(bw[i])\n",
    "            kde_estimated[i, j] = kdes[j].score_samples(xpdf)\n",
    "    \n",
    "    # Write results to hdf5 file\n",
    "    with h5py.File(filename, \"w\") as f:\n",
    "        f.create_dataset(\"kde_estimated\", data=kde_estimated)\n",
    "        f.create_dataset(\"xpdf\", data=xpdf)\n",
    "        f.create_dataset(\"ypdf\", data=ypdf)\n",
    "        f.create_dataset(\"bw\", data=bw)\n",
    "else:\n",
    "    with h5py.File(filename, \"r\") as f:\n",
    "        kde_estimated = f[\"kde_estimated\"][:]\n",
    "        xpdf = f[\"xpdf\"][:]\n",
    "        ypdf = f[\"ypdf\"][:]\n",
    "        bw = f[\"bw\"][:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "real_mise = np.zeros(len(nn))\n",
    "est_mise = np.zeros((len(nn), nrepeat))\n",
    "real_mise_corr = np.zeros_like(real_mise)\n",
    "est_mise_corr = np.zeros_like(est_mise)\n",
    "dx = np.mean(np.gradient(xpdf))\n",
    "muk = 1 / (2 * np.sqrt(np.pi))\n",
    "for i, (y, h, n) in enumerate(zip(kde_estimated, bw, nn)):\n",
    "    real_mise[i] = np.trapz(np.mean((y - ypdf)**2, axis=0), xpdf)\n",
    "    laplacian = np.gradient(np.gradient(y, axis=1), axis=1) / dx**2\n",
    "    integral_laplacian = np.trapz(laplacian**2, xpdf)\n",
    "    est_mise[i] = muk / (n * h) + integral_laplacian * h**4 / 4\n",
    "    est_mise_corr[i] = muk / (n * h)\n",
    "    bias = laplacian*h**2/2\n",
    "    real_mise_corr[i] = np.trapz(np.mean((y - bias - ypdf)**2, axis=0), xpdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 4))\n",
    "for ax, real, est in zip([ax1, ax2], [real_mise, real_mise_corr], [est_mise, est_mise_corr]):\n",
    "    ax.loglog(nn, real, label=\"Real MISE\")\n",
    "    ax.loglog(nn, np.mean(est, axis=1), label=\"Est MISE\")\n",
    "    ax.set_xlabel(\"Number of samples\")\n",
    "    ax.set_ylabel(\"MISE\")\n",
    "    ax.legend()\n",
    "ax1.set_title(\"Not corrected\")\n",
    "ax2.set_title(\"Corrected\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plot again the real MISE and the estimated MISE and export plot to tikz\n",
    "f, ax = plt.subplots(1, 1, figsize=(7, 5))\n",
    "ax.loglog(nn, real_mise, label=\"Real MISE\", lw=5)\n",
    "ax.loglog(nn, est_mise[:, 0], label=\"Estimated MISE\", lw=5)\n",
    "ax.set_xlabel(\"Number of samples\")\n",
    "ax.set_ylabel(\"MISE\")\n",
    "ax.legend()\n",
    "ax.grid(True)\n",
    "ax.set_xlim([np.min(nn), np.max(nn)])\n",
    "save(os.path.join('..', '20181002 Completeness question', 'figures', 'mise_example.tikz'),\n",
    "     figureheight='\\\\figureheight', figurewidth='\\\\figurewidth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot again the real MISE and the estimated MISE and export plot to tikz\n",
    "f, ax = plt.subplots(1, 1, figsize=(7, 5))\n",
    "est_mean = np.mean(est_mise, axis=1)\n",
    "est_std = np.std(est_mise, axis=1)\n",
    "ax.loglog(nn, real_mise, label=\"Real MISE\", lw=5)\n",
    "p = ax.loglog(nn, est_mean, label=\"Estimated MISE\", lw=5)\n",
    "ax.loglog(nn, est_mean+2*est_std, '--', color=p[0].get_color(), lw=5)\n",
    "ax.loglog(nn, est_mean-2*est_std, '--', color=p[0].get_color(), lw=5)\n",
    "ax.set_xlabel(\"Number of samples\")\n",
    "ax.set_ylabel(\"MISE\")\n",
    "ax.legend()\n",
    "ax.grid(True)\n",
    "ax.set_xlim([np.min(nn), np.max(nn)])\n",
    "save(os.path.join('..', '20181002 Completeness question', 'figures', 'mise_example2.tikz'),\n",
    "     figureheight='\\\\figureheight', figurewidth='\\\\figurewidth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Show that MISE can be used to use for two distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "nmax = 5000\n",
    "nmin = 100\n",
    "nstep = 100\n",
    "seed = 0\n",
    "nrepeat = 100\n",
    "overwrite = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate data\n",
    "np.random.seed(seed)\n",
    "A = gm.generate_samples(nmax*nrepeat).reshape((nrepeat, nmax))\n",
    "B = gmb.generate_samples(nmax*nrepeat).reshape((nrepeat, nmax))\n",
    "AB = np.concatenate((A[:, :, np.newaxis], B[:, :, np.newaxis]), axis=2)\n",
    "nn = np.arange(nmin, nmax+nstep, nstep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the real pdfs\n",
    "(xpdf,), ypdfa = gm.pdf(minx=[xlim[0]], maxx=[xlim[1]], n=npdf)\n",
    "_, ypdfb = gmb.pdf(minx=[xlim[0]], maxx=[xlim[1]], n=npdf)\n",
    "ypdfab = np.dot(ypdfb[:, np.newaxis], ypdfa[np.newaxis, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "filename = os.path.join(\"hdf5\", \"mise_2d.hdf5\")\n",
    "if overwrite or not os.path.exists(filename):\n",
    "    pdfa_est = np.zeros((len(nn), nrepeat, len(xpdf)))\n",
    "    pdfb_est = np.zeros_like(pdfa_est)\n",
    "    pdfab_est = np.zeros((len(nn), nrepeat, len(xpdf), len(xpdf)))\n",
    "    bwa = np.zeros(len(nn))\n",
    "    bwb = np.zeros_like(bwa)\n",
    "    bwab = np.zeros_like(bwa)\n",
    "    \n",
    "    # We use the first set of datapoints (i.e., X[0]) for determining the bandwidth\n",
    "    # This has quite some influence on the remainder of the procedure...\n",
    "    i_bw = 0\n",
    "    \n",
    "    # For now, just set the data for all the kdes\n",
    "    print(\"Initialize all {:d} KDEs\".format(nrepeat))\n",
    "    kdesa = [KDE(data=x) for x in A]\n",
    "    kdesb = [KDE(data=x) for x in B]\n",
    "    kdesab = [KDE(data=x) for x in AB]\n",
    "    for kdea, kdeb, kdeab in tqdm(zip(kdesa, kdesb, kdesab)):\n",
    "        kdea.set_score_samples(xpdf)\n",
    "        kdeb.set_score_samples(xpdf)\n",
    "        kdeab.set_score_samples(xpdf2)\n",
    "    \n",
    "    # Compute the bandwidths\n",
    "    print(\"Compute the bandwidths\")\n",
    "    for i, n in enumerate(tqdm(nn)):\n",
    "        # Compute the optimal bandwidth using 1-leave-out\n",
    "        kdesa[i_bw].set_n(n)\n",
    "        kdesa[i_bw].compute_bw()\n",
    "        bwa[i] = kdesa[i_bw].bw\n",
    "        kdesb[i_bw].set_n(n)\n",
    "        kdesb[i_bw].compute_bw()\n",
    "        bwb[i] = kdesb[i_bw].bw\n",
    "        kdesab[i_bw].set_n(n)\n",
    "        kdesab[i_bw].compute_bw()\n",
    "        bwab[i] = kdesab[i_bw].bw\n",
    "        \n",
    "    # Compute all the nrepeats pdfs. We need to do this many times in order to determine the real MISE.\n",
    "    print(\"Estimate pdfs\")\n",
    "    for j in tqdm(range(nrepeat)):\n",
    "        for i, n in enumerate(nn):\n",
    "            kdesa[j].set_n(n)\n",
    "            kdesa[j].set_bw(bwa[i])\n",
    "            pdfa_est[i, j] = kdesa[j].score_samples()\n",
    "            kdesb[j].set_n(n)\n",
    "            kdesb[j].set_bw(bwb[i])\n",
    "            pdfb_est[i, j] = kdesb[j].score_samples()\n",
    "            kdesab[j].set_n(n)\n",
    "            kdesab[j].set_bw(bwab[i])\n",
    "            pdfab_est[i, j] = kdesab[j].score_samples()\n",
    "    \n",
    "    # Write results to hdf5 file\n",
    "    with h5py.File(filename, \"w\") as f:\n",
    "        f.create_dataset(\"xpdf\", data=xpdf)\n",
    "        f.create_dataset(\"xpdf2\", data=xpdf2)\n",
    "        f.create_dataset(\"pdfa_est\", data=pdfa_est)\n",
    "        f.create_dataset(\"pdfb_est\", data=pdfb_est)\n",
    "        f.create_dataset(\"pdfab_est\", data=pdfab_est)\n",
    "        f.create_dataset(\"bwa\", data=bwa)\n",
    "        f.create_dataset(\"bwb\", data=bwb)\n",
    "        f.create_dataset(\"bwab\", data=bwab)\n",
    "else:\n",
    "    with h5py.File(filename, \"r\") as f:\n",
    "        xpdf = f[\"xpdf\"][:]\n",
    "        xpdf2 = f[\"xpdf2\"][:]\n",
    "        pdfa_est = f[\"pdfa_est\"][:]\n",
    "        pdfb_est = f[\"pdfb_est\"][:]\n",
    "        pdfab_est = f[\"pdfab_est\"][:]\n",
    "        bwa = f[\"bwa\"][:]\n",
    "        bwb = f[\"bwb\"][:]\n",
    "        bwab = f[\"bwab\"][:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the real MISE and estimate the MISE\n",
    "real_mise_dep = np.zeros(len(nn))  # Real MISE for multivariate KDE, i.e., data assumed to be dependent\n",
    "est_mise_dep = np.zeros((len(nn), nrepeat))\n",
    "real_mise_ind = np.zeros_like(real_mise_dep)  # Real MISE for when data is assumed to be independent\n",
    "est_mise_ind = np.zeros_like(est_mise_dep)\n",
    "muka = kdesa[0].muk\n",
    "mukab = kdesab[0].muk\n",
    "for i, (y, h, n) in enumerate(zip(pdfab_est, bwab, nn)):\n",
    "    real_mise_dep[i] = np.trapz(np.trapz(np.mean((y - ypdfab)**2, axis=0), xpdf), xpdf)\n",
    "    laplacian = (np.gradient(np.gradient(y, axis=1), axis=1) + \n",
    "                 np.gradient(np.gradient(y, axis=2), axis=2)) / dx**2\n",
    "    integral_laplacian = np.trapz(np.trapz(laplacian**2, xpdf), xpdf)\n",
    "    est_mise_dep[i] = mukab / (n * h**2) + integral_laplacian * h**4 / 4\n",
    "    \n",
    "for i, (ya, yb, ha, hb, n) in enumerate(zip(pdfa_est, pdfb_est, bwa, bwb, nn)):\n",
    "    # Estimate the real MISE\n",
    "    pdfab_ind = np.zeros((len(ya), len(xpdf), len(xpdf)))\n",
    "    for j, (a, b) in enumerate(zip(ya, yb)):\n",
    "        pdfab_ind[j] = np.dot(b[:, np.newaxis], a[np.newaxis, :])\n",
    "    real_mise_ind[i] = np.trapz(np.trapz(np.mean((pdfab_ind - ypdfab)**2, axis=0), xpdf), xpdf)\n",
    "    \n",
    "    # Compute the MISE for pdf a\n",
    "    laplaciana = np.gradient(np.gradient(ya, axis=1), axis=1) / dx**2\n",
    "    integrala = np.trapz(laplaciana**2, xpdf)\n",
    "    est_misea = muka / (n * ha) + integrala * ha**4 / 4\n",
    "    \n",
    "    # Compute the MISE for pdf b\n",
    "    laplacianb = np.gradient(np.gradient(yb, axis=1), axis=1) / dx**2\n",
    "    integralb = np.trapz(laplacianb**2, xpdf)\n",
    "    est_miseb = muka / (n * hb) + integrala * hb**4 / 4\n",
    "    \n",
    "    # Compute the integrals of the squared probabilities\n",
    "    integral_sqya = np.trapz(ya**2, xpdf)\n",
    "    integral_sqyb = np.trapz(yb**2, xpdf)\n",
    "    \n",
    "    # Finally, estimate the MISE\n",
    "    est_mise_ind[i] = integral_sqyb*est_misea + integral_sqya*est_miseb + est_misea*est_miseb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(1, 1, figsize=(7, 5))\n",
    "ax.loglog(nn, real_mise_dep, lw=5, label=\"Real MISE\")\n",
    "ax.loglog(nn, est_mise_dep[:, 0], lw=5, label=\"Estimated MISE\")\n",
    "ax.set_xlabel(\"Number of samples\")\n",
    "ax.set_ylabel(\"MISE\")\n",
    "ax.legend()\n",
    "ax.grid(True)\n",
    "ax.set_xlim([np.min(nn), np.max(nn)])\n",
    "ax.set_title(\"Assuming that data is dependent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(1, 1, figsize=(7, 5))\n",
    "ax.loglog(nn, real_mise_ind, lw=5, label=\"Real MISE\")\n",
    "ax.loglog(nn, est_mise_ind[:, 0], lw=5, label=\"Estimated MISE\")\n",
    "ax.set_xlabel(\"Number of samples\")\n",
    "ax.set_ylabel(\"MISE\")\n",
    "ax.legend()\n",
    "ax.grid(True)\n",
    "ax.set_xlim([np.min(nn), np.max(nn)])\n",
    "ax.set_title(\"Assuming that data is independent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Show whether it is \"better\" to use multivariate distribution or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "n = 200\n",
    "seed = 2\n",
    "nrepeat = 100\n",
    "npdf = 101\n",
    "xlim = [-3, 3]\n",
    "overwrite = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate data\n",
    "np.random.seed(seed)\n",
    "x = gm.generate_samples(n)\n",
    "y = gm.generate_samples(n)\n",
    "xy = np.concatenate((x, y), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the real pdfs\n",
    "(xpdf,), ypdf_univar = gm.pdf(minx=[xlim[0]], maxx=[xlim[1]], n=npdf)\n",
    "gm_multivar = GaussianMixture([[-1, -1], [-1, 1], [1, -1], [1, 1]], [np.diag([0.5, 0.5]), np.diag([0.5, 0.3]), \n",
    "                                                                     np.diag([0.3, 0.5]), np.diag([0.3, 0.3])])\n",
    "(x1pdf, x2pdf), ypdf_multivar = gm_multivar.pdf(minx=[xlim[0], xlim[0]], maxx=[xlim[1], xlim[1]], n=npdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create three KDE (for x, y, and xy, respectively) and estimate MISE\n",
    "kdex = KDE(data=x)\n",
    "kdey = KDE(data=y)\n",
    "kdexy = KDE(data=xy)\n",
    "print(\"Description   Bandwidth   1LO Loglikelihood         MISE\")\n",
    "for kde, description in zip([kdex, kdey, kdexy], ['kdex', 'kdey', 'kdexy']):\n",
    "    kde.compute_bw()\n",
    "    print(\"{:>11s}   {:9.3f}   {:17.5f}   {:10.5f}\".format(description, kde.bw, kde.score_leave_one_out(), \n",
    "                                                          kde.muk / (n * kde.bw**kde.d)))\n",
    "bwx = kdex.bw\n",
    "bwy = kdey.bw\n",
    "bwxy = kdexy.bw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kdex.set_bw(0.025)\n",
    "integral_laplacian_x = np.trapz((np.gradient(np.gradient(kdex.score_samples(xpdf))) / np.mean(np.gradient(xpdf))**2)**2, xpdf)\n",
    "est_bias = kdex.bw**4 / 4 * integral_laplacian_x\n",
    "est_bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kdex.set_bw(0.5)\n",
    "laplacian = np.gradient(np.gradient(kdex.score_samples(xpdf))) / np.mean(np.gradient(xpdf))**2\n",
    "bias = laplacian * kdex.bw**2 / 2\n",
    "plt.plot(xpdf, ypdf_univar, label=\"Real\")\n",
    "plt.plot(xpdf, kdex.score_samples(xpdf), label=\"Estimated\")\n",
    "plt.plot(xpdf, kdex.score_samples(xpdf) - bias, label=\"Corrected\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the real MISE\n",
    "filename = os.path.join(\"hdf5\", \"approx_mise.hdf5\")\n",
    "\n",
    "if overwrite or not os.path.exists(filename):\n",
    "    pdfsx = np.zeros((nrepeat, len(xpdf)))\n",
    "    pdfsy = np.zeros_like(pdfsx)\n",
    "    pdfsxy = np.zeros((nrepeat, len(xpdf), len(xpdf)))\n",
    "    pdfsxy2 = np.zeros((nrepeat, len(xpdf), len(xpdf)))\n",
    "    for i in tqdm(range(nrepeat)):\n",
    "        x = gm.generate_samples(n)\n",
    "        y = gm.generate_samples(n)\n",
    "        xy = np.concatenate((x, y), axis=1)\n",
    "        kdex = KDE(data=x, bw=bwx)\n",
    "        kdey = KDE(data=y, bw=bwy)\n",
    "        kdexy = KDE(data=xy, bw=bwxy)\n",
    "        pdfsx[i] = kdex.score_samples(xpdf)\n",
    "        pdfsy[i] = kdey.score_samples(xpdf)\n",
    "        pdfsxy[i] = kdexy.score_samples(xpdf2)\n",
    "        pdfsxy2[i] = np.kron(pdfsx[i], pdfsy[i][:, np.newaxis])\n",
    "    \n",
    "    # Write results to hdf5 file\n",
    "    with h5py.File(filename, \"w\") as f:\n",
    "        f.create_dataset(\"pdfsx\", data=pdfsx)\n",
    "        f.create_dataset(\"pdfsy\", data=pdfsy)\n",
    "        f.create_dataset(\"pdfsxy\", data=pdfsxy)\n",
    "        f.create_dataset(\"pdfsxy2\", data=pdfsxy2)\n",
    "else:\n",
    "    with h5py.File(filename, \"r\") as f:\n",
    "        pdfsx = f[\"pdfsx\"][:]\n",
    "        pdfsy = f[\"pdfsy\"][:]\n",
    "        pdfsxy = f[\"pdfsxy\"][:]\n",
    "        pdfsxy2 = f[\"pdfsxy2\"][:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_misex = np.trapz(np.mean((pdfsx - ypdf_univar)**2, axis=0), xpdf)\n",
    "real_misey = np.trapz(np.mean((pdfsy - ypdf_univar)**2, axis=0), xpdf)\n",
    "real_misexy = np.trapz(np.trapz(np.mean((pdfsxy - ypdf_multivar)**2, axis=0), xpdf), xpdf)\n",
    "real_misexy2 = np.trapz(np.trapz(np.mean((pdfsxy2 - ypdf_multivar)**2, axis=0), xpdf), xpdf)\n",
    "dx = np.mean(np.gradient(xpdf))\n",
    "integral_laplacian_univar = np.trapz((np.gradient(np.gradient(ypdf_univar)) / dx**2)**2, xpdf)\n",
    "integral_laplacian_multivar = np.trapz(np.trapz((np.gradient(np.gradient(ypdf_multivar, axis=0), axis=0) / dx**2 +\n",
    "                                                 np.gradient(np.gradient(ypdf_multivar, axis=1), axis=1) / dx**2)**2, \n",
    "                                                xpdf), xpdf)\n",
    "est_misex = kdex.muk / (n * bwx) + integral_laplacian_univar * bwx**4 / 4\n",
    "est_misey = kdey.muk / (n * bwy) + integral_laplacian_univar * bwy**4 / 4\n",
    "est_misexy = kdexy.muk / (n * bwxy**2) + integral_laplacian_multivar * bwxy**4 / 4\n",
    "print(\"Description  Real MISE  MISE Formula\")\n",
    "for description, real, est in zip(['kdex', 'kdey', 'kdexy', 'extra'], [real_misex, real_misey, real_misexy, real_misexy2],\n",
    "                                  [est_misex, est_misey, est_misexy, \n",
    "                                   np.trapz(ypdf_univar**2, xpdf) * (est_misex + est_misey) + est_misex*est_misey]):\n",
    "    print(\"{:>11s}  {:9.5f}  {:12.5f}\".format(description, real, est))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_misex = np.trapz(np.mean((pdfsx - ypdf_univar)**2, axis=0), xpdf)\n",
    "real_misey = np.trapz(np.mean((pdfsy - ypdf_univar)**2, axis=0), xpdf)\n",
    "real_misexy = np.trapz(np.trapz(np.mean((pdfsxy - ypdf_multivar)**2, axis=0), xpdf), xpdf)\n",
    "real_misexy2 = np.trapz(np.trapz(np.mean((pdfsxy2 - ypdf_multivar)**2, axis=0), xpdf), xpdf)\n",
    "dx = np.mean(np.gradient(xpdf))\n",
    "integral_laplacian_univar = np.trapz((np.gradient(np.gradient(ypdf_univar)) / dx**2)**2, xpdf)\n",
    "integral_laplacian_multivar = np.trapz(np.trapz((np.gradient(np.gradient(ypdf_multivar, axis=0), axis=0) / dx**2 +\n",
    "                                                 np.gradient(np.gradient(ypdf_multivar, axis=1), axis=1) / dx**2)**2, \n",
    "                                                xpdf), xpdf)\n",
    "est_misex = kdex.muk / (n * bwx) + integral_laplacian_univar * bwx**4 / 4\n",
    "est_misey = kdey.muk / (n * bwy) + integral_laplacian_univar * bwy**4 / 4\n",
    "est_misexy = kdexy.muk / (n * bwxy**2) + integral_laplacian_multivar * bwxy**4 / 4\n",
    "print(\"Description  Real MISE  MISE Formula\")\n",
    "for description, real, est in zip(['kdex', 'kdey', 'kdexy', 'extra'], [real_misex, real_misey, real_misexy, real_misexy2],\n",
    "                                  [est_misex, est_misey, est_misexy, \n",
    "                                   np.trapz(ypdf_univar**2, xpdf) * (est_misex + est_misey) + est_misex*est_misey]):\n",
    "    print(\"{:>11s}  {:9.5f}  {:12.5f}\".format(description, real, est))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
