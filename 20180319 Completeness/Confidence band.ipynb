{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "from fastkde import KDE\n",
    "from gaussianmixture import GaussianMixture\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import scipy.stats\n",
    "import h5py\n",
    "import os\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Show KDE with confidence intervals univariate case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "seed = 0\n",
    "ndatapoints = 500\n",
    "confidence = 0.95\n",
    "nrepeat = 1000  # Number of repeats for the bootstrap\n",
    "npdf = 101\n",
    "overwrite = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create object for generating data from a Gaussian mixture\n",
    "xlim = [-3, 3]\n",
    "gm = GaussianMixture([-1, 1], [0.5, 0.3])\n",
    "(xpdf,), ypdf = gm.pdf(minx=[xlim[0]], maxx=[xlim[1]], n=npdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plug-in method\n",
    "\n",
    "The uncertainty can be approximated by $\\sqrt{\\frac{\\mu_K}{n h^d} p(x)}$ with:\n",
    "\n",
    "- $mu_K$ is a constant that only depends on the chosen Kernel: $mu_K = \\int K(x)^2 dx$ where the integral is taken over all $x$.\n",
    "- $n$ represents the number of datapoints\n",
    "- $h$ denotes the bandwidth\n",
    "- $d$ denotes the dimension of $x$\n",
    "- $p(x)$ is the real probability density at $x$\n",
    "\n",
    "In reality, the uncertainty also depends on terms that are proportional with higher order terms of $h$. It is assumed that $h$ is sufficiently small such that these terms can be ignored.\n",
    "\n",
    "The plug-in method simply replaces $p(x)$ with the estimated probability density function $\\hat{p}(x)$, where $\\hat{p}(x)$ is estimated using Kernel Density Estimation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the plug-in method\n",
    "np.random.seed(seed)\n",
    "x = gm.generate_samples(ndatapoints)\n",
    "kde = KDE(data=x)\n",
    "kde.compute_bw()  # Compute the bandwidth using one-leave-out cross validation\n",
    "bandwidth = kde.bw  # Store the bandwidth for later usage\n",
    "print(\"Bandwidth: {:.5f}\".format(bandwidth))\n",
    "kde.compute_kde()\n",
    "ypdf_estimated = kde.score_samples(xpdf)\n",
    "low_plugin, up_plugin = kde.confidence_interval(xpdf, confidence=confidence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bootstrap\n",
    "\n",
    "With bootstrapping, we create $N$ artifical datasets by taking each time $n$ datapoints from the original dataset with replacement. In total, the real PDF $p(x)$ is $N$ times estimated. \n",
    "\n",
    "The first bootstrap method (bootstrap and plug-in approach) estimates the variance of the estimated probability at $x$ using the $N$ estimates. This variance is used to compose the confidence intervals.\n",
    "\n",
    "The second bootstrap method (simply called Bootstrap) directly composes the confedence intervals by taking the appropriate percentiles of the estimated probabilities at each value of $x$.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the bootstrap method for determining the confidence interval\n",
    "np.random.seed(seed)\n",
    "filename = os.path.join(\"hdf5\", \"pdfs_bootstrap.hdf5\")\n",
    "if overwrite or not os.path.exists(filename):\n",
    "    pdfs_bootstrap = np.zeros((nrepeat, len(xpdf)))\n",
    "    for i in tqdm(range(nrepeat)):\n",
    "        kde = KDE(data=x[np.random.choice(len(x), size=len(x), replace=True)], bw=bandwidth)\n",
    "        kde.compute_kde()\n",
    "        pdfs_bootstrap[i] = kde.score_samples(xpdf)\n",
    "        \n",
    "    # Store the PDFs\n",
    "    with h5py.File(filename, \"w\") as f:\n",
    "        f.create_dataset(\"pdfs\", data=pdfs_bootstrap)\n",
    "else:\n",
    "    with h5py.File(filename, \"r\") as f:\n",
    "        pdfs_bootstrap = f[\"pdfs\"][:]\n",
    "        \n",
    "# The first bootstrap method (bootstrap and plug-in approach)\n",
    "std = np.std(pdfs_bootstrap, axis=0)\n",
    "zvalue = scipy.stats.norm.ppf(confidence/2+0.5)\n",
    "low_bootstrap1 = ypdf_estimated - zvalue*std  # \"Bootstrap and plug-in approach\"\n",
    "up_bootstrap1 = ypdf_estimated + zvalue*std\n",
    "\n",
    "# The second bootstrap method (simply called \"bootstrap\")\n",
    "deviation = np.percentile(np.abs(pdfs_bootstrap - np.mean(pdfs_bootstrap, axis=0)), confidence*100, axis=0)\n",
    "low_bootstrap2 = ypdf_estimated - deviation\n",
    "up_bootstrap2 = ypdf_estimated + deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform the KDE many times to see the real uncertainty\n",
    "np.random.seed(seed)\n",
    "pdfs = np.zeros((nrepeat, len(xpdf)))\n",
    "filename = os.path.join(\"hdf5\", \"pdfs.hdf5\")\n",
    "if overwrite or not os.path.exists(filename):\n",
    "    for i in tqdm(range(nrepeat)):\n",
    "        x = gm.generate_samples(ndatapoints)\n",
    "        kde = KDE(data=x)\n",
    "        kde.compute_bw(min_bw=0.05, max_bw=0.5)\n",
    "        kde.compute_kde()\n",
    "        pdfs[i] = kde.score_samples(xpdf)\n",
    "        \n",
    "    # Store the PDFs\n",
    "    with h5py.File(filename, \"w\") as f:\n",
    "        f.create_dataset(\"pdfs\", data=pdfs)\n",
    "else:\n",
    "    with h5py.File(filename, \"r\") as f:\n",
    "        pdfs = f[\"pdfs\"][:]\n",
    "\n",
    "low_real = np.percentile(pdfs, (1-confidence)*50, axis=0)\n",
    "up_real = np.percentile(pdfs, (1+confidence)*50, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(1, 1, figsize=(16, 10))\n",
    "\n",
    "# Plot the results\n",
    "plt_pdf, = ax.plot(xpdf, ypdf)\n",
    "plt_estimated, = ax.plot(xpdf, ypdf_estimated)\n",
    "plt_plugin = ax.plot(np.array([xpdf, xpdf]).T, np.array([low_plugin, up_plugin]).T, '--', color='#ff8080')\n",
    "plt_bootstrap1 = ax.plot(np.array([xpdf, xpdf]).T, np.array([low_bootstrap1, up_bootstrap1]).T, '--', color='#80ff80')\n",
    "plt_bootstrap2 = ax.plot(np.array([xpdf, xpdf]).T, np.array([low_bootstrap2, up_bootstrap2]).T, '--', color='#8080ff')\n",
    "plt_realconf = ax.fill_between(xpdf, low_real, up_real, facecolor=[.6, .6, .6], alpha=.5)\n",
    "ax.legend([plt_pdf, plt_estimated, plt_plugin[0], plt_bootstrap1[0], plt_bootstrap2[0], plt_realconf], \n",
    "          ['Real', 'Estimated', '{:.0f}% Confidence (plug-in)'.format(confidence*100),\n",
    "           '{:.0f}% Confidence (bootstrap and plug-in)'.format(confidence*100),\n",
    "           '{:.0f}% Confidence (bootstrap)'.format(confidence*100),\n",
    "           '{:.0f}% Confidence (real)'.format(confidence*100)])\n",
    "ax.grid(True)\n",
    "_ = ax.set_xlim(xlim)  # The \"_ =\" suppresses the output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Show confidence interval multivariate case (2-d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "seed = 0\n",
    "ndatapoints = 500\n",
    "confidence = 0.95\n",
    "nrepeat = 1000  # Number of repeats for the bootstrap\n",
    "factor_of_max = 0.1  # Control height of PDF for contour plot\n",
    "npdf = 101\n",
    "overwrite = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create object for generating data from a Gaussian mixture\n",
    "x1lim, x2lim = [-3, 3], [-3, 3]\n",
    "gm2 = GaussianMixture([[-1, -1], [1, 1]], [[[0.5, 0.25], [0.25, 0.4]], [[0.5, -0.25], [-0.25, 0.4]]])\n",
    "(x1pdf, x2pdf), ypdf = gm2.pdf(minx=[x1lim[0], x2lim[0]], maxx=[x1lim[1], x2lim[1]], n=npdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the plug-in method\n",
    "np.random.seed(seed)\n",
    "x = gm2.generate_samples(ndatapoints)\n",
    "kde = KDE(data=x)\n",
    "kde.compute_bw()  # Compute the bandwidth using one-leave-out cross validation\n",
    "bandwidth = kde.bw  # Store the bandwidth for later usage\n",
    "print(\"Bandwidth: {:.5f}\".format(bandwidth))\n",
    "kde.compute_kde()\n",
    "xpdf2 = np.transpose(np.array([x1pdf, x2pdf]), [1, 2, 0])\n",
    "ypdf_estimated = kde.score_samples(xpdf2)\n",
    "low_plugin2, up_plugin2 = kde.confidence_interval(xpdf2, confidence=confidence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Use the bootstrap method for determining the confidence interval\n",
    "np.random.seed(seed)\n",
    "filename = os.path.join(\"hdf5\", \"pdfs_bootstrap2.hdf5\")\n",
    "pdfs_bootstrap2 = np.zeros((nrepeat, len(x1pdf), len(x2pdf)))\n",
    "\n",
    "if overwrite or not os.path.exists(filename):\n",
    "    for i in tqdm(range(nrepeat)):\n",
    "        kde = KDE(data=x[np.random.choice(len(x), size=len(x), replace=True)], bw=bandwidth)\n",
    "        kde.compute_kde()\n",
    "        pdfs_bootstrap2[i] = kde.score_samples(xpdf2)\n",
    "\n",
    "        # Store the PDFs\n",
    "        with h5py.File(filename, \"w\") as f:\n",
    "            f.create_dataset(\"pdfs\", data=pdfs_bootstrap2)\n",
    "else:\n",
    "    with h5py.File(filename, \"r\") as f:\n",
    "        pdfs_bootstrap2 = f[\"pdfs\"][:]\n",
    "        \n",
    "std = np.std(pdfs_bootstrap2, axis=0)\n",
    "zvalue = scipy.stats.norm.ppf(confidence/2+0.5)\n",
    "low_bootstrap1_2 = ypdf_estimated - zvalue*std  # \"Bootstrap and plug-in approach\"\n",
    "up_bootstrap1_2 = ypdf_estimated + zvalue*std\n",
    "deviation = np.percentile(np.abs(pdfs_bootstrap2 - np.mean(pdfs_bootstrap2, axis=0)), confidence*100, axis=0)\n",
    "low_bootstrap2_2 = ypdf_estimated - deviation\n",
    "up_bootstrap2_2 = ypdf_estimated + deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform the KDE many times to see the real uncertainty\n",
    "np.random.seed(seed)\n",
    "filename = os.path.join(\"hdf5\", \"pdfs2.hdf5\")\n",
    "pdfs2 = np.zeros((nrepeat, len(x1pdf), len(x2pdf)))\n",
    "\n",
    "if overwrite or not os.path.exists(filename):\n",
    "    for i in tqdm(range(nrepeat)):\n",
    "        x = gm2.generate_samples(ndatapoints)\n",
    "        kde = KDE(data=x)\n",
    "        kde.compute_bw(min_bw=0.05, max_bw=0.5)\n",
    "        kde.compute_kde()\n",
    "        pdfs2[i] = kde.score_samples(xpdf2)\n",
    "\n",
    "        # Store the PDFs\n",
    "        with h5py.File(filename, \"w\") as f:\n",
    "            f.create_dataset(\"pdfs\", data=pdfs2)\n",
    "else:\n",
    "    with h5py.File(filename, \"r\") as f:\n",
    "        pdfs2 = f[\"pdfs\"][:]\n",
    "        \n",
    "low_real2 = np.percentile(pdfs2, (1-confidence)*50, axis=0)\n",
    "up_real2 = np.percentile(pdfs2, (1+confidence)*50, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(1, 1, figsize=(16, 10))\n",
    "\n",
    "# Plot the result of the plug-in method\n",
    "ycontour = np.max(ypdf)*factor_of_max\n",
    "plt.contour(x1pdf, x2pdf, ypdf, ycontour, colors=plt_pdf.get_color())\n",
    "plt.contour(x1pdf, x2pdf, ypdf_estimated, ycontour, colors=plt_estimated.get_color())\n",
    "plt.contour(x1pdf, x2pdf, low_plugin2, ycontour, linestyles='dashed', colors=plt_plugin[0].get_color())\n",
    "plt.contour(x1pdf, x2pdf, up_plugin2, ycontour, linestyles='dashed', colors=plt_plugin[0].get_color())\n",
    "plt.contour(x1pdf, x2pdf, low_bootstrap1_2, ycontour, linestyles='dashed', colors=plt_bootstrap1[0].get_color())\n",
    "plt.contour(x1pdf, x2pdf, up_bootstrap1_2, ycontour, linestyles='dashed', colors=plt_bootstrap1[0].get_color())\n",
    "plt.contour(x1pdf, x2pdf, low_bootstrap2_2, ycontour, linestyles='dashed', colors=plt_bootstrap2[0].get_color())\n",
    "plt.contour(x1pdf, x2pdf, up_bootstrap2_2, ycontour, linestyles='dashed', colors=plt_bootstrap2[0].get_color())\n",
    "plt.contourf(x1pdf, x2pdf, up_real2, [ycontour, 10000], colors=plt_realconf.get_facecolor())\n",
    "plt.contourf(x1pdf, x2pdf, low_real2, [ycontour, 10000], colors=[[1, 1, 1, 1]])\n",
    "conf_patch = mpatches.Patch(color=plt_realconf.get_facecolor()[0])  # Because plt.contourf cannot be used as legend handle\n",
    "plt.xlabel('$x_1$')\n",
    "plt.ylabel('$x_2$')\n",
    "plt.title(\"Contour plots at p = {:.3f} (p = pmax * {:.0f}%)\".format(ycontour, factor_of_max*100))\n",
    "plt.grid(True)\n",
    "ax.legend([plt_pdf, plt_estimated, plt_plugin[0], plt_bootstrap1[0], plt_bootstrap2[0], conf_patch], \n",
    "          ['Real', 'Estimated', '{:.0f}% Confidence (plug-in)'.format(confidence*100),\n",
    "           '{:.0f}% Confidence (bootstrap and plug-in)'.format(confidence*100),\n",
    "           '{:.0f}% Confidence (bootstrap)'.format(confidence*100),\n",
    "           '{:.0f}% Confidence (real)'.format(confidence*100)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estimated MISE for increasing $n$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "nmax = 5000\n",
    "nmin = 100\n",
    "nstep = 10\n",
    "seed = 1\n",
    "overwrite = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate datapoints\n",
    "np.random.seed(seed)\n",
    "x = gm.generate_samples(nmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize KDE\n",
    "kde = KDE(data=x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = os.path.join(\"hdf5\", \"mise.hdf5\")\n",
    "if overwrite or not os.path.exists(filename):\n",
    "    nn = np.arange(nmin, nmax, nstep)\n",
    "    bw = np.zeros_like(nn).astype(np.float)\n",
    "    std = np.zeros_like(bw)\n",
    "    for i, n in enumerate(tqdm(nn)):\n",
    "        kde.set_n(n)\n",
    "        kde.compute_bw()\n",
    "        bw[i] = kde.bw\n",
    "        std[i] = np.std(x[:n])\n",
    "    mise = kde.muk / (nn * bw)\n",
    "    \n",
    "    # Write results to hdf5 file\n",
    "    with h5py.File(filename, \"w\") as f:\n",
    "        f.create_dataset(\"mise\", data=mise)\n",
    "        f.create_dataset(\"nn\", data=nn)\n",
    "        f.create_dataset(\"bw\", data=bw)\n",
    "else:\n",
    "    with h5py.File(filename, \"r\") as f:\n",
    "        nn = f[\"nn\"][:]\n",
    "        bw = f[\"bw\"][:]\n",
    "        mise = f[\"mise\"][:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.loglog(nn, mise)\n",
    "plt.xlabel(\"Number of samples\")\n",
    "plt.ylabel(\"MISE\")\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Show whether it is \"better\" to use multivariate distribution or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "n = 100\n",
    "seed = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate data\n",
    "np.random.seed(seed)\n",
    "x = gm.generate_samples(n)\n",
    "x /= np.std(x)\n",
    "y = gm.generate_samples(n)\n",
    "y /= np.std(y)\n",
    "xy = np.concatenate((x, y), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create three KDE (for x, y, and xy, respectively) and estimate MISE\n",
    "kdex = KDE(data=x)\n",
    "kdey = KDE(data=y)\n",
    "kdexy = KDE(data=xy)\n",
    "print(\"Description   Bandwidth   1LO Loglikelihood         MISE\")\n",
    "for kde, description in zip([kdex, kdey, kdexy], ['kdex', 'kdey', 'kdexy']):\n",
    "    kde.compute_bw()\n",
    "    print(\"{:>11s}   {:9.3f}   {:17.5e}   {:7.4e}\".format(description, kde.bw, kde.score_leave_one_out(), \n",
    "                                                          kde.muk / (n * kde.bw**kde.d)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
