{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from fastkde import KDE\n",
    "import os\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import h5py\n",
    "import scipy.special\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply method for approximating the MISE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the dataset\n",
    "with open(os.path.join('pickles', 'df.p'), 'rb') as f:\n",
    "    dfs, scaling = pickle.load(f)\n",
    "scaling = scaling.T  # [time vstart vend]\n",
    "scaling = scaling[scaling[:, 2] > 0, :]  # Remove full stops\n",
    "scaling[:, 1] = scaling[:, 1] - scaling[:, 2]  # Now it becomes: [time deltav vend] (less correlation)\n",
    "scaling[:, 0] = scaling[:, 1] / scaling[:, 0]  # Now it becomes: [deceleration deltav vend] (better behaved)\n",
    "std_scaling = np.std(scaling, axis=0)\n",
    "mean_scaling = np.mean(scaling, axis=0)\n",
    "scaling = (scaling - mean_scaling) / std_scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "nmin = 600\n",
    "nstep = 300\n",
    "xmax = 3\n",
    "nx = 61\n",
    "nrepeat = 10\n",
    "seed = 0\n",
    "d = scaling.shape[1]\n",
    "overwrite = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = np.arange(nmin, scaling.shape[0], nstep)\n",
    "bw = np.zeros((nrepeat, len(nn)))\n",
    "pdfs = np.zeros(np.concatenate(([nrepeat, len(nn)], np.ones(d, dtype=np.int)*nx)))\n",
    "x = np.linspace(-xmax, xmax, nx)\n",
    "dx = np.mean(np.gradient(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Initialize the KDE\n",
    "np.random.seed(seed)\n",
    "data = scaling.copy()\n",
    "filename = os.path.join('hdf5', 'pdfs_real_dataset.hdf5')\n",
    "if overwrite or not os.path.exists(filename):\n",
    "    for repeat in tqdm(range(nrepeat)):\n",
    "        np.random.shuffle(data)\n",
    "        kde = KDE(data=data)\n",
    "\n",
    "        # Evaluate MISE for different values of n\n",
    "        for i, n in enumerate(nn):\n",
    "            # Compute the bandwidth\n",
    "            kde.set_n(n)\n",
    "            kde.compute_bw()\n",
    "            bw[repeat, i] = kde.bw\n",
    "\n",
    "            # Compute the pdf\n",
    "            xx = np.array(np.meshgrid(*[x for _ in range(d)])).reshape((d, nx**d)).T\n",
    "            pdf = kde.score_samples(xx).reshape(tuple([nx for _ in range(d)]))\n",
    "            pdfs[repeat, i] = pdf\n",
    "            \n",
    "    # Write data to file\n",
    "    with h5py.File(filename, \"w\") as f:\n",
    "        f.create_dataset(\"pdfs\", data=pdfs)\n",
    "        f.create_dataset(\"bw\", data=bw)\n",
    "else:\n",
    "     with h5py.File(filename, \"r\") as f:\n",
    "        pdfs = f[\"pdfs\"][:]\n",
    "        bw = f[\"bw\"][:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mise_var = np.zeros_like(bw)\n",
    "mise_bias = np.zeros_like(bw)\n",
    "mise = np.zeros_like(bw)\n",
    "kde = KDE(data=scaling)  # Just to get kde.muk\n",
    "\n",
    "# Evaluate MISE for different values of n\n",
    "for repeat in tqdm(range(nrepeat)):\n",
    "    for i, n in enumerate(nn):\n",
    "        # Compute integral of Laplacianxx = np.reshape(xx, (d, nx**d)).T\n",
    "        pdf = pdfs[repeat, i]\n",
    "        laplacian = np.sum(np.array([np.gradient(np.gradient(pdf, axis=i), axis=i) / dx**2 for i in range(d)]), axis=0)\n",
    "        integral = laplacian**2\n",
    "        for _ in range(d):\n",
    "            integral = np.trapz(integral, x)\n",
    "\n",
    "        # Estimate MISE\n",
    "        mise_var[repeat, i] = kde.muk / (n * bw[repeat, i]**d)\n",
    "        mise_bias[repeat, i] = bw[repeat, i]**4 / 4 * integral\n",
    "        mise[repeat, i] = mise_var[repeat, i] + mise_bias[repeat, i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for m in mise:\n",
    "    plt.loglog(nn, m, color=[.5, .5, 1])\n",
    "plt.loglog(nn, np.mean(mise, axis=0), 'r-', lw=3, label=\"Mean\")\n",
    "plt.xlabel(\"Number of samples\")\n",
    "plt.ylabel(\"MISE\")\n",
    "plt.legend()\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First map the input data\n",
    "\n",
    "The data is mapped, such that each marginal distribution is closer to a normal distribution. The idea is that the estimation of the MISE should be more consistent. For example, when looking at the plot above, it shows that the approximation of the MISE can be significantly different when the data is reshuffled. It is expected that this is less when the data is closer to a well-behaved Gaussian distribution. Still, the data will not be Gaussian, as only the marginal distributions will be, but it is nevertheless expected that this improve things."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "nmin = 600\n",
    "nstep = 300\n",
    "xmax = 3\n",
    "nx = 61\n",
    "xmax_transform = np.max(np.abs(scaling)) + 3\n",
    "nx_transform = 500\n",
    "seed = 0\n",
    "nrepeat = 10\n",
    "d = scaling.shape[1]\n",
    "overwrite = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = scaling.copy()\n",
    "kdes = [KDE(data=scaling[:, i], bw=0.25) for i in range(d)]  # 0.25 is approximately similar to Silverman's rule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xpdf_transform = np.linspace(-xmax_transform, xmax_transform, nx_transform)\n",
    "dx_transform = np.mean(np.gradient(xpdf_transform))\n",
    "f, axs = plt.subplots(1, 3, figsize=(14, 4))\n",
    "ypdf_transform = np.zeros((d, nx_transform))\n",
    "ycdf_transform = np.zeros_like(ypdf_transform)\n",
    "for i, (kde, ax) in enumerate(zip(kdes, axs)):\n",
    "    ax.hist(scaling[:, i], density=True)\n",
    "    ypdf_transform[i] = kde.score_samples(xpdf_transform)\n",
    "    ypdf_transform[i] /= np.trapz(ypdf_transform[i], xpdf_transform)\n",
    "    ycdf_transform[i] = np.cumsum(ypdf_transform[i]) * dx_transform\n",
    "    ycdf_transform[i] /= ycdf_transform[i, -1]\n",
    "    xlim = ax.get_xlim()\n",
    "    ax.plot(xpdf_transform, ypdf_transform[i], 'r-')\n",
    "    ax.set_xlim(xlim)\n",
    "    ax.set_xlabel('original value')\n",
    "    ax.set_ylabel('pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, axs = plt.subplots(1, 3, figsize=(14, 4))\n",
    "for i, ax in enumerate(axs):\n",
    "    new = np.interp(scaling[:, i], xpdf_transform, ycdf_transform[i]) * 2 - 1\n",
    "    data[:, i] = scipy.special.erfinv(new)\n",
    "    ax.hist(data[:, i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = np.arange(nmin, scaling.shape[0], nstep)\n",
    "bw = np.zeros((nrepeat, len(nn)))\n",
    "pdfs = np.zeros(np.concatenate(([nrepeat, len(nn)], np.ones(d, dtype=np.int)*nx)))\n",
    "x = np.linspace(-xmax, xmax, nx)\n",
    "dx = np.mean(np.gradient(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Initialize the KDE\n",
    "np.random.seed(seed)\n",
    "filename = os.path.join('hdf5', 'pdfs_real_dataset_transformed.hdf5')\n",
    "if overwrite or not os.path.exists(filename):\n",
    "    for repeat in tqdm(range(nrepeat)):\n",
    "        np.random.shuffle(data)\n",
    "        kde = KDE(data=data)\n",
    "\n",
    "        # Evaluate MISE for different values of n\n",
    "        for i, n in enumerate(nn):\n",
    "            # Compute the bandwidth\n",
    "            kde.set_n(n)\n",
    "            kde.compute_bw()\n",
    "            bw[repeat, i] = kde.bw\n",
    "\n",
    "            # Compute the pdf\n",
    "            xx = np.array(np.meshgrid(*[x for _ in range(d)])).reshape((d, nx**d)).T\n",
    "            pdf = kde.score_samples(xx).reshape(tuple([nx for _ in range(d)]))\n",
    "            pdfs[repeat, i] = pdf\n",
    "            \n",
    "    # Write data to file\n",
    "    with h5py.File(filename, \"w\") as f:\n",
    "        f.create_dataset(\"pdfs\", data=pdfs)\n",
    "        f.create_dataset(\"bw\", data=bw)\n",
    "else:\n",
    "     with h5py.File(filename, \"r\") as f:\n",
    "        pdfs = f[\"pdfs\"][:]\n",
    "        bw = f[\"bw\"][:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mise_var = np.zeros_like(bw)\n",
    "mise_bias = np.zeros_like(bw)\n",
    "mise = np.zeros_like(bw)\n",
    "\n",
    "# Evaluate MISE for different values of n\n",
    "for repeat in tqdm(range(nrepeat)):\n",
    "    for i, n in enumerate(nn):\n",
    "        # Compute integral of Laplacianxx = np.reshape(xx, (d, nx**d)).T\n",
    "        pdf = pdfs[repeat, i]\n",
    "        laplacian = np.sum(np.array([np.gradient(np.gradient(pdf, axis=i), axis=i) / dx**2 for i in range(d)]), axis=0)\n",
    "        integral = laplacian**2\n",
    "        for _ in range(d):\n",
    "            integral = np.trapz(integral, x)\n",
    "\n",
    "        # Estimate MISE\n",
    "        mise_var[repeat, i] = kde.muk / (n * bw[repeat, i]**d)\n",
    "        mise_bias[repeat, i] = bw[repeat, i]**4 / 4 * integral\n",
    "        mise[repeat, i] = mise_var[repeat, i] + mise_bias[repeat, i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for m in mise:\n",
    "    plt.loglog(nn, m, color=[.5, .5, 1])\n",
    "plt.loglog(nn, np.mean(mise, axis=0), 'r-', lw=3, label=\"Mean\")\n",
    "plt.xlabel(\"Number of samples\")\n",
    "plt.ylabel(\"MISE\")\n",
    "plt.legend()\n",
    "plt.grid(True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
