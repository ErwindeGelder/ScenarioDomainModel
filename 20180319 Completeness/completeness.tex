%%==========================================================================
%% This is the very first version of a Streetwise LaTeX template. By
%% chosing the document class, it is easily convertible to a two-column IEEE
%% paper format, provided any conflicting packages and (new)commmands are
%% removed.
%%
%% Erwin de Gelder, January 6, 2018
%%==========================================================================

%%==========================================================================
%% Title:
%% Author(s):
%% To be published in:
%% Date of submission:
%% Date of submission R1:
%% Date of submission R2:
%% Date of final submission:
%%==========================================================================


%% Generic
\documentclass[10pt,final,a4paper,oneside,onecolumn]{article}

%% IEEE
%% Document class options: replace "draftcls" by "final" for final document.
%% All other options may just as well be omitted because they are the default values.
%\documentclass[10pt,final,journal,letterpaper,twoside,twocolumn]{IEEEtran}


%%==========================================================================
%% Document automation
%%==========================================================================

\def\reptitle{Completeness of real-world driving data}
\def\repauthor{Erwin de Gelder, etc.}


%%==========================================================================
%% Packages
%%==========================================================================

\usepackage[a4paper,left=3.5cm,right=3.5cm,top=3cm,bottom=3cm]{geometry} %% change page layout; remove for IEEE paper format
\usepackage[T1]{fontenc}                        %% output font encoding for international characters (e.g., accented)
\usepackage[cmex10]{amsmath}                    %% math typesetting; consider using the [cmex10] option
\usepackage{amssymb}                            %% special (symbol) fonts for math typesetting
\usepackage{amsthm}                             %% theorem styles
\usepackage{dsfont}                             %% double stroke roman fonts: the real numbers R: $\mathds{R}$
\usepackage{mathrsfs}                           %% formal script fonts: the Laplace transform L: $\mathscr{L}$
\usepackage[pdftex]{graphicx}                   %% graphics control; use dvips for TeXify; use pdftex for PDFTeXify
\usepackage{array}                              %% array functionality (array, tabular)
\usepackage{upgreek}                            %% upright Greek letters; add the prefix 'up', e.g. \upphi

\usepackage[utf8]{inputenc}   				 	%% utf8 support (required for biblatex)
\usepackage[style=ieee,doi=false,isbn=false,url=false,date=year,minbibnames=15,maxbibnames=15,backend=biber]{biblatex}
%\renewcommand*{\bibfont}{\footnotesize}		%% Use this for papers
\setlength{\biblabelsep}{\labelsep}
\bibliography{../bib}

\usepackage{stfloats}                           %% improved handling of floats
\usepackage{multirow}                           %% cells spanning multiple rows in tables
%\usepackage{subfigure}                         %% subfigures and corresponding captions (for use with IEEEconf.cls)
%\usepackage{subfig}                             %% subfigures (IEEEtran.cls: set caption=false)
\usepackage{fancyhdr}                           %% page headers and footers
\usepackage[official,left]{eurosym}             %% the euro symbol; command: \euro
\usepackage{appendix}                           %% appendix layout
\usepackage{xspace}                             %% add space after macro depending on context
\usepackage{verbatim}                           %% provides the comment environment
\usepackage[dutch,USenglish]{babel}             %% language support
\usepackage{wrapfig}                            %% wrapping text around figures
\usepackage{longtable}                          %% tables spanning multiple pages
\usepackage{pgfplots}                           %% support for TikZ figures (Matlab)
\usepackage[breaklinks=true,hidelinks,          %% implement hyperlinks (dvips yields minor problems with breaklinks;
            bookmarksnumbered=true]{hyperref}   %% IEEEtran: set bookmarks=false)
%\usepackage[hyphenbreaks]{breakurl}            %% allow line breaks in URLs (don't use with PDFTeX)
\usepackage{lmodern} 
\usepackage{etoolbox}							%% Needed for apptocmd later
\usepackage[capitalize]{cleveref}
\usepackage{units}
\usepackage{subcaption}
\usepackage{csquotes}							%% Quoted texts are typeset according to rules of main language

%%==========================================================================
%% Fancy headers and footers
%%==========================================================================

\newtoggle{standalone}
\togglefalse{standalone}
\pagestyle{fancy}                                       %% set page style
\fancyhf{}                                              %% clear all header & footer fields
\iftoggle{standalone}{%
	\fancyhead[L]{\includegraphics[width=15mm]{Streetwise}} %% define headers (LE: left field/even pages, etc.)
	\fancyhead[R]{\small\emph{\reptitle}}                   %% similar
	\fancyfoot[C]{\thepage}                                 %% define footer
	\setlength{\headheight}{18pt}                           %% increase head height to accommodate an oversized picture in the header
	\renewcommand*{\headrulewidth}{0.25pt}                  %% header line width
	%% Redefine the default "plain" page style (automatically activated by \maketitle, \section, ...)
	\fancypagestyle{plain}{%
		\fancyhf{}
		\fancyhead[C]{\includegraphics[width=30mm]{Streetwise}}
		\renewcommand{\headrulewidth}{0pt}
		\renewcommand{\footrulewidth}{0pt}
		\setlength{\headheight}{36pt}
	}
}{%
	\renewcommand*{\headrulewidth}{0pt}                  	%% No line in this case
	%% Redefine the default "plain" page style (automatically activated by \maketitle, \section, ...)
	\fancypagestyle{plain}{%
		\fancyhf{}
	}
}
\renewcommand*{\footrulewidth}{0pt}                     %% footer line width


%%==========================================================================
%% TikZ figures
%%==========================================================================

\newlength\figurewidth
\setlength\figurewidth{0.35\textwidth}              %% set figure width
\newlength\figureheight
\setlength\figureheight{0.3\textwidth}              %% set figure height
\pgfplotsset{every axis/.append style={
    scaled y ticks=false,
    scaled x ticks=false,
    y tick label style={/pgf/number format/fixed},
    x tick label style={/pgf/number format/fixed},
    legend style={font=\small}},
    compat=1.9}                                     %% PGFPlots package options
\usetikzlibrary{shapes.geometric, arrows, arrows.meta}
\usepgfplotslibrary{groupplots}
%\usetikzlibrary{external}                           %% Create pdf figures from TikZ. Use PDFTeXify ...
%\tikzexternalize[prefix=./tikz/]                    %% ... with --tex-option=--shell-escape switch.
%\tikzset{external/force remake}                    %% force pdf figure update


%%==========================================================================
%% User-defined commands
%%==========================================================================

\newcommand*{\mat}[1]{\mathbf{#1}}                              %% matrix/vector notation
\newcommand*{\matsym}[1]{\boldsymbol{#1}}                       %% matrix/vector notation for Greek letters
\newcommand*{\T}{^{\scriptscriptstyle\mathsf{T}}}               %% transpose operator
\newcommand*{\Hr}{^{\scriptscriptstyle\mathsf{H}}}              %% conjugate transpose operator
\newcommand*{\ud}{\mathrm{\,d}}                                 %% differential operator (upright d)
\newcommand*{\defeq}{\mathrel{\mathop:}=}                       %% definition sign :=
\newcommand*{\eqdef}{=\mathrel{\mathop:}}                       %% definition sign =:
\newcommand*{\ip}[2]{\left\langle#1\,{,}\,#2\right\rangle}      %% inner product
\newcommand*{\real}[1]{\mathrm{Re}(#1)}                         %% real part
\newcommand*{\imag}[1]{\mathrm{Im}(#1)}                         %% imaginary part
\newcommand*{\lsup}[1]{{}^{#1}\!}                               %% left superscript
\newcommand*{\hi}[1]{$^\text{#1}$}                              %% superscript in normal text
\newcommand*{\lo}[1]{$_\text{#1}$}                              %% subscript in normal text
\newcommand*{\w}[1]{\mathrm{#1}}                                %% multiple character super-/subscript in math mode
\newcommand*{\capskip}{\vspace{-12pt}}                          %% caption skip for figures with subfloats
\newcommand*{\etal}{et al.}                                     %% may be required for Natbib bibliography styles
\renewcommand*{\qedsymbol}{$\blacksquare$}                      %% redefine the end-of-proof symbol
\renewcommand*{\labelitemi}{$\bullet$}                          %% first level item list bullet
\renewcommand*{\labelitemii}{$-$}                               %% second level item list bullet
%\renewcommand*{\theenumi}{\textit{\roman{enumi}}}               %% first level enumerator
\renewcommand*{\labelenumi}{\theenumi.}
\renewcommand*{\theenumii}{\textit{\alph{enumii}}}              %% second level enumerator
\renewcommand*{\labelenumii}{\theenumii.}
\DeclareMathOperator{\tr}{tr}                                   %% trace of a matrix
\DeclareMathOperator{\sgn}{sgn}                                 %% signum function
\DeclareMathOperator{\atan}{atan}                               %% arc tangent

\newcommand{\deuclid}{d_{\textup{Euclidian}}}
\newcommand{\ddtw}{d_{\textup{DTW}}}
\newcommand{\imagi}{\textbf{j}}
\newcommand{\profile}[1]{\textbf{v}_{#1}}
\newcommand{\barprofile}[1]{\bar{\textbf{v}}_{#1}}
\newcommand{\parvstart}{v_{\textup{start}}}
\newcommand{\parvend}{v_{\textup{end}}}
\newcommand{\pardv}{v_{\Delta}}
\newcommand{\pardt}{t_{\textup{duration}}}
\newcommand{\expectation}[1]{\textup{E} \left[ #1 \right]}
\newcommand{\entropy}[1]{\textup{H} \left[ #1 \right]}
\newcommand{\hatentropy}[1]{\hat{\textup{H}} \left[ #1 \right]}

\crefname{figure}{Figure}{Figures}
\crefname{equation}{}{}
\Crefname{equation}{Equation}{Equations}

%%==========================================================================
%% User-defined environments
%%==========================================================================

\theoremstyle{plain}\newtheorem{definition}{Definition}[section]    %% definition
                    \newtheorem{theorem}{Theorem}[section]          %% theorem
                    \newtheorem{lemma}[theorem]{Lemma}              %% lemma
                    \newtheorem{corollary}[theorem]{Corollary}      %% corollary
                    \newtheorem{assumption}{Assumption}[section]    %% assumption
                    \newtheorem{condition}{Condition}[section]      %% condition
\theoremstyle{definition}\newtheorem{example}{Example}[section]     %% examples
\theoremstyle{remark}\newtheorem{remarkenv}{Remark}[section]        %% remarks
\newenvironment{remark}{\begin{remarkenv}}%
                       {\hfill$\blacklozenge$\end{remarkenv}}       %% end remark with a lozenge
\newenvironment{reviewer}{\itshape}{\upshape}                       %% environment for reviewer's comments


%%==========================================================================
%% Miscellaneous
%%==========================================================================

\graphicspath{{./../}{./figures/}{./more_figures/}} %% (graphicx) directory path for figures
%\setlength{\parindent}{0pt}                        %% no paragraph indentation
%\setlength{\parskip}{2ex}                          %% create empty line between paragraphs
%\interdisplaylinepenalty=2500                      %% (amsmath) allow for page breaks within multiline equations
%\numberwithin{equation}{section}                   %% (amsmath) include section number in equation numbering
%\numberwithin{figure}{section}                     %% (amsmath) include section number in figure numbering
%\numberwithin{table}{section}                      %% (amsmath) include section number in table numbering
\addtolength{\arraycolsep}{-0.5mm}                  %% squeeze matrix columns a little
\fnbelowfloat                                       %% (stfloats) put footnote below a float at the page bottom
\urlstyle{same}                                     %% (hyperref) use current font for URLs
\hypersetup{pdftitle={\reptitle},
            pdfauthor={\repauthor}}                 %% (hyperref) pdf properties title and author
%\raggedbottom                                      %% don't add inter-paragraph spacing to achieve \textheight
%\setlength\subfigcapskip{-3pt}                     %% (subfigure) distance between subfloat and subcaption
%\setlength\subfigbottomskip{-3pt}                  %% (subfigure) distance between subcaption and caption
%\renewcommand*{\subcapsize}{\small}                %% (subfigure) subcaption font size
\renewcommand*{\thesubfigure}{(\alph{subfigure})}   %% (subfig) implement 1(a) instead of 1a ...as subfigure reference
\captionsetup[subfloat]{labelformat=simple}        %% ... as subfigure reference
%\captionsetup[subfloat]{
%    farskip=0pt,
%    nearskip=8pt,
%    captionskip=1.5pt,
%    labelfont={small,bf},
%    textfont=small}                                %% (subfig) subfloat caption format
%\captionsetup[figure]{
%    labelfont={small,bf},
%    textfont=small}                                %% (subfig/caption) figure caption format
%\captionsetup[table]{
%    aboveskip=2pt,
%    labelfont={small,bf},
%    textfont={small,sc}}                           %% (subfig/caption) table caption format
\apptocmd{\thebibliography}{\raggedright}{}{}		%% Suppress badness warnings in bibliography


% Table stuff
\usepackage{booktabs}
\usepackage{tabularx}
\setlength{\heavyrulewidth}{0.1em}
\newcommand{\otoprule}{\midrule[\heavyrulewidth]}

%%==========================================================================
%% Begin document
%%==========================================================================

\begin{document}

\selectlanguage{USenglish}

\title{\textbf{\reptitle}}
\author{\repauthor}
\date{\today}
\maketitle

\tableofcontents

\newpage

\section{Introduction}
\label{sec:introduction}

% Scenario-based approach for assessment of AVs -> this scenarios are important
An important aspect in the development of automated vehicles (AVs) is the assessment of quality and performance aspects of the AVs, such as safety, comfort, and efficiency \cite{bengler2014threedecades, stellet2015taxonomy, wachenfeld2016release, putz2017pegasus, roesener2016scenariobased, kompass2015sicherheitsveranderung}. 
For legal and public acceptance, it is important that there is a clear definition of system performance and that there are quantitative measures for the system quality. 
The more traditional methods \cite{response2006code, ISO26262}, used for evaluation of driver assistance systems, are no longer valid for the assessment of quality and performance aspects of an AV \cite{wachenfeld2016release}. 
Therefore, a scenario-based approach is proposed \cite{roesener2016scenariobased, putz2017pegasus, kompass2015sicherheitsveranderung}. 
For the scenario-based assessment, proper specification of scenarios is crucial since they are directly reflected in test cases used for scenario-based assessment \cite{stellet2015taxonomy}.

% Completeness
In order to draw conclusions on how an automated vehicle would perform in real-world traffic, it is necessary to know how representative the scenario database, which is used for the scenario-based assessment of the automated vehicle, is. Therefore it is important to quantify how complete the scenario database is \cite{geyer2014, alvarez2017prospective, stellet2015taxonomy}.

% Approach
To achieve a complete database, the process shown in \cref{fig:completeness collection} is adopted. Newly collected data is compared with an existing database. If the data is not ``similar'', the data is added and the process is repeated. If, at some point, the data is ``similar'', it is concluded that the database is complete. This document describes different ways to determine to what degree the newly collected data is ``similar'' to the earlier collected data.

\tikzstyle{startstop}=[rectangle, rounded corners, minimum width=4em, minimum height=4em, text centered, draw=black, fill=red!30, text width=4em]
\tikzstyle{process} = [rectangle, minimum width=9em, text width=8.5em, minimum height=4em, text centered, draw=black, fill=orange!30, node distance=10em]
\tikzstyle{decision} = [diamond, minimum width=4em, text width=4em, minimum height=4em, text centered, draw=black, fill=green!30, node distance=10em]
\tikzstyle{arrow} = [-{Triangle[width=2mm, length=3mm, round]}, line width=0.6mm] % rounded corners=0.4em
\begin{figure}[b]
	\centering
	\begin{tikzpicture}
		% Draw the blocks
		\node[startstop](start){Start};
		\node[process, right of=start, node distance=7em, text width=5em, minimum width=5.5em](collect){Collect new data};
		\node[process, right of=collect, node distance=9em](compare){Compare new data with database};
		\node[decision, right of=compare](similar){New data similar?};
		\node[startstop, right of=similar, node distance=9em](stop){Database complete};
		\node[process, below of=compare, node distance=7em](add){Add collected data to database};
		
		% Arrows
		\draw[arrow] (start) -- (collect);
		\draw[arrow] (collect) -- (compare);
		\draw[arrow] (compare) -- (similar);
		\draw[arrow] (similar) -- node[anchor=south]{Yes} (stop);
		\draw[arrow] (similar) |- node[anchor=north]{No} (add);
		\draw[arrow] (add) -| (collect);
	\end{tikzpicture}
	\caption{Schematic process of collecting data until a certain level of completeness is reached.}
	\label{fig:completeness collection}
\end{figure}

% Purpose of this document
This document describes different methods for quantifying the completeness of a dataset. To illustrate the different methods, use is made of artificial data. The advantage of using artificial data is that it is easy to obtain a huge amount of data. The data consists of the speed of a vehicle sampled at $\unit[10]{Hz}$. Furthermore, the acceleration is estimated by filtering the derivative of the speed. A portion of the data is shown in \cref{fig:artificial data}. Note that the data is not very realistic, because in real life the relative time a vehicle is cruising, i.e., driving at a constant speed, will be higher. However, for the purpose of illustrating the different methods, the artificial data suffices.

\setlength\figurewidth{0.8\linewidth}              %% set figure width
\setlength\figureheight{0.3\textwidth}              %% set figure height
\begin{figure}[b]
	\centering
	\input{figures/artificial_data.tikz}
	\caption{Artificial data of the speed of a vehicle. The black vertical lines indicate the events. In between two events, the vehicle is either acceleration, cruising, or braking.}
	\label{fig:artificial data}
\end{figure}

% Structure of document
The methods can be divided into two groups, i.e., the sample-based methods and the activity-based methods. The sample-based methods are discussed in \cref{sec:sample based} and the activity-based methods are discussed in \cref{sec:activity based methods}.


\section{Sample-based methods}
\label{sec:sample based}


\subsection{Comparing probability density functions}
\label{sec:sample based pdf}

According to Wang \etal\ \cite{wang2017much}, the question of ``how much driving data is sufficient to address problems such as the cause of accidents, distraction and inattention, eco-driving styles, modeling driver behavior, and the effects of driver assistance systems on driver behavior'' has not been fully studied. Therefore, Wang \etal\ \cite{wang2017much} present a method for quantifying the completeness regarding naturalistic driving data. 

Wang \etal\ \cite{wang2017much} use a statistical method for determining the appropriate amount of data. Based on the data, a probability density function (PDF) is estimated. The point at which the PDF converges determines the amount of appropriate data. 

Let the PDF of a certain state $x \in \mathds{R}^D$ be denoted by $f(x)$. Based on the $n$ data points $\textbf{x}=\{x_i\}_{i=1}^n$, the PDF $f(x)$ can be estimated. This estimation is denoted by $\hat{f}(x;n)$. For estimating $f(x)$, Kernel Density Estimation (KDE) \cite{rosenblatt1956remarks, parzen1962estimation} is employed, i.e.,
\begin{equation} \label{eq:kde}
	\hat{f}(x;n) = \sum_{i=1}^n \frac{1}{h^D n} K \left( \frac{x - x_i}{h} \right),
\end{equation}
where $h$ denotes the so-called bandwidth and $K(\cdot)$ represents the kernel. A Gaussian kernel is used, i.e.,
\begin{equation}
	K(x) = \frac{1}{\sqrt{2\pi}} \exp \left\{ -\frac{||x||^2}{2}\right\}.
\end{equation}
For the bandwidth, the Silverman rule \cite{silverman1986density} is used, i.e., 
\begin{equation}
	h=1.06\cdot\hat{\sigma}\cdot n^{-1/5},
\end{equation}
where $\hat{\sigma}$ is the standard deviation of the data $\{x_i\}_{i=1}^n$.

\begin{remark}
	The power of the Silverman rule is in its simplicity. The disadvantage, however, is that it tends to oversmooth the density when it is used for multimodal and highly skewed densities \cite{silverman1986density}. When comparing two estimated PDFs, they will be more similar when both are oversmoothed. As the comparison of two estimated PDFs leads to the quantification of completeness, the use of the Silverman rule might lead to an overestimation of the degree of completeness.
\end{remark}

The distribution $f(x;n)$ will change as $n$ increases. It is expected, however, that the change will decrease as $n$ increases. Therefore, the distribution $\hat{f}(x;n+m)$ with $m \in \mathds{R}^+$ is compared with $\hat{f}(x;n)$. For the comparison of two PDFs, the Kullback-Leibler divergence is employed \cite{kullback1951, bishop2006pattern}. For the two distributions $\hat{f}(x;n+m)$ and $\hat{f}(x;n)$, the Kullback-Leibler divergence is as follows:
\begin{equation} \label{eq:KL divergence}
	KL\left( \hat{f}(x;n+m) || \hat{f}(x;n) \right) = \int \hat{f}(x;n+m) \ln \left( \frac{\hat{f}(x;n+m)}{\hat{f}(x;n)} \right) \ud x.
\end{equation}

The Kullback-Leibler divergence is always positive and zero if and only if $\hat{f}(x;n+m) = \hat{f}(x;n)$. Wang~\etal\ determine the proper amount of driving data so that $KL(\hat{f}(x;n+m)||\hat{f}(x;n)$ changes very slightly if more data samples are to be added, i.e.,
\begin{equation} \label{eq:completeness wang}
	\left| KL\left(\hat{f}(x;n+m)||\hat{f}(x;n)\right) - KL\left(\hat{f}(x;n+2m)||\hat{f}(x;n+m)\right) \right| \leq \epsilon,
\end{equation}
for a small value of $\epsilon\in\mathds{R}^+$. The proper number of data-points $n$ equals the minimum value of $n$ for which \cref{eq:completeness wang} is satisfied.

\begin{remark}
	Although Wang~\etal\ use \cref{eq:completeness wang} to determine the appropriate number of data points $n$, it seems a bad choice as \cref{eq:completeness wang} is satisfied if the difference between $\hat{f}(x;n+m)$ and $\hat{f}(x;n)$ is almost similar to the difference between $\hat{f}(x;n+m)$ and $\hat{f}(x;n+2m)$. A better should would be to use
	\begin{equation} \label{eq:kl epsilon}
		KL\left(\hat{f}(x;n+m) || \hat{f}(x;n) \right) \leq \epsilon,
	\end{equation}
	for a small value of $\epsilon\in\mathds{R}^+$.
\end{remark}

\begin{example}
	\Cref{eq:completeness wang} is applied for the artificial data as shown in \cref{fig:artificial data}. When using similar values for $m$ and $\epsilon$ as in \cite{wang2017much}, i.e., $m=2000$ and $\epsilon=10^{-4}$, and the velocity as the feature $x$, it appears that 27 minutes of data suffices. The Kullback-Leibler divergence \cref{eq:KL divergence} is shown in \cref{fig:kl sample based} for different amount of data. \Cref{fig:kl sample based} shows that as $n$ increases, the Kullback-Leibler divergence becomes less. 
	
	\setlength\figurewidth{0.6\linewidth}
	\setlength\figureheight{0.7\figurewidth}
	\begin{figure}
		\centering
		\input{figures/kl_sample_based.tikz}%
		\caption{Kullback-Leibler divergence according to \cref{eq:KL divergence} with $m=2000$.}
		\label{fig:kl sample based}
	\end{figure}
	
	The result depends on the choice of $m$ and on the feature that is used. \Cref{tab:wang result} lists the results for different values of $m$, where both the speed and the acceleration are considered as features. Considering the estimations regarding the different features, Wang \etal\ use the maximum as final estimation of the appropriate amount of data. The maximum required time is shown in the last column of \cref{tab:wang result}. \Cref{tab:wang result} shows that the final estimated of the required data heavily depends on $m$.
		
	\begin{table}
		\centering
		\caption{Appropriate data according to \cref{eq:completeness wang}.}
		\label{tab:wang result}
		\begin{tabular}{rrrr}
			\toprule
			& \multicolumn{2}{c}{Feature} & \\
			$m$ & Speed & Acceleration & Maximum \\ \otoprule
			500 & $\unit[7]{min}$ & $\unit[5]{min}$ & $\unit[7]{min}$ \\
			100 & $\unit[7]{min}$ & $\unit[12]{min}$ & $\unit[12]{min}$ \\
			2000 & $\unit[27]{min}$ & $\unit[23]{min}$ & $\unit[27]{min}$ \\
			4000 & $\unit[67]{min}$ & $\unit[27]{min}$ & $\unit[67]{min}$ \\
			6000 & $\unit[50]{min}$ & $\unit[40]{min}$ & $\unit[50]{min}$ \\
			8000 & $\unit[93]{min}$ & $\unit[40]{min}$ & $\unit[93]{min}$ \\
			10000 & $\unit[100]{min}$ & $\unit[33]{min}$ & $\unit[100]{min}$ \\
			\bottomrule
		\end{tabular}
	\end{table}
\end{example}


\subsection{Likelihood of new samples}
\label{sec:likelihood sample based}

This method is similar to the previous method in that a PDF is estimated using $n$ samples, i.e., $\hat{f}(x;n)$. But instead of comparing $\hat{f}(x;n)$ with $\hat{f}(x;n+m)$, the $m$ new samples are directly compared with first $n$ samples by determining the likelihood of the new $m$ samples based on the estimated PDF $\hat{f}(x;n)$. 

Assuming that the samples are independent, the likelihood is simply the product of $\hat{f}(x_i;n)$ with $i=\{n+1, \ldots, n+m\}$. To get a value that is less dependent on the choice of $m$, the following value is used:
\begin{equation} \label{eq:likelihood estimation}
	J(n) = \left( \prod_{i=n+1}^{n+m} \hat{f}(x_i;n) \right)^{\frac{1}{m}}.
\end{equation}
To make the computations easier, the log of \cref{eq:likelihood estimation} is considered:
\begin{equation} \label{eq:loglikelihood estimation}
	\ln J(n) = \frac{1}{m} \sum_{i=n+1}^{n+m} \ln \hat{f}(x_i;n).
\end{equation}

\begin{remark}
	Obviously, consecutive samples are not independent.
\end{remark}

An important choice is when $J(n)$ is good enough. The goal is to estimate the underlying distribution $f(x)$ as accurately as possible. Therefore, when the true distribution $f(x)$ is used instead of $\hat{f}(x_i;n)$ in \cref{eq:loglikelihood estimation}, the expected value is
\begin{equation} \label{eq:minus entropy}
	\ln J^* = \expectation{ \ln f(x) }.
\end{equation}
Notice that \cref{eq:minus entropy} equals the negative entropy of the $f(x)$. The entropy $\entropy{x}$ is defined as follows \cite{bishop2006pattern}:
\begin{equation} \label{eq:entropy}
	\entropy{x} = -\int f(x) \ln f(x) \ud x,
\end{equation}
where use is made of $f(x) \ln f(x) = 0$ if $f(x)=0$. Since the distribution $f(x)$ is unknown, the entropy can be estimated using $\hat{f}(x_i;n)$:
\begin{equation} \label{eq:entropy estimation}
	\hatentropy{x; n} = -\int \hat{f}(x;n) \ln \hat{f}(x;n) \ud x.
\end{equation}
The estimated entropy \cref{eq:entropy estimation} can be used to determine the desired value of the log-likelihood \eqref{eq:loglikelihood estimation}.

\begin{remark}
	Using the estimated entropy \eqref{eq:entropy estimation} promotes the use of high values of the bandwidth $h$, as this will result in higher values of the entropy. As a result, even though the estimation PDF $\hat{f}(x_i;n)$ might be worse with a higher value of $h$, the log-likelihood \cref{eq:loglikelihood estimation} might be closer to the estimated entropy \eqref{eq:entropy estimation}.
	
	A work-around could be to determine the log-likelihood of the first $n$ samples, i.e.,
	\begin{equation}
		\frac{1}{n} \sum_{i=1}^n \ln \hat{f} (x_i; n).
	\end{equation}
	If this value is significantly higher than $-\hatentropy{x;n}$, this might be an indication that $\hat{f}(x_i; n)$ is an inaccurate estimation of $f(x)$.
\end{remark}

\begin{example} \label{example:likelihood sample based}
	Based on the artificial data shown in \cref{fig:artificial data}, the log-likelihood \eqref{eq:loglikelihood estimation} is computed with $m=2000$, see \cref{fig:likelihood sample based}. It can be seen that the log-likelihood $\ln J(n)$ is on average the same as the negative entropy $-\hatentropy{x;n}$. However, there is not a clear trend visible regarding the log-likelihood and the amount of data.
	
	\setlength\figurewidth{0.6\linewidth}
	\setlength\figureheight{0.7\figurewidth}
	\begin{figure}
		\centering
		\input{figures/likelihood_samplebased.tikz}%
		\caption{Log-likelihood \cref{eq:loglikelihood estimation} with $m=2000$ and the estimated entropy \cref{eq:entropy estimation}.}
		\label{fig:likelihood sample based}
	\end{figure}
\end{example}


\section{Activity-based methods}
\label{sec:activity based methods}


\subsection{Comparing probability density functions}
\label{sec:comparing pdf activities}

Instead of considering the PDFs of the time samples (see \cref{sec:sample based pdf}), the PDFs of the activities' features will be considered in this section. An activity refers to the behavior of the vehicle between two events. As the number of activities is much less than the number of samples, the analysis using the activities is much less computationally intensive.

For illustrating the methods, three features that describe an activity are considered:
\begin{itemize}
	\item $v_{\textup{end}}$: The end speed of the activity.
	\item $\Delta v$: The difference between the end and start speed.
	\item $\Delta t$: The duration of the activity.
\end{itemize}

For each feature, a univariate PDF is estimated using Kernel Density Estimation (KDE), see \cref{eq:kde}. For each feature, the appropriate number of activities is estimated using the Kullback-Leibler divergence, according to \cref{eq:kl epsilon}, resulting in $n_{v_{\textup{end}}}$, $n_{\Delta v}$, and $n_{\Delta t}$. The total number of activities is simply the maximum of $n_{v_{\textup{end}}}$, $n_{\Delta v}$, and $n_{\Delta t}$.

\begin{example}
	Based on the artificial data, the KL divergence \cref{eq:KL divergence} is computed. \Cref{fig:kl activity based} shows the result for $m=10$. Using $\epsilon = 10^{-3}$ results in $n_{v_{\textup{end}}}=150$, $n_{\Delta v}=80$, and $n_{\Delta t}=20$ which correspond to $\unit[46]{min}$, $\unit[25]{min}$, and $\unit[7]{min}$, respectively. Therefore, according to this method, the appropriate amount of data is $\unit[46]{min}$. 
	
	\setlength\figurewidth{0.6\linewidth}
	\setlength\figureheight{0.7\figurewidth}
	\begin{figure}
		\centering
		\input{figures/KL_activity_based.tikz}%
		\caption{Kullback-Leibler divergence \cref{eq:KL divergence} using the features from the activities with $m=10$.}
		\label{fig:kl activity based}
	\end{figure}

	The result depends hugely on the choice of $m$. Obviously, when more data is added, the difference between the two PDFs $\hat{f}(x;n+m)$ and $\hat{f}(x;n)$ is likely to be higher. For example, when $m=5$ is used, the estimated amount of appropriate data is $\unit[21]{min}$ instead of $\unit[46]{min}$. When $m=15$, this is $\unit[74]{min}$.
\end{example}


\subsection{Likelihood of new activities}
\label{sec:likelihood new activities}

This method is similar to the method presented in \cref{sec:likelihood sample based}, but instead of computing the likelihood of the samples, the likelihood of the activities is computed using the features of the activities. 

\begin{example}
	Based on the artificial data, the likelihood of the feature $\Delta v$ is computed, see \cref{fig:likelihood activity based}. Similarly when applying this method for the samples (see \cref{example:likelihood sample based}), there is not a clear trend visible regarding the log-likelihood and the amount of data.
	
	\setlength\figurewidth{0.6\linewidth}
	\setlength\figureheight{0.7\figurewidth}
	\begin{figure}
		\centering
		\input{figures/likelihood_activitybased.tikz}%
		\caption{Log-likelihood \cref{eq:loglikelihood estimation} with $m=10$ and the estimated entropy \cref{eq:entropy estimation} using the feature $\Delta v$.}
		\label{fig:likelihood activity based}
	\end{figure}

	A possible reason why the result in \cref{fig:likelihood activity based} is inconclusive could be the fact that the chosen bandwidth $h$ is too large and therefore the estimated PDF $\hat{f}(x;n)$ is oversmoothed. When using pseudo likelihood cross-validation \cite{turlach1993bandwidthselection} to determine the optimal bandwidth, a smaller bandwidth is obtained, see \cref{fig:bandwidth}. This results in a different log-likelihood, see \cref{fig:loglikelihood activities small bandwidth}.
	
	\setlength\figureheight{0.4\linewidth}
	\setlength\figurewidth{0.48\linewidth}
	\begin{figure}
		\centering
		\begin{minipage}{0.48\linewidth}
			\centering
			\input{figures/likelihood_activitybased_small_bandwidth.tikz}%
			\subcaption{Log-likelihood.}
			\label{fig:loglikelihood activities small bandwidth}
		\end{minipage}%
		\begin{minipage}{0.48\linewidth}
			\centering
			\input{figures/bandwidth.tikz}%
			\subcaption{Bandwidth $h$.}
			\label{fig:bandwidth}
		\end{minipage}
		\caption{Left: The log-likelihood \cref{eq:loglikelihood estimation} with $m=10$ and the estimated entropy \cref{eq:entropy estimation} using the features $\Delta v$ and a bandwidth estimated using cross validation. Right: comparison of the bandwidth computed using the Silverman rule (for which the resulting log-likelihood is shown in \cref{fig:likelihood activity based}) and using cross validation (for which the result is shown in the left figure).}
		\label{fig:loglikelihood and bandwidth}
	\end{figure}
\end{example}


\subsection{Dissimilarity of new activities}
\label{sec:dissimilarity new activities}

This method compares the newly collected activities with all earlier collected activities. If the newly collected activities are very ``similar'' to the earlier collected activities, it is concluded that the dataset is complete. The main difference with the previous methods is that this method focuses on having distinct activities, while the distribution of the activities (i.e., the PDFs) are not taken into account. 

For determining the dissimilarity between two activities, the features of the activity are used. The feature vector is denoted as $\theta$. For this study, $\theta = \begin{bmatrix} v_{\textup{end}} & \Delta v & \Delta t \end{bmatrix}^T$, where $v_{\textup{end}}$, $\Delta v$, and $\Delta t$ are defined in \cref{sec:comparing pdf activities}.

A max-minimum method is used to determine to what degree the newly collected activities differ from the earlier collected activities, i.e., 
\begin{equation} \label{eq:maxmin dissimilarity}
	J_d(n, m) = \max_{n+1 \leq j \leq n+m} \min_{1 \leq i \leq n} g(\theta_i, \theta_j),
\end{equation}
where $g(\theta_i, \theta_j)$ denotes the dissimilarity measure that is defined as follows:
\begin{equation}
	g(\theta_i, \theta_j) = \sqrt{ \left( \theta_i - \theta_j \right)^T W \left( \theta_i - \theta_j \right)},
\end{equation}
where $W$ is a positive definite weighting matrix. For this study, $W$ is chosen to be a diagonal matrix with the $(k,k)$-th element equal to the reciprocal of the variance of the $k$-th feature, based on the $n$ activities.

\begin{example}
	Based on the artificial data, the max-minimum of the dissimilarity between the $m=10$ new activities and the earlier activities is computed, see \cref{fig:jd}. Although it is not clearly visible, there is a trend visible where $J_d(n,m)$ decreases as more data is added. There are, however, still some high values near the end. Furthermore, there is not a clear point in time where \emph{completeness} is reached.
	
	\setlength\figurewidth{0.6\linewidth}
	\setlength\figureheight{0.7\figurewidth}
	\begin{figure}
		\centering
		\input{figures/dissimilarity.tikz}%
		\caption{Dissimilarity \cref{eq:maxmin dissimilarity} of the $m=10$ new activities compared to the earlier recorded activities.}
		\label{fig:jd}
	\end{figure}
\end{example}

\printbibliography

\end{document} 