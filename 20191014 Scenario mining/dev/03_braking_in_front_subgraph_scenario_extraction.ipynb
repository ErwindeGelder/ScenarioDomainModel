{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we are getting there! so we first focus on breaking in front, any \"in front\", for each template sequence we extract a subgraph of possible nodes, then the subgraph of pre_post nodes (i dont think it matters distinguishing them) and then we further trim the graphs by taking only the valid paths seen in the data + extraction of the raw data pointers. another thing we could do is preparing a unique html page with that stuff!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO: \n",
    "- these todo could also be fixed when we deal with cutin etc\n",
    "- review the old dictionary-template matching function from OLD_03_search_function_and_visualisations.ipynb\n",
    "- add possibilities of \"*\" values\n",
    "- figure out an elegant way to deal with int-dict-index and str-dict-index\n",
    "- merge the rendering of point nodes with image nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import re\n",
    "from copy import deepcopy\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imports.general_settings import *\n",
    "from imports.general_functions import *\n",
    "from imports.NGram_KNrealRecursion import recursive_NGramKneserNey\n",
    "from pyvis.network import Network, Node, Edge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UTILITY FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def patch_network_filenames(filename_stem) : \n",
    "    # output_path\n",
    "    # inputPath_dictionaryImg\n",
    "    # baseFolder\n",
    "    fromPath = inputPath_dictionaryImg.replace('\\\\', '\\\\\\\\')\n",
    "    toPath = \"..\\\\\\\\\" + \"..\\\\\\\\\" + inputFolder_dictionary + \"\\\\\\\\\" + inputFolder_dictionaryImg + \"\\\\\"\n",
    "    \n",
    "    with open(filename_stem + \".html\", \"r\") as f:\n",
    "        content = f.readlines()\n",
    "    \n",
    "    newContent = []\n",
    "    for x in content :\n",
    "        y = x\n",
    "        while fromPath in y :\n",
    "            #print(x)\n",
    "            y = y[:y.find(fromPath)] + toPath + y[y.find(fromPath) + len(fromPath) : ]\n",
    "        newContent.append(y)\n",
    "\n",
    "    #for x in newContent :\n",
    "    #    if fromPath in x :\n",
    "    #        print(x)\n",
    "    with open(output_path + filename_stem + \".html\", \"w\") as f :\n",
    "        f.writelines(\"\".join(newContent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_breaking_in_front_nodes() :\n",
    "    dict_valid_scenario_nodes = []\n",
    "    print(len(dictionary))\n",
    "    for i in range(len(dictionary)) :\n",
    "        print(i, 6*' ', end='\\r')\n",
    "        entry = dict(dictionary.iloc[i])\n",
    "        for t in range(8) :\n",
    "            if entry['target_' + str(t) + '_relative_position_lateral'] == 'same_lane' :\n",
    "                if entry['target_' + str(t) + '_relative_position_longitudinal'] in ['0<=x<=10', '10<x<=30'] :\n",
    "                    if entry['target_' + str(t) + '_longitudinal'] == 'd' :\n",
    "                        dict_valid_scenario_nodes.append(i)\n",
    "                        break\n",
    "    dict_valid_scenario_nodes = list(set(dict_valid_scenario_nodes))\n",
    "    print()\n",
    "    print(len(dict_valid_scenario_nodes))\n",
    "    \n",
    "    dict_valid_scenario_nodes = [str(x) for x in dict_valid_scenario_nodes]\n",
    "\n",
    "    return dict_valid_scenario_nodes\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SETTINGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_input_filename = \"20170524_PP_01_Run_1.hdf5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stem_filename = raw_input_filename[:raw_input_filename.find(\".hdf5\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stem_filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = '..\\\\..\\\\data\\\\'\n",
    "baseFolder = data_folder + projectName + '\\\\'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputFolder_dictionary = \"01_activity_dictionary\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary_filename_stem = \"activityDictionary\"\n",
    "dataset_filename_stem = \"mappedDataset\"\n",
    "nGramGraph_filename_stem = \"nGramGraph\"\n",
    "networkX_filename_stem = \"networkx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_subnetwork_filename_stem = \"dataset_subnetwork\"\n",
    "dataset_subnetwork_img_filename_stem = \"dataset_subnetwork_img\"\n",
    "full_dataset_network_filename_stem = \"full_dataset_network\"\n",
    "scenario_subnetwork_img_filename_stem = \"scenario_subnetwork_img\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gridSizeSubfolder = str(len(valid_relative_positions['lateral'])-1) + \"x\" + str(len(valid_relative_positions['longitudinal'])-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputFolder_dictionaryImg = \"img_\" + gridSizeSubfolder + '\\\\'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgExtension = \".jpg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary_filename = dictionary_filename_stem + \"_\" + gridSizeSubfolder + \".csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputPath_dictionary = baseFolder + inputFolder_dictionary + \"\\\\\"\n",
    "inputPath_dictionaryImg = inputPath_dictionary + inputFolder_dictionaryImg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputPath_dictionaryImg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_folder = \"03_retrieved_scenarios\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = baseFolder + output_folder + \"\\\\\" + stem_filename + \"\\\\\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if output_folder not in os.listdir(baseFolder) :\n",
    "    print(\"creating\", baseFolder + output_folder)\n",
    "    os.mkdir(baseFolder + output_folder)\n",
    "else :\n",
    "    print(\"folder\", output_folder, \"already exists\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if stem_filename not in os.listdir(baseFolder + output_folder) :\n",
    "    print(\"creating\", stem_filename)\n",
    "    os.mkdir(output_path)\n",
    "else :\n",
    "    print(\"folder\", stem_filename, \"already exists\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DICTIONARY LOADING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = pd.read_pickle(inputPath_dictionary + dictionary_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#idx = np.random.randint(len(dictionary))\n",
    "#img = plt.imread(inputPath_dictionaryImg + str(idx) + imgExtension)\n",
    "#img = np.zeros(shape=img.shape) + img\n",
    "#render(img, title=idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dict_entry = dict(dictionary.iloc[idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# let's \"fake\" a brake in front search\n",
    "simple: same lane, lonfitudinal > 0, breaking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_valid_scenario_nodes = extract_breaking_in_front_nodes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dict_valid_scenario_nodes = [str(x) for x in dict_valid_scenario_nodes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"the dictionary has\", len(dict_valid_scenario_nodes), \"nodes satisfying breaking in front\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#for idx in dict_valid_scenario_nodes[:5] :\n",
    "#    img = plt.imread(inputPath_dictionaryImg + str(idx) + imgExtension)\n",
    "#    img = np.zeros(shape=img.shape) + img\n",
    "#    render(img, title=idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOADING RAW DATASET/NETWORKS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_filename = dataset_filename_stem + \"_\" + stem_filename + \".csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputFolder_data = \"02_mapped_dataset\"\n",
    "inputPath_data = baseFolder + inputFolder_data + \"\\\\\" + gridSizeSubfolder + \"\\\\\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = pd.read_csv(inputPath_data + input_filename, dtype='str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nGramGraph_filename = nGramGraph_filename_stem + \"_\" + stem_filename + \".p\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngram_model = pickle.load(open(inputPath_data + nGramGraph_filename, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "networkX_filename = networkX_filename_stem + \"_\" + stem_filename + \".p\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_nx = pickle.load(open(inputPath_data + networkX_filename, \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_nx.number_of_nodes(), model_nx.number_of_edges()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATASET NODE IDENTIFICATION\n",
    "- we need to partition the dataset network into 3 node types:\n",
    "    - scenario nodes\n",
    "    - pre_post scenario nodes\n",
    "    - non relevant nodes\n",
    "- scenario nodes are the nodes in the dataset which are also part of dict_valid_scenario_nodes\n",
    "- pre_post are extracted accordingly - predecessors\n",
    "\n",
    "- we also represent the information via network colouring"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. find valid scenario nodes: all nodes in dataset which are part of dict_valid_scenario_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nodes = list(dataframe['dictionary_index'].drop_duplicates())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#len(df_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scenario_nodes = sorted(list(set(df_nodes).intersection(dict_valid_scenario_nodes)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"the dataset has\", len(df_scenario_nodes), \"scenario nodes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. from the dataset network extract the pre/post scenario nodes\n",
    "i.e. all predecessors/successors of scenario nodes which are not scenario nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_nodes = []\n",
    "for node in df_scenario_nodes :\n",
    "    pre_nodes += list(set(model_nx.predecessors(node)).difference(df_scenario_nodes))\n",
    "pre_nodes = list(set(pre_nodes))\n",
    "len(pre_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_nodes = []\n",
    "for node in df_scenario_nodes :\n",
    "    \n",
    "    post_nodes += list(set(model_nx.successors(node)).difference(df_scenario_nodes))\n",
    "post_nodes = list(set(post_nodes))\n",
    "len(post_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pre_post_nodes = pre_nodes + post_nodes\n",
    "df_pre_post_nodes = list(set(df_pre_post_nodes))\n",
    "print(\"the dataset has\", len(df_pre_post_nodes), \"pre-post scenario nodes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_remaining_nodes = list(set(model_nx.nodes()).difference(df_scenario_nodes + df_pre_post_nodes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"remaining nodes:\", len(df_remaining_nodes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_remaining_nodes) + len(df_pre_post_nodes) + len(df_scenario_nodes) == model_nx.number_of_nodes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_scenario_nodes[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_pre_post_nodes[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_remaining_nodes[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. draw the full network with the 3 node types (pyvis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_dataset_network = Network(height=500, width=1200, directed=True, notebook=False)\n",
    "\n",
    "for node in df_remaining_nodes :\n",
    "    full_dataset_network.add_node(int(node), node, color='black')\n",
    "    \n",
    "for node in df_pre_post_nodes :\n",
    "    full_dataset_network.add_node(int(node), node, color='blue')\n",
    "\n",
    "for node in df_scenario_nodes :\n",
    "    full_dataset_network.add_node(int(node), node, color='green')\n",
    "\n",
    "for edge in list(model_nx.edges()) :\n",
    "    full_dataset_network.add_edge(int(edge[0]), int(edge[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_dataset_network.save_graph(full_dataset_network_filename_stem + \".html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. subgraph extraction i.e. remove df_remaining_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scenario_subgraph_nx = model_nx.subgraph(df_pre_post_nodes + df_scenario_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_subnetwork = Network(height=500, width=1200, directed=True, notebook=False)\n",
    "\n",
    "for node in df_pre_post_nodes :\n",
    "    dataset_subnetwork.add_node(int(node), node, color='blue')\n",
    "\n",
    "for node in df_scenario_nodes :\n",
    "    dataset_subnetwork.add_node(int(node), node, color='green')\n",
    "\n",
    "for edge in list(model_nx.edges()) :\n",
    "    if edge[0] in df_pre_post_nodes and edge[1] in df_scenario_nodes :\n",
    "        dataset_subnetwork.add_edge(int(edge[0]), int(edge[1]))\n",
    "    elif edge[0] in df_scenario_nodes and edge[1] in df_pre_post_nodes :\n",
    "        dataset_subnetwork.add_edge(int(edge[0]), int(edge[1]))\n",
    "    elif edge[0] in df_scenario_nodes and edge[1] in df_scenario_nodes :\n",
    "        dataset_subnetwork.add_edge(int(edge[0]), int(edge[1]))\n",
    "\n",
    "for i in range(len(dataset_subnetwork.edges)) :\n",
    "    dataset_subnetwork.edges[i]['color'] = edgeProperties['color']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_subnetwork.save_graph(dataset_subnetwork_filename_stem + \".html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. subgraph rendering with images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_subnetwork_img = deepcopy(dataset_subnetwork)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(dataset_subnetwork_img.nodes)) :\n",
    "    node_id = dataset_subnetwork_img.nodes[i]['id']\n",
    "    dataset_subnetwork_img.nodes[i]['shape'] = 'image'\n",
    "    dataset_subnetwork_img.nodes[i]['image'] = inputPath_dictionaryImg + str(node_id) + imgExtension\n",
    "    \n",
    "for i in range(len(dataset_subnetwork_img.edges)) :\n",
    "    dataset_subnetwork_img.edges[i]['color'] = edgeProperties['color']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_subnetwork_img.save_graph(dataset_subnetwork_img_filename_stem + \".html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SCENARIO INSTANCE EXTRACTION\n",
    "it's a multilevel thingie:\n",
    "- the dataset now should ahve labels as \"pre_post\", \"scenario\", \"nothing\"\n",
    "- then we can shrink the dataset to have unique repetitions for the sequence\n",
    "- in that way we can just extract sequencese \"pre_post\"-\"scenario\"-\"pre-post\"\n",
    "- we will have to see who we can deal with multi sequence scenarios like cut-in, as a node can be in both sequences (imagine 2 targets performing cut-in one after the other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scenario_indices = ['n', 'p', 's']\n",
    "scenario_indices_long = ['nothing', 'pre_post', 'scenario']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_scenario_label(dict_index) :\n",
    "    x = str(dict_index)\n",
    "    if x in df_scenario_nodes :\n",
    "        return scenario_indices.index(\"s\")\n",
    "    elif x in df_pre_post_nodes :\n",
    "        return scenario_indices.index(\"p\")\n",
    "    else :\n",
    "        return scenario_indices.index(\"n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe['scenario_label'] = dataframe['dictionary_index'].apply(lambda x : assign_scenario_label(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmpDf = dataframe.loc[dataframe['scenario_label'].diff() != 0]\n",
    "no_dup_dataframe = pd.DataFrame()\n",
    "no_dup_dataframe['df_idx'] = list(tmpDf.index)\n",
    "no_dup_dataframe['scenario_idx'] = list(tmpDf['scenario_label'])\n",
    "no_dup_dataframe['scenario_str'] = [scenario_indices[x] for x in  list(no_dup_dataframe['scenario_idx'])]\n",
    "no_dup_dataframe['dic_idx'] = list(tmpDf['dictionary_index'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(no_dup_dataframe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# mining!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scenario_sequence = ''.join(no_dup_dataframe['scenario_str'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scenario_sequence[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "scenario_instances = [m.start() for m in re.finditer('psp', scenario_sequence)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(\"i found\", len(scenario_instances), \"instances of breaking in front!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fig = plt.figure()\n",
    "#plt.plot(no_dup_dataframe['scenario_idx'])\n",
    "#plt.scatter(x=scenario_instances, y=[2 for _ in scenario_instances], c='r')\n",
    "#plt.yticks(np.arange(len(scenario_indices)), scenario_indices_long)\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# scenario-to-raw-indices retrieval!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe['dictionary_index_int'] = dataframe['dictionary_index'].apply(lambda x : int(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scenario_retrieval_df = pd.DataFrame(columns=['init_pre_idx', 'init_scenario_idx', 'init_post_idx', 'end_post_idx',\n",
    "                                              'init_pre_ts', 'init_scenario_ts', 'init_post_ts', 'end_post_ts',\n",
    "                                              'pre_scenario_post_dict_sequence', 'scenario_dict_sequence'])\n",
    "for si in scenario_instances :\n",
    "    mini_df = no_dup_dataframe.loc[si:si+len('sps')]\n",
    "    init_pre_idx = mini_df.iloc[0]['df_idx']\n",
    "    init_scenario_idx = mini_df.iloc[1]['df_idx']\n",
    "    init_post_idx = mini_df.iloc[2]['df_idx']\n",
    "    end_post_idx = mini_df.iloc[3]['df_idx']\n",
    "    \n",
    "    init_pre_ts = float(dataframe.loc[init_pre_idx, 'timestamp'])\n",
    "    init_scenario_ts = float(dataframe.loc[init_scenario_idx, 'timestamp'])\n",
    "    init_post_ts = float(dataframe.loc[init_post_idx, 'timestamp'])\n",
    "    end_post_ts = float(dataframe.loc[end_post_idx, 'timestamp'])\n",
    "    \n",
    "    tmp_df = dataframe.loc[init_pre_idx:end_post_idx-1]\n",
    "    dict_seq = list(tmp_df.loc[tmp_df['dictionary_index_int'].diff() != 0]['dictionary_index'])\n",
    "    dict_seq_full = '-'.join(dict_seq)\n",
    "    dict_seq_str = '-'.join(dict_seq[1:-1])\n",
    "    \n",
    "    scenario_retrieval_df = scenario_retrieval_df.append({'init_pre_idx' : init_pre_idx, \n",
    "                                                          'init_scenario_idx' : init_scenario_idx, \n",
    "                                                          'init_post_idx' : init_post_idx, \n",
    "                                                          'end_post_idx' : end_post_idx,\n",
    "                                                          'init_pre_ts' : init_pre_ts, \n",
    "                                                          'init_scenario_ts' : init_scenario_ts, \n",
    "                                                          'init_post_ts' : init_post_ts, \n",
    "                                                          'end_post_ts' : end_post_ts,\n",
    "                                                          'pre_scenario_post_dict_sequence' : dict_seq_full,\n",
    "                                                          'scenario_dict_sequence' : dict_seq_str\n",
    "                                                         }, ignore_index=True)\n",
    "    #break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "scenario_retrieval_df.to_csv(output_path + \"scenario_instances.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# scenario instance visualisation as graph (nah for now?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scenario_subnetwork_img_node(lbl) :\n",
    "    node = None\n",
    "    for n in dataset_subnetwork_img.nodes :\n",
    "        if n['label'] == lbl :\n",
    "            node = n\n",
    "            break\n",
    "    return node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scenario_subnetwork_img = Network(height=500, width=1200, directed=True, notebook=False)\n",
    "\n",
    "added_nodes = []\n",
    "for i in range(len(scenario_retrieval_df)) :\n",
    "    print(i, '/', len(scenario_retrieval_df), end='\\r')\n",
    "    \n",
    "    seq = scenario_retrieval_df.iloc[i]['pre_scenario_post_dict_sequence']\n",
    "    seq = seq.split('-')\n",
    "    \n",
    "    # add nodes\n",
    "    for j in range(len(seq)) :\n",
    "        \n",
    "        node_label = seq[j]\n",
    "        if node_label not in added_nodes :\n",
    "            # new node to add\n",
    "            added_nodes.append(node_label)\n",
    "            # get the style and augment\n",
    "            node = get_scenario_subnetwork_img_node(node_label)\n",
    "            colour = 'green'\n",
    "            if j == 0 or j == len(seq) - 1 :\n",
    "                colour = 'blue'\n",
    "        \n",
    "            node['color'] = {'border' : colour, 'highlight' : colour}\n",
    "            node['borderWidth'] = 10\n",
    "            node['borderWidth'] = 4\n",
    "            node['borderWidthSelected'] = 4\n",
    "            node['shapeProperties'] = {'useBorderWithImage' : True}\n",
    "            \n",
    "            scenario_subnetwork_img.add_node(int(node_label))\n",
    "            \n",
    "            for x in scenario_subnetwork_img.nodes :\n",
    "                if x['id'] == int(node_label) :\n",
    "                    for k in node.keys() :\n",
    "                        x[k] = node[k]\n",
    "            \n",
    "        #break\n",
    "    \n",
    "    # add edges\n",
    "    for j in range(len(seq) - 1) :\n",
    "        from_node = int(seq[j])\n",
    "        to_node = int(seq[j+1])\n",
    "        found = False\n",
    "        for e in scenario_subnetwork_img.edges :\n",
    "            if e['from'] == from_node and e['to'] == to_node :\n",
    "                found = True\n",
    "                break\n",
    "           \n",
    "        if not found :\n",
    "            # we can add an edge\n",
    "            scenario_subnetwork_img.add_edge(from_node, to_node)\n",
    "            for i in range(len(scenario_subnetwork_img.edges)) :\n",
    "                if scenario_subnetwork_img.edges[i]['from'] == from_node and scenario_subnetwork_img.edges[i]['to'] == to_node :\n",
    "                    # fix style\n",
    "                    scenario_subnetwork_img.edges[i]['color'] = edgeProperties['color']\n",
    "            \n",
    "        #break\n",
    "    \n",
    "        \n",
    "    #break\n",
    "print(\"\\n.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scenario_subnetwork_img.save_graph(scenario_subnetwork_img_filename_stem + \".html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# patching the graphs to then move them to the appropriate folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in [dataset_subnetwork_filename_stem, dataset_subnetwork_img_filename_stem, full_dataset_network_filename_stem, scenario_subnetwork_img_filename_stem] :\n",
    "    patch_network_filenames(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"done!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
