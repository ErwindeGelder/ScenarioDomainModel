{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INTRO\n",
    "we map a dataset with the dictionary and also extract all relevant information for graph construction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HEADER\n",
    "adding a link to a shared folder where to find the ngram class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath('../shared/'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from NGram_KNrealRecursion import recursive_NGramKneserNey\n",
    "import pickle\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imports.general_settings import *\n",
    "from imports.general_functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GENERAL SETTINGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "projectName"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_valid_relative_positions(valid_relative_positions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseFolder = '..\\\\..\\\\data\\\\' + projectName + '\\\\'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputFolder_data = \"00_raw_data\"\n",
    "inputFolder_dictionary = \"01_activity_dictionary\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputFolder = \"02_mapped_dataset\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputSubfolder = str(len(valid_relative_positions['lateral'])-1) + \"x\" + str(len(valid_relative_positions['longitudinal'])-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputFilename_dataset = \"mappedDataset\"\n",
    "outputFilename_nGramGraph = \"nGramGraph\"\n",
    "outputFilename_networkX = \"networkx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionaryFilename = \"activityDictionary_\" + str(len(valid_relative_positions['lateral'])-1) + \"x\" + str(len(valid_relative_positions['longitudinal'])-1) + \".csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionaryFilename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary_indexColumnName = \"index\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputPath_data = baseFolder + inputFolder_data + \"\\\\\"\n",
    "inputPath_dictionary = baseFolder + inputFolder_dictionary + \"\\\\\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputPath = baseFolder + outputFolder + \"\\\\\" + outputSubfolder + \"\\\\\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputPath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if outputFolder not in os.listdir(baseFolder) :\n",
    "    print(\"creating\", baseFolder + outputFolder)\n",
    "    os.mkdir(baseFolder + outputFolder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if outputSubfolder not in os.listdir(baseFolder + outputFolder) :\n",
    "    print(\"creating\", outputPath)\n",
    "    os.mkdir(outputPath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOAD TAGGED DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fileList = os.listdir(inputPath_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fileList"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='meh'> file to be considere</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputFile = \"20170524_PP_02_Run_3.hdf5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputFile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = pd.HDFStore(inputPath_data + inputFile)\n",
    "tagged_dataset = s.get('df')\n",
    "s.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged_dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged_dataset.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valuesPerCol = {}\n",
    "colsWithRawValues = []\n",
    "lastTargetID = 0\n",
    "for c in tagged_dataset.columns :\n",
    "    if tagged_dataset[c].dtype == float :\n",
    "        colsWithRawValues.append(c)\n",
    "        continue\n",
    "    cName = c\n",
    "    if 'host' in c[:len('host')] :\n",
    "        if c not in valuesPerCol :\n",
    "            valuesPerCol[c] = []\n",
    "    elif 'target' in c[:len('target')] :\n",
    "        targetId = int(c.split('_')[1])\n",
    "        if targetId > lastTargetID :\n",
    "            lastTargetID = targetId\n",
    "        last = '_'.join(c.split('_')[2:])\n",
    "        cName = 'target_' + last\n",
    "        if cName not in valuesPerCol :\n",
    "            valuesPerCol[cName] = []\n",
    "    uniq = list(tagged_dataset[c].drop_duplicates())\n",
    "    valuesPerCol[cName] = list(set(valuesPerCol[cName] + uniq))\n",
    "print(\"categorical columns and values:\")\n",
    "for k in valuesPerCol :\n",
    "    print(\"\\t\", k, valuesPerCol[k])\n",
    "print(\"\\ncontinuous columns:\")\n",
    "for k in colsWithRawValues :\n",
    "    print(\"\\t\", k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOAD DICTIONARY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputPath_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = pd.read_pickle(inputPath_dictionary + dictionaryFilename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TAGGED DATAFRAME CORRECTION BASED ON VALID RELATIVE POSITIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrected_dataset = tagged_dataset.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in range(n_targets) :\n",
    "    corrected_dataset.loc[(~corrected_dataset['target_' + str(t) + '_relative_position_longitudinal'].isin(valid_relative_positions['longitudinal'])) |\n",
    "              (~corrected_dataset['target_' + str(t) + '_relative_position_lateral'].isin(valid_relative_positions['lateral'])),\n",
    "                        ['target_' + str(t) + '_lateral', \n",
    "                         'target_' + str(t) + '_longitudinal',\n",
    "                         'target_' + str(t) + '_relative_position_longitudinal', \n",
    "                         'target_' + str(t) + '_relative_position_lateral',\n",
    "                         'target_' + str(t) + '_velocity']] = ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATASET MAPPING\n",
    "we assume that the dataframe has already been parsed in the previous step - dictionary_update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapped_dataset = corrected_dataset.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapped_dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "shared_columns = []\n",
    "for c in mapped_dataset.columns :\n",
    "    if c in dictionary.columns :\n",
    "        shared_columns.append(c)\n",
    "shared_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapped_dataset = mapped_dataset.merge(dictionary, on=shared_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapped_dataset.sort_values(by=['timestamp'], ascending=[True], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrected_dataset.shape, mapped_dataset.shape, dictionary.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(np.asarray(list(corrected_dataset['timestamp'])) == np.asarray(list(mapped_dataset['timestamp']))) == len(corrected_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "plt.plot(list(mapped_dataset['index']))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CLEAN UP MAPPED DATASET\n",
    "so basically remove all images, leave \"index\" but rename it \"dictionary_index\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_remove = []\n",
    "for c in mapped_dataset.columns :\n",
    "    if c not in [dictionary_indexColumnName, 'n_objects'] + list(corrected_dataset.columns) :\n",
    "        cols_to_remove.append(c)\n",
    "\n",
    "cols_to_remove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapped_dataset.drop(cols_to_remove, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapped_dataset.rename(columns={dictionary_indexColumnName : 'dictionary_index'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for c in mapped_dataset.columns :\n",
    "    print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SAVING THE MAPPED DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputPath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mapped_dataset.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputFilename_dataset = outputFilename_dataset + \"_\" + inputFile[:inputFile.find(\".hdf5\")] + \".csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputFilename_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputPath + outputFilename_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapped_dataset.to_csv(outputPath + outputFilename_dataset, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# N-GRAM GRAPH CREATION\n",
    "we build a unique ngram, the rendering/filtering should occur later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = recursive_NGramKneserNey(\"world_model_\" + str(len(valid_relative_positions['lateral'])-1) + \"x\" + str(len(valid_relative_positions['longitudinal'])-1), n=nGram_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "string = \"-\".join([str(x) for x in list(mapped_dataset['dictionary_index'])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.feed(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputFilename_nGramGraph += \"_\" + inputFile[:inputFile.find(\".hdf5\")] + \".p\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputFilename_nGramGraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(model, open(outputPath + outputFilename_nGramGraph, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NETWORKX CREATION\n",
    "i.e. a network from nGramGraph without self loops and <start><stop><unk>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = nx.DiGraph()\n",
    "\n",
    "for edge in model.grams[2] :\n",
    "    n0 = edge[0]\n",
    "    n1 = edge[1]\n",
    "    if n0 in [\"<start>\", \"<stop>\", \"<unk>\"] or n1 in [\"<start>\", \"<stop>\", \"<unk>\"] or n0 == n1:\n",
    "        if n0 != n1 :\n",
    "            print(edge)\n",
    "        continue\n",
    "    G.add_edge(n0, n1)\n",
    "print(\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G.number_of_edges()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G.number_of_nodes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputFilename_networkX += \"_\" + inputFile[:inputFile.find(\".hdf5\")] + \".p\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputFilename_networkX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(G, open(outputPath + outputFilename_networkX, \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\".\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a href='#meh'> back </a>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
